---
title: "Simulation Study"
output:
  pdf_document: default
  html_document: default
date: "2024-06-03"
---





```{r}
library(MASS)
library(mstate)
library(survival)
library(ggplot2)
library(gridExtra)
library(reshape2)
library(data.table)
library(scales)
library(Matrix)
library(grid)
library(xtable)
library(reshape)
library(foreach)
library(parallel)
library(mvtnorm)
library(dplyr)
library(pracma)
library(gplots)
library(tidyr)

Print_flug = FALSE
```

# Function to generate covariate matrix, A,$\Gamma$ and B

```{r}
simulate_Beta_matrix = function(p,r,k){
#GENERATE A, \Gamma and B matrix
#A matrix
A_matrix <- matrix(NA, nrow = p, ncol = r)
for (i in 1:r) {
  A_matrix [, i] <- runif(p, 0,1)
}
#Gamma matrix
Gamma_matrix <- matrix(NA, nrow = k, ncol = r)
for (i in 1:r) {
  Gamma_matrix [, i] <- runif(k, 0,1)
}
#Coefficient matrix B (p x k)
Beta_matrix = A_matrix %*% t(Gamma_matrix)

return(Beta_matrix
       )
}
```


# Function to simulate Dataset

```{r}

simulate_data_redrank_model = function(Beta_matrix,n){
  
p=nrow(Beta_matrix)
k=ncol(Beta_matrix)

#GENERATE covariate matrix X
X <- matrix(NA, nrow = n, ncol = p)
#n random variables p times and place them in the matrix columns, to be uncorrelated
for (i in 1:p) {
  X[, i] <- rnorm(n, mean = 0, sd = 1)
}
#check correlation
cor(X)


#GENERATE T(times)
# later: mean_age = 60 
# Linear predictors
eta <- X %*% Beta_matrix
# Outcome times T from exp distribution
T <- matrix(NA, nrow = n, ncol = k)
for (i in 1:k) {
  T[, i] <- rexp(n, rate =  exp(eta)[,i])
}

#GENERATE censoring 
#Random censoring time from Uniform distribution
C <- runif(n, min = 0, max = 1)

#DETERMINE the observed time and censoring indicator
T_obs <- pmin(T, C)
delta <- as.integer(T <= C)  # 1 if the event occurred, 0 if censored
# Check the proportion of censored vs event occurrences
prop_censored <- sum(delta == 0)/n/k
prop_event <- sum(delta == 1)/n/k
# Output proportions 

if (Print_flug == TRUE){
print(paste('Proportion of censored events',prop_censored))
print(paste('Proportion of events',prop_event))
}

#CREATE matrix with data for each outcome
observed_times = T_obs
event = matrix(delta,n,k)


#CREATE data frame
data <- data.frame(id = 1:n)
for (i in 1:k) {
  data[[paste0("t", i)]] <- observed_times[, i]
  data[[paste0("d", i)]] <- event[, i]
}
# Add predictors 
for (i in 1:p) {
  data[[paste0("x", i)]] <- X[, i]
}

return (list(
    T_sim=T,
    T_obs=T_obs,
    data = data,
    X=X
  ))
}
```

# Function to preprocess data to be ready to enter the redrank model

```{r}

preprocess_data_redrank_model = function(data,k){

#subjects
n <- nrow(data)

# Transition matrix(k outcomes)
tmat <- trans.comprisk(k)

# Column names for survival times and event indicators
tnames <- paste("t", 1:k, sep="")
dnames <- paste("d", 1:k, sep="")

# Create dlong
dlong <- data.frame(
  id = rep(data$id, each = k),
  Tstart = 0,
  Tstop = c(t(matrix(unlist(c(data[, tnames])), n, k))),
  status = c(t(matrix(unlist(c(data[, dnames])), n, k))),
  from = 1,
  to = rep(2:(k+1), n),
  trans = rep(1:k, n)
)

#add predicotrs
predictor_names <- grep("^x", names(data), value = TRUE)
# Add each predictor to the dlong data frame
for (predictor in predictor_names) {
  dlong[[predictor]] <- rep(data[[predictor]], each = k)
}

# Calculate the time
dlong$time <- dlong$Tstop - dlong$Tstart

# Define the transition matrix and class
class(dlong) <- c("msdata", "data.frame")
attr(dlong, "trans") <- tmat

return(dlong)
}
```

# Function to check performance of given model

```{r}

performance_redrank_model=function(Beta_matrix, model,Print_flug) {
  
estimated_Gamma <- model$Gamma
estimated_Alpha = model$Alpha
estimated_Beta <- estimated_Alpha %*% estimated_Gamma

estimated_coefficients <- estimated_Beta   
true_coefficients <- Beta_matrix 

#correlation coeff
cor_coeff = cor(as.vector(estimated_Beta),as.vector(true_coefficients))
# Each coefficient separately
# bias
bias <- (estimated_coefficients - true_coefficients)
# MSE
mse<- (estimated_coefficients - true_coefficients)^2


bias_each_coeff = mean(abs(bias))
mse_each_coeff = mean(mse)

if (Print_flug == TRUE){
print(paste('Mean bias takihg each coeff seperatly',bias_each_coeff))
print(paste('Mean mse taking each coeff seperatly',mse_each_coeff))
}


#For each outcome separately

b_pred=c()
for (i in 1:k){
  b_pred[i] = mean(estimated_coefficients[,i]-true_coefficients[,i])
  if (Print_flug == TRUE){
  print(paste('Mean bias for outcome',i,': ',b_pred[i]))
  }
}
mse_pred=c()
for (i in 1:k){
  mse_pred[i] = mean((estimated_coefficients[,i]-true_coefficients[,i])^2)
  if (Print_flug == TRUE){
  print(paste('Mean mse for outcome',i,': ',mse_pred[i]))
  }
}


return(list(
    bias_out = bias_each_coeff,
    mse_out = mse_each_coeff,
    b_pred = b_pred,
    mse_pred = mse_pred,
    cor_coeff = cor_coeff,
    bias_for_all_coeffs = bias,
    mse_for_all_coeffs = mse
    
  ))

}
```


# Function to simulate n_simulation datasets of given rank  r

```{r}
simulations_data = function(n,p,r,k,n_simulations){
  
Beta_matrix = simulate_Beta_matrix(p,r,k)
  
dlong_list <- vector("list", n_simulations)

X_list <- vector("list", n_simulations)

T_sim_list <- vector("list", n_simulations)

T_obs_list <- vector("list", n_simulations)

for (i in 1:n_simulations){
  
  sim_data = simulate_data_redrank_model(Beta_matrix,n)
  data = sim_data$data
  
  X_list[[i]] = sim_data$X
  
  T_sim_list[[i]] = sim_data$T_sim
  
  T_obs_list[[i]] = sim_data$T_obs
  
  dlong = preprocess_data_redrank_model(data,k)
  dlong_list[[i]] <- dlong
  
}

return(list( dlong_list = dlong_list,
             Beta_matrix= Beta_matrix,
             X_list=X_list,
             T_sim_list=T_sim_list, 
             T_obs_list=T_obs_list))
}
```

# Function to fit data n_simulations times and access overall perforamnce 

```{r}
simulations_fit_and_preformance = function(datas,p,k,n_simulations,R2){
  
bias1= c()
mse1= c()
cor_coeff_vec = c()
  
bias3_list <- vector("list", n_simulations)
mse3_list <- vector("list", n_simulations)

bias_for_all_coeffs = vector("list", n_simulations)
mse_for_all_coeffs = vector("list", n_simulations)

simulation_times = numeric(n_simulations)

Beta_matrix = datas$Beta_matrix
dlong_list = datas$dlong_list
  

for (i in 1:n_simulations){
  start_time = Sys.time()

  
  dlong = dlong_list[[i]] 
  
  
  #construct formula for the model
  predictor_names <- grep("^x", names(dlong), value = TRUE)
  formula_str <- paste("Surv(Tstop, status) ~", paste(predictor_names, collapse = " + "))
  formula <- as.formula(formula_str)
  

  if (Print_flug == TRUE){
    model <- redrank(formula, data = dlong, R = R2)
  }
  else{
    model <- redrank(formula, data = dlong, R = R2, print.level = 0)
  }
  result <- performance_redrank_model(Beta_matrix, model,Print_flug)
  
  #unpack result
  bias1 = c(bias1, result$bias_out)
  mse1 = c(mse1, result$mse_out)
  
  bias3_list[[i]] <- result$b_pred
  mse3_list[[i]] <- result$mse_pred
  
  bias_for_all_coeffs[[i]] = result$bias_for_all_coeff
  mse_for_all_coeffs[[i]] = result$mse_for_all_coeffs
  
  cor_coeff_vec = c(cor_coeff_vec,result$cor_coeff)
  #Print time needed for each simulation
  end_time = Sys.time()  
  simulation_time = as.numeric(difftime(end_time, start_time, units = "secs"))  
  simulation_times[i] = simulation_time
  
  thresh = max(1,n_simulations/10)
  if ((i%%thresh) == 0){
    if (i==thresh){
    print(paste("Time for first", thresh, "simulations:", sum(simulation_times[1:thresh]), "seconds"))
    }
    else{
      print(paste("Time for simulations", (i - (thresh-1)), "to", i, ":", sum(simulation_times[(i - (thresh-1)):i]), "seconds"))
    }
  }
  
}

print(paste("Time for all", n_simulations, "simulations:", sum(simulation_times), "seconds"))
# Convert the lists to data frames
bias3_df <- do.call(rbind, bias3_list)
mse3_df <- do.call(rbind, mse3_list)

# Name the columns as Outcome_1, Outcome_2, etc.
colnames(bias3_df) <- paste0("Outcome_", 1:k)
colnames(mse3_df) <- paste0("Outcome_", 1:k)

return(list(
    bias3_df = bias3_df,
    mse3_df = mse3_df,
    bias1 = bias1,
    mse1 = mse1,
    dlong_list = dlong_list,
    cor_coeff_vec = cor_coeff_vec,
    simulation_times = simulation_times,
    bias_for_all_coeffs_and_simulations = bias_for_all_coeffs,
    mse_for_all_coeffs_and_simulation = mse_for_all_coeffs
    ))

}

```

# Function for Clculating Monte Carlo Error for MSE and Bias

```{r}
MC_errors_calc = function(mse_for_all_sims,bias_for_all_sims){
  
  MC_error_mse = sd(mse_for_all_sims)/sqrt(length(mse_for_all_sims))
  MC_error_bias = sd(bias_for_all_sims)/sqrt(length(bias_for_all_sims))
  
  return(list(
    MC_error_mse = MC_error_mse,
    MC_error_bias = MC_error_bias
  ))
}
```





# Function to generate plots



```{r}
generate_histograms <- function(performance_results_list, k,R2,barslength=0.005) {
  
  
bias3_df = performance_results_list$bias3_df
mse3_df = performance_results_list$mse3_df
bias1 = performance_results_list$bias1
mse1 = performance_results_list$mse1
cor_coeff_vec = performance_results_list$cor_coeff_vec


  # Helper function to create individual histograms
  create_histogram <- function(data, title, xlab, fill_color,binw) {
    ggplot(data.frame(value = data), aes(x = value)) +
      geom_histogram(aes(y = ..density..), binwidth = binw, fill = fill_color, color = "black", alpha = 0.7) +
      geom_density(color = "blue", linetype = "dashed", linewidth = 1) +
      geom_vline(aes(xintercept = mean(data)), color = "red", linetype = "dotted", linewidth = 1) +
      labs(title = title, x = xlab, y = "Density") +
      theme_minimal()
  }
  
  # Plot 1: Histogram of Mean Bias
  p1 <- create_histogram(bias1, "", "Mean Absolute Bias", "pink",barslength)
  
  # Plot 2: Histogram of Mean MSE
  p2 <- create_histogram(mse1, "", "Mean MSE", "orange",barslength/10)
  
  # Plot 3: Histograms of Mean Bias for each outcome
  bias_plots <- lapply(1:k, function(i) {
    create_histogram(bias3_df[,i], paste("Outcome", i), "Mean Bias", "pink",barslength)
  })
  
  # Plot 4: Histograms of Mean MSE for each outcome
  mse_plots <- lapply(1:k, function(i) {
    create_histogram(mse3_df[,i], paste("Outcome", i), "Mean MSE", "orange",barslength/10)
  })
  
  # Plot 5: Histogram of Correlation coefficient between B est and B true
  corr_plot <- create_histogram(cor_coeff_vec, "", "Corelation Coefficient between B true and B", "green",barslength/10)
  
  # Arrange plots in a grid 
  top <- textGrob(
  paste('Fitted rank:',R2),
  gp = gpar(fontsize = 20)  # Adjust the fontsize as needed
)
  
  
  pL1 = grid.arrange(corr_plot, ncol = 1, top = top)
                    #"Histogrm of Corelation Coefficient between B true and B estimate across 200 simulations")
  pL2 =grid.arrange(p1, p2, ncol = 2, top = top)
                     #'Histograms of Mean Bias(left) and Mean MSE(right) across 200 Simulations')
  pL3 =grid.arrange(grobs = bias_plots, ncol = k/2, top = top)
                     #"Histograms of Mean Bias for each outcome across 200 Simulations")
  pL4 =grid.arrange(grobs = mse_plots, ncol = k/2, top = top)
  #"Histograms of Mean MSE for each outcome across 200 Simulations")
  
  return(list(
    meanbias1 = mean(bias1),
    meanmse1 = mean(mse1),
    meancorr =mean(cor_coeff_vec),
    pL1=pL1,
    pL2=pL2,
    pL3=pL3,
    pL4=pL4
  ))
}
```

# Function to plot simulated data

```{r}
generate_data_plots <- function(dlong_list,n,r,p,k,T_sim=NULL,T_obs=NULL) {

combined_data=dlong_list
T_sim_combined=T_sim
T_obs_combined=T_obs
# Combine all data sets into a single data frame
if (!is.data.frame(dlong_list)){
combined_data <- do.call(rbind, dlong_list)
print(summary(combined_data))
}
  
if (!is.null(T_sim)){
T_sim_combined=do.call(rbind, T_sim)
T_obs_combined=do.call(rbind, T_obs)
}

predictor_names <- grep("^x", names(combined_data), value = TRUE)
predictors_long <- reshape2::melt(combined_data, id.vars = 'id', measure.vars = predictor_names)

# Plot the distribution of predictors on the same plot
p1 = ggplot(predictors_long, aes(x = value, color = variable, fill = variable)) +
  geom_density(alpha = 0.05) +
  labs(title = 'Distribution of \n Predictors', x = 'Value', y = 'Density') +
  theme(legend.title = element_blank())+
  theme(
    axis.text.x = element_text(angle = 90, hjust = 1, size = 15),  
    axis.text.y = element_text(size = 15),                         
    axis.title.x = element_text(size = 15),                       
    axis.title.y = element_text(size = 15),                       
    legend.text = element_text(size = 15),                         
    legend.title = element_text(size = 15),                       
    plot.title = element_text(size = 20),           
    strip.text = element_text(size = 15)                           
  )
theme_minimal() 

# Plot the distribution of Tstop
p2 = ggplot(combined_data, aes(x = Tstop)) +
  geom_density(fill = 'blue', alpha = 0.05) +
  labs(title = 'Distribution of Tstop', x = 'Tstop', y = 'Density') +
  theme(legend.title = element_blank())+
  theme(
    axis.text.x = element_text(angle = 90, hjust = 1, size = 15),  
    axis.text.y = element_text(size = 15),                         
    axis.title.x = element_text(size = 15),                       
    axis.title.y = element_text(size = 15),                       
    legend.text = element_text(size = 15),                         
    legend.title = element_text(size = 15),                       
    plot.title = element_text(size = 20),           
    strip.text = element_text(size = 15)                           
  )
theme_minimal() 

# T_obs_long <- as.data.frame(T_obs_combined) %>%
#   pivot_longer(
#     cols = everything(), 
#     names_to = "variable",  
#     values_to = "value"     
#   )

# p3 = ggplot(T_obs_long, aes(x = value, color = variable)) +
#   geom_density() +
#   labs(title = 'T_obs Plot for Each Outcome',
#        x = 'T_obs',
#        y = 'Density') +
#   theme_minimal()
# 
# T_sim_long <- as.data.frame(T_sim_combined) %>%
#   pivot_longer(
#     cols = everything(),  
#     names_to = "variable",  
#     values_to = "value"     
#   )
# 
# p4 = ggplot(T_sim_long, aes(x = value, color = variable)) +
#   geom_density() +
#   labs(title = 'T_sim Plot for Each Outcome',
#        x = 'T_simulated',
#        y = 'Density') +
#   theme_minimal()

top = paste('Datasets with ',p,' covariates and ',k,' outcomes for ',n,' individuals')
top <- textGrob(
  paste('Datasets with ',p,' covariates, \n',k,' outcomes, ',n,' individuals'),
  gp = gpar(fontsize = 25)  
)
grid_plots = grid.arrange(p1, p2, ncol = 2, top = top)
 
status_counts <- table(combined_data$status)
status_percentages <- prop.table(status_counts) * 100
# Print the percentages
print(paste('Percentage of Censored data:', status_percentages[1],' Percentage of Uncensored data', status_percentages[2]))
return(grid_plots)
}
```

# Linear Predictor error

```{r}
linear_predictor_err <- function(datas1, performance_results11) {
    
    X_list = datas1$X_list
    
    bias_list = performance_results11$bias_for_all_coeffs_and_simulations
    Beta_true = datas1$Beta_matrix
    
    Beta_list_estimated = list()
    eta_list_estimated = list()
    eta_list_real = list()
    
    lp_errors=list()
    
    ave_lp_errors = list()
    
    cor_lp=c()
    
    for(i in 1:length(datas1$dlong_list)){
      Beta_list_estimated[[i]] = bias_list[[i]] + Beta_true
      eta_list_estimated[[i]] = X_list[[i]] %*% Beta_list_estimated[[i]]
      
      eta_list_real[[i]]=X_list[[i]]  %*% Beta_true
      
      cor_lp=c(cor_lp,cor(as.vector(eta_list_estimated[[i]]),as.vector(eta_list_real[[i]])))
      
      K=dim(eta_list_estimated[[i]])[2]
      
      lp_errors[[i]]=rowSums((eta_list_estimated[[i]] - eta_list_real[[i]])^2)/K
      
      ave_lp_errors[[i]] = mean(lp_errors[[i]])
      
    }
    
    return(list(
    ave_lp_errors = ave_lp_errors,
    lp_errors = lp_errors,
    cor_lp=cor_lp))
}
```


# Heatmaps

```{r}
beta_heatmap = function(title,datas1, performance_results11,minb,maxb,minmax=TRUE) {

    bias_list = performance_results11$bias_for_all_coeffs_and_simulations
    Beta_true = datas1$Beta_matrix
    Beta_list_estimated = list()
    
    for(i in 1:length(datas1$dlong_list)){
      Beta_list_estimated[[i]] = bias_list[[i]] + Beta_true
       }

   sum_betas_estimated <- Reduce("+", Beta_list_estimated) #sum element wise all matrices in the list
  
  # Calculate the mean
  mean_beta <- sum_betas_estimated / length(Beta_list_estimated)
  
  colnames(mean_beta) <- paste("k=", 1:ncol(mean_beta), sep="")
  rownames(mean_beta) <- paste("X", 1:nrow(mean_beta), sep="")
  
  beta_df <- reshape2::melt(mean_beta)
  
  


if(minmax==TRUE){
breaks <- seq(minb, maxb, length.out = 101)
}
else{
 breaks <- seq(min(mean_beta), max(mean_beta), length.out = 101) 
}

norm_breaks <- (breaks - min(breaks)) / (max(breaks) - min(breaks))
my_palette <- colorRampPalette(c("lightgrey", "orange", "black"))(100)
# Generate the heatmap
# heatmap.2(mean_beta, Rowv=F, Colv=F, scale = "none", col = my_palette,
#           density.info = "none", trace = "none", dendrogram = "none",breaks=breaks)

p <- ggplot(beta_df, aes(x = Var2, y = Var1, fill = value)) +
  geom_tile() +
  scale_fill_gradientn(colors = my_palette, 
                       values = norm_breaks, # specify normalized breaks here
                       limits = range(breaks)) + # Set limits to the min and max of your breaks
  theme_minimal() +
  labs(x = "", y = "", fill = "Value") +
  theme(
    axis.text.x = element_text(angle = 90, hjust = 1, size = 15),  
    axis.text.y = element_text(size = 15),                         
    axis.title.x = element_text(size = 15),                       
    axis.title.y = element_text(size = 15),                       
    legend.text = element_text(size = 20),                         
    legend.title = element_text(size = 20),                       
    plot.title = element_text(size = 30),           
    strip.text = element_text(size = 20)                           
  )+
  ggtitle(title)

print(min(mean_beta))
print(max(mean_beta))
print(mean_beta)
return(p)
}



```

# First Configuration (sample size:500, p=5,K=4, n_simulations = 200)

## Call functions to simulate  n_simulations = 200 datasets using rank r=1 

```{r}
#set seed
set.seed(123)
#chose variables 
n = 500   # Number of observations
p <- 5    # Number of covariates
k <- 4   # Number of outcomes
n_simulations = 200

r <- 1  # Rank of simulated data

datas1 = simulations_data(n,p,r,k,n_simulations)


Beta_matrix_1=datas1$Beta_matrix
colnames(Beta_matrix_1) <- paste("k=", 1:ncol(Beta_matrix_1), sep="")
rownames(Beta_matrix_1) <- paste("X", 1:nrow(Beta_matrix_1), sep="")
beta_df <- reshape2::melt(Beta_matrix_1)
my_palette <- colorRampPalette(c("lightgrey", "orange", "black"))(100)
breaks <- seq(0.0050,  0.86, length.out = 101)
norm_breaks <- (breaks - min(breaks)) / (max(breaks) - min(breaks))
heatmap1_1_real <- ggplot(beta_df, aes(x = Var2, y = Var1, fill = value)) +
  geom_tile() +
  scale_fill_gradientn(colors = my_palette, 
                       values = norm_breaks, # specify normalized breaks here
                       limits = range(breaks)) + # Set limits to the min and max of your breaks
  theme_minimal() +
  labs(x = "", y = "", fill = "Value") +
  theme(
    axis.text.x = element_text(angle = 90, hjust = 1, size = 15),  
    axis.text.y = element_text(size = 15),                         
    axis.title.x = element_text(size = 15),                       
    axis.title.y = element_text(size = 15),                       
    legend.text = element_text(size = 20),                         
    legend.title = element_text(size = 20),                       
    plot.title = element_text(size = 30),           
    strip.text = element_text(size = 20)                           
  )+
  ggtitle("Simulated under rank 1")
```



### Generated Data Statistics Plots

```{r}
first1=generate_data_plots(datas1$dlong_list,500,1,5,4,datas1$T_sim_list,datas1$T_obs_list)
```

### Fit simulated datasets with model of rank R2=1

#### Get performance
```{r}
#set seed
set.seed(123)

R2 = 1 #rank of fitted model

performance_results11 = simulations_fit_and_preformance(datas1,p,k,n_simulations,R2)

```


#### Get Average Beta Heatmap
```{r}
beta_heat_1_11=beta_heatmap("Fitted under rank 1",datas1, performance_results11,0.0050,  0.86) 
```

#### Plot performance
```{r}
r11 = generate_histograms(performance_results11, 4,1)
```
#### ΜC ERRORS

```{r}
msesmc = unlist(lapply(performance_results11$mse_for_all_coeffs_and_simulation, as.vector))
biasmc = unlist(lapply(performance_results11$bias_for_all_coeffs_and_simulations, as.vector))
MC_errors11 = MC_errors_calc(msesmc,biasmc)
```

#### LP ERRORS

```{r}
LP_errorsfirst11 = linear_predictor_err(datas1, performance_results11)

lp_errors_mc = unlist(lapply(LP_errorsfirst11$lp_errors, as.vector))

MC_error_lp_first11=sd(lp_errors_mc)/sqrt(length(lp_errors_mc))
```


### Fit simulated datasets with model of rank R2=2

#### Get performance
```{r}
#set seed
set.seed(123)

R2 = 2 #rank of fitted model

performance_results12 = simulations_fit_and_preformance(datas1,p,k,n_simulations,R2)

```


#### Get Average Beta Heatmap
```{r}
beta_heat_1_12=beta_heatmap("Fitted under rank 2",datas1, performance_results12,0.0050,  0.86)
```

#### Plot performance
```{r}
r12 = generate_histograms(performance_results12, 4,2)
```
#### ΜC ERRORS

```{r}
msesmc = unlist(lapply(performance_results12$mse_for_all_coeffs_and_simulation, as.vector))
biasmc = unlist(lapply(performance_results12$bias_for_all_coeffs_and_simulations, as.vector))
MC_errors12 = MC_errors_calc(msesmc,biasmc)
```


#### LP ERRORS

```{r}
LP_errorsfirst12 = linear_predictor_err(datas1, performance_results12)

lp_errors_mc = unlist(lapply(LP_errorsfirst12$lp_errors, as.vector))

MC_error_lp_first12=sd(lp_errors_mc)/sqrt(length(lp_errors_mc))
```


### Fit simulated datasets with model of rank R2=3

#### Get performance
```{r}
#set seed
set.seed(123)

R2 = 3 #rank of fitted model

performance_results13 = simulations_fit_and_preformance(datas1,p,k,n_simulations,R2)

```

#### Plot performance
```{r}
r13 = generate_histograms(performance_results13,4,3)
```


#### ΜC ERRORS

```{r}
msesmc = unlist(lapply(performance_results13$mse_for_all_coeffs_and_simulation, as.vector))
biasmc = unlist(lapply(performance_results13$bias_for_all_coeffs_and_simulations, as.vector))
MC_errors13 = MC_errors_calc(msesmc,biasmc)
```

#### LP ERRORS

```{r}
LP_errorsfirst13 = linear_predictor_err(datas1, performance_results13)

lp_errors_mc = unlist(lapply(LP_errorsfirst13$lp_errors, as.vector))

MC_error_lp_first13=sd(lp_errors_mc)/sqrt(length(lp_errors_mc))
```



#### Plots
```{r}
plot_listf1 <- list()
plot_listf1[[1]]=r11$pL2
plot_listf1[[2]]=r12$pL2
# plot_listf1[[3]]=r13$pL2


top=paste('Histograms of mean absolute bias(left) and MSE(right) for 200 simulated under rank 1 datasets with 500 individuals, 5 predictors and 4 outcomes when fitted with ranks 1,2 or 3')
top=''

combined_plotsf1 <- grid.arrange(grobs = plot_listf1, ncol = 2, nrow = 1,top=top)

# Save the combined plot grid as a PNG
ggsave("combined_plotsf1p1.png", plot = combined_plotsf1, width = 23, height = 6, dpi = 300)


plot_listf1 <- list()
plot_listf1[[1]]=r11$pL3
plot_listf1[[2]]=r12$pL3
# plot_listf1[[3]]=r13$pL3


top=paste('Histograms of mean bias per outcome for 200 simulated under rank 1 datasets with 500 individuals, 5 predictors and 4 outcomes when fitted with ranks 1,2 or 3')
top=''

combined_plotsf1 <- grid.arrange(grobs = plot_listf1, ncol = 2, nrow = 1,top=top)

# Save the combined plot grid as a PNG
ggsave("combined_plotsf1p2.png", plot = combined_plotsf1, width = 23, height = 6, dpi = 300)


plot_listf1 <- list()
plot_listf1[[1]]=r11$pL4
plot_listf1[[2]]=r12$pL4
# plot_listf1[[3]]=r13$pL4


top=paste('Histograms of mean MSE per outcome for 200 simulated under rank 1 datasets with 500 individuals, 5 predictors and 4 outcomes when fitted with ranks 1,2 or 3')
top=''

combined_plotsf1 <- grid.arrange(grobs = plot_listf1, ncol = 2, nrow = 1,top=top)

# Save the combined plot grid as a PNG
ggsave("combined_plotsf1p3.png", plot = combined_plotsf1, width = 23, height = 6, dpi = 300)

```



## Call functions to simulate  n_simulations = 200 datasets using rank r=2 

```{r}
#set seed
set.seed(123)

r <- 2  # Rank of simulated data

datas2 = simulations_data(n,p,r,k,n_simulations)

Beta_matrix_1=datas2$Beta_matrix
colnames(Beta_matrix_1) <- paste("k=", 1:ncol(Beta_matrix_1), sep="")
rownames(Beta_matrix_1) <- paste("X", 1:nrow(Beta_matrix_1), sep="")
beta_df <- reshape2::melt(Beta_matrix_1)
my_palette <- colorRampPalette(c("lightgrey", "orange", "black"))(100)
breaks <- seq(0.13, 1.02, length.out = 101)
norm_breaks <- (breaks - min(breaks)) / (max(breaks) - min(breaks))
heatmap1_2_real <- ggplot(beta_df, aes(x = Var2, y = Var1, fill = value)) +
  geom_tile() +
  scale_fill_gradientn(colors = my_palette, 
                       values = norm_breaks, # specify normalized breaks here
                       limits = range(breaks)) + # Set limits to the min and max of your breaks
  theme_minimal() +
  labs(x = "", y = "", fill = "Value") +
  theme(
    axis.text.x = element_text(angle = 90, hjust = 1, size = 15),  
    axis.text.y = element_text(size = 15),                         
    axis.title.x = element_text(size = 15),                       
    axis.title.y = element_text(size = 15),                       
    legend.text = element_text(size = 20),                         
    legend.title = element_text(size = 20),                       
    plot.title = element_text(size = 30),           
    strip.text = element_text(size = 20)                           
  )+
  ggtitle("Simulated under rank 2")
```

### Generated Data Statistics Plots

```{r}
first2=generate_data_plots(datas2$dlong_list,500,2,5,4,datas2$T_sim_list,datas2$T_obs_list)
```

### Fit simulated datasets with model of rank R2=1

#### Get performance
```{r}
#set seed
set.seed(123)

R2 = 1 #rank of fitted model

performance_results21 = simulations_fit_and_preformance(datas2,p,k,n_simulations,R2)

```


#### Get Average Beta Heatmap
```{r}
beta_heat_1_21=beta_heatmap("Fitted under rank 1",datas2, performance_results21,0.13, 1.02)
```


#### Plot performance
```{r}
r21 = generate_histograms(performance_results21, 4,1)
```
#### ΜC ERRORS

```{r}
msesmc = unlist(lapply(performance_results21$mse_for_all_coeffs_and_simulation, as.vector))
biasmc = unlist(lapply(performance_results21$bias_for_all_coeffs_and_simulations, as.vector))
MC_errors21 = MC_errors_calc(msesmc,biasmc)
```


#### LP ERRORS

```{r}
LP_errorsfirst21 = linear_predictor_err(datas2, performance_results21)

lp_errors_mc = unlist(lapply(LP_errorsfirst21$lp_errors, as.vector))

MC_error_lp_first21=sd(lp_errors_mc)/sqrt(length(lp_errors_mc))
```


### Fit simulated datasets with model of rank R2=2

#### Get performance
```{r}
#set seed
set.seed(123)

R2 = 2 #rank of fitted model

performance_results22 = simulations_fit_and_preformance(datas2,p,k,n_simulations,R2)

```


#### Get Average Beta Heatmap
```{r}
beta_heat_1_22=beta_heatmap("Fitted under rank 2",datas2, performance_results22,0.13, 1.02)

```

#### Plot performance
```{r}
r22 = generate_histograms(performance_results22, 4,2)
```

#### ΜC ERRORS

```{r}
msesmc = unlist(lapply(performance_results22$mse_for_all_coeffs_and_simulation, as.vector))
biasmc = unlist(lapply(performance_results22$bias_for_all_coeffs_and_simulations, as.vector))
MC_errors22 = MC_errors_calc(msesmc,biasmc)
```


#### LP ERRORS

```{r}
LP_errorsfirst22 = linear_predictor_err(datas2, performance_results22)

lp_errors_mc = unlist(lapply(LP_errorsfirst22$lp_errors, as.vector))

MC_error_lp_first22=sd(lp_errors_mc)/sqrt(length(lp_errors_mc))
```


### Fit simulated datasets with model of rank R2=3

#### Get performance
```{r}
#set seed
set.seed(123)

R2 = 3 #rank of fitted model

performance_results23 = simulations_fit_and_preformance(datas2,p,k,n_simulations,R2)

```


#### Get Average Beta Heatmap
```{r}
beta_heat_1_23=beta_heatmap("Fitted under rank 3",datas2, performance_results23,0.13, 1.02)
```


#### Plot performance
```{r}
r23 = generate_histograms(performance_results23, 4, 3)
```



#### ΜC ERRORS

```{r}
msesmc = unlist(lapply(performance_results23$mse_for_all_coeffs_and_simulation, as.vector))
biasmc = unlist(lapply(performance_results23$bias_for_all_coeffs_and_simulations, as.vector))
MC_errors23 = MC_errors_calc(msesmc,biasmc)
```


#### LP ERRORS

```{r}
LP_errorsfirst23 = linear_predictor_err(datas2, performance_results23)

lp_errors_mc = unlist(lapply(LP_errorsfirst23$lp_errors, as.vector))

MC_error_lp_first23=sd(lp_errors_mc)/sqrt(length(lp_errors_mc))
```


#### Plots
```{r}
plot_listf1 <- list()
plot_listf1[[1]]=r21$pL2
plot_listf1[[2]]=r22$pL2
plot_listf1[[3]]=r23$pL2


top=paste('Histograms of mean absolute bias(left) and MSE(right) for 200 simulated under rank 2 datasets with 500 individuals, 5 predictors and 4 outcomes when fitted with ranks 1,2 or 3')
top=''

combined_plotsf1 <- grid.arrange(grobs = plot_listf1, ncol = 3, nrow = 1,top=top)

# Save the combined plot grid as a PNG
ggsave("combined_plotsf2p1.png", plot = combined_plotsf1, width = 23, height = 6, dpi = 300)


plot_listf1 <- list()
plot_listf1[[1]]=r21$pL3
plot_listf1[[2]]=r22$pL3
plot_listf1[[3]]=r23$pL3


top=paste('Histograms of mean bias per outcome for 200 simulated under rank 2 datasets with 500 individuals, 5 predictors and 4 outcomes when fitted with ranks 1,2 or 3')
top=''
combined_plotsf1 <- grid.arrange(grobs = plot_listf1, ncol = 3, nrow = 1,top=top)

# Save the combined plot grid as a PNG
ggsave("combined_plotsf2p2.png", plot = combined_plotsf1, width = 23, height = 6, dpi = 300)


plot_listf1 <- list()
plot_listf1[[1]]=r21$pL4
plot_listf1[[2]]=r22$pL4
plot_listf1[[3]]=r23$pL4


top=paste('Histograms of mean MSE per outcome for 200 simulated under rank 2 datasets with 500 individuals, 5 predictors and 4 outcomes when fitted with ranks 1,2 or 3')
top=''
combined_plotsf1 <- grid.arrange(grobs = plot_listf1, ncol = 3, nrow = 1,top=top)

# Save the combined plot grid as a PNG
ggsave("combined_plotsf2p3.png", plot = combined_plotsf1, width = 23, height = 6, dpi = 300)

```


### Heatmaps

```{r}
plot_listf1 <- list()
plot_listf1[[1]]=heatmap1_1_real
plot_listf1[[2]]=beta_heat_1_11
plot_listf1[[3]]=beta_heat_1_12
plot_listf1[[4]]=NULL
plot_listf1[[5]]=heatmap1_2_real
plot_listf1[[6]]=beta_heat_1_21
plot_listf1[[7]]=beta_heat_1_22
plot_listf1[[8]]=beta_heat_1_23

top=''

heatmaps_1 <- grid.arrange(grobs = plot_listf1, ncol = 4, nrow = 2,top=top)

# Save the combined plot grid as a PNG
ggsave("heatmaps_1.png", plot = heatmaps_1, width = 23, height = 6, dpi = 300)
```

#  Second Configuration (sample size:1000, p=5,K=4, n_simulations = 200)

## Call functions to simulate  n_simulations = 200 datasets using rank r=1 

```{r}
#set seed
set.seed(123)
#chose variables 
n = 1000   # Number of observations
p <- 5    # Number of covariates
k <- 4   # Number of outcomes
 n_simulations = 200

r <- 1  # Rank of simulated data

datas1a = simulations_data(n,p,r,k,n_simulations)

Beta_matrix_1=datas1a$Beta_matrix
colnames(Beta_matrix_1) <- paste("k=", 1:ncol(Beta_matrix_1), sep="")
rownames(Beta_matrix_1) <- paste("X", 1:nrow(Beta_matrix_1), sep="")
beta_df <- reshape2::melt(Beta_matrix_1)
my_palette <- colorRampPalette(c("lightgrey", "orange", "black"))(100)
breaks <- seq(0.0050,  0.86, length.out = 101)
norm_breaks <- (breaks - min(breaks)) / (max(breaks) - min(breaks))
heatmap2_1_real <- ggplot(beta_df, aes(x = Var2, y = Var1, fill = value)) +
  geom_tile() +
  scale_fill_gradientn(colors = my_palette, 
                       values = norm_breaks, # specify normalized breaks here
                       limits = range(breaks)) + # Set limits to the min and max of your breaks
  theme_minimal() +
  labs(x = "", y = "", fill = "Value") +
  theme(     axis.text.x = element_text(angle = 90, hjust = 1, size = 15),       axis.text.y = element_text(size = 15),                              axis.title.x = element_text(size = 15),                            axis.title.y = element_text(size = 15),                            legend.text = element_text(size = 20),                              legend.title = element_text(size = 20),                            plot.title = element_text(size = 30),                strip.text = element_text(size = 20)                              )+
  ggtitle("Simulated under rank 1")
```

### Generated Data Statistics Plots

```{r}
second1=generate_data_plots(datas1a$dlong_list,1000,1,5,4,datas1a$T_sim_list,datas1a$T_obs_list)
```

### Fit simulated datasets with model of rank R2=1

#### Get performance
```{r}
#set seed
set.seed(123)

R2 = 1 #rank of fitted model

performance_results11a = simulations_fit_and_preformance(datas1a,p,k,n_simulations,R2)

```


#### Get Average Beta Heatmap
```{r}
beta_heat_2_11=beta_heatmap("Fitted under rank 1",datas1a, performance_results11a,0.0050,  0.86)
```


#### Plot performance
```{r}
r11a = generate_histograms(performance_results11a, 4,1)
```

#### ΜC ERRORS

```{r}
msesmc = unlist(lapply(performance_results11a$mse_for_all_coeffs_and_simulation, as.vector))
biasmc = unlist(lapply(performance_results11a$bias_for_all_coeffs_and_simulations, as.vector))
MC_errors11a = MC_errors_calc(msesmc,biasmc)
```


#### LP ERRORS

```{r}
LP_errorssecond11 = linear_predictor_err(datas1a, performance_results11a)

lp_errors_mc = unlist(lapply(LP_errorssecond11$lp_errors, as.vector))

MC_error_lp_second11=sd(lp_errors_mc)/sqrt(length(lp_errors_mc))
```


### Fit simulated datasets with model of rank R2=2

#### Get performance
```{r}
#set seed
set.seed(123)

R2 = 2 #rank of fitted model

performance_results12a = simulations_fit_and_preformance(datas1a,p,k,n_simulations,R2)

```


#### Get Average Beta Heatmap
```{r}
beta_heat_2_12=beta_heatmap("Fitted under rank 2",datas1a, performance_results12a,0.0050,  0.86)
```

#### Plot performance
```{r}
r12a = generate_histograms(performance_results12a, 4,2)
```

#### ΜC ERRORS

```{r}
msesmc = unlist(lapply(performance_results12a$mse_for_all_coeffs_and_simulation, as.vector))
biasmc = unlist(lapply(performance_results12a$bias_for_all_coeffs_and_simulations, as.vector))
MC_errors12a = MC_errors_calc(msesmc,biasmc)
```

#### LP ERRORS

```{r}
LP_errorssecond12 = linear_predictor_err(datas1a, performance_results12a)

lp_errors_mc = unlist(lapply(LP_errorssecond12$lp_errors, as.vector))

MC_error_lp_second12=sd(lp_errors_mc)/sqrt(length(lp_errors_mc))
```


### Fit simulated datasets with model of rank R2=3

#### Get performance
```{r}
#set seed
set.seed(123)

R2 = 3 #rank of fitted model

performance_results13a = simulations_fit_and_preformance(datas1a,p,k,n_simulations,R2)

```


#### Plot performance
```{r}
r13a = generate_histograms(performance_results13a, 4,3)
```


#### ΜC ERRORS

```{r}
msesmc = unlist(lapply(performance_results13a$mse_for_all_coeffs_and_simulation, as.vector))
biasmc = unlist(lapply(performance_results13a$bias_for_all_coeffs_and_simulations, as.vector))
MC_errors13a = MC_errors_calc(msesmc,biasmc)
```


#### LP ERRORS

```{r}
LP_errorssecond13 = linear_predictor_err(datas1a, performance_results13a)

lp_errors_mc = unlist(lapply(LP_errorssecond13$lp_errors, as.vector))

MC_error_lp_second13=sd(lp_errors_mc)/sqrt(length(lp_errors_mc))
```

#### Plots

```{r}
plot_listf1 <- list()
plot_listf1[[1]]=r11a$pL2
plot_listf1[[2]]=r12a$pL2
# plot_listf1[[3]]=r13a$pL2


top=paste('Histograms of mean absolute bias(left) and MSE(right) for 200 simulated under rank 1 datasets with 1000 individuals, 5 predictors and 4 outcomes when fitted with ranks 1,2 or 3')
top=''

combined_plotsf1 <- grid.arrange(grobs = plot_listf1, ncol = 2, nrow = 1,top=top)

# Save the combined plot grid as a PNG
ggsave("combined_plotss1p1.png", plot = combined_plotsf1, width = 23, height = 6, dpi = 300)


plot_listf1 <- list()
plot_listf1[[1]]=r11a$pL3
plot_listf1[[2]]=r12a$pL3
# plot_listf1[[3]]=r13a$pL3


top=paste('Histograms of mean bias per outcome for 200 simulated under rank 1 datasets with 1000 individuals, 5 predictors and 4 outcomes when fitted with ranks 1,2 or 3')
top=''

combined_plotsf1 <- grid.arrange(grobs = plot_listf1, ncol = 2, nrow = 1,top=top)

# Save the combined plot grid as a PNG
ggsave("combined_plotss1p2.png", plot = combined_plotsf1, width = 23, height = 6, dpi = 300)


plot_listf1 <- list()
plot_listf1[[1]]=r11a$pL4
plot_listf1[[2]]=r12a$pL4
# plot_listf1[[3]]=r13a$pL4


top=paste('Histograms of mean MSE per outcome for 200 simulated under rank 1 datasets with 1000 individuals, 5 predictors and 4 outcomes when fitted with ranks 1,2 or 3')
top=''

combined_plotsf1 <- grid.arrange(grobs = plot_listf1, ncol = 2, nrow = 1,top=top)

# Save the combined plot grid as a PNG
ggsave("combined_plotss1p3.png", plot = combined_plotsf1, width = 23, height = 6, dpi = 300)

```




## Call functions to simulate  n_simulations = 200 datasets using rank r=2 

```{r}
#set seed
set.seed(123)

r <- 2  # Rank of simulated data

datas2a = simulations_data(n,p,r,k,n_simulations)

Beta_matrix_1=datas2a$Beta_matrix
colnames(Beta_matrix_1) <- paste("k=", 1:ncol(Beta_matrix_1), sep="")
rownames(Beta_matrix_1) <- paste("X", 1:nrow(Beta_matrix_1), sep="")
beta_df <- reshape2::melt(Beta_matrix_1)
my_palette <- colorRampPalette(c("lightgrey", "orange", "black"))(100)
breaks <- seq(0.13, 1.02, length.out = 101)
norm_breaks <- (breaks - min(breaks)) / (max(breaks) - min(breaks))
heatmap2_2_real <- ggplot(beta_df, aes(x = Var2, y = Var1, fill = value)) +
  geom_tile() +
  scale_fill_gradientn(colors = my_palette, 
                       values = norm_breaks, # specify normalized breaks here
                       limits = range(breaks)) + # Set limits to the min and max of your breaks
  theme_minimal() +
  labs(x = "", y = "", fill = "Value") +
  theme(     axis.text.x = element_text(angle = 90, hjust = 1, size = 15),       axis.text.y = element_text(size = 15),                              axis.title.x = element_text(size = 15),                            axis.title.y = element_text(size = 15),                            legend.text = element_text(size = 20),                              legend.title = element_text(size = 20),                            plot.title = element_text(size = 30),                strip.text = element_text(size = 20)                              )+
  ggtitle("Simulated under rank 2")
```

### Generated Data Statistics Plots

```{r}
second2=generate_data_plots(datas2a$dlong_list,1000,2,5,4,datas2a$T_sim_list,datas2a$T_obs_list)
```

### Fit simulated datasets with model of rank R2=1

#### Get performance
```{r}
#set seed
set.seed(123)

R2 = 1 #rank of fitted model

performance_results21a = simulations_fit_and_preformance(datas2a,p,k,n_simulations,R2)

```


#### Get Average Beta Heatmap
```{r}
beta_heat_2_21=beta_heatmap("Fitted under rank 1",datas2a, performance_results21a,0.13, 1.02)
```

#### Plot performance
```{r}
r21a = generate_histograms(performance_results21a, 4,1)
```
#### ΜC ERRORS

```{r}
msesmc = unlist(lapply(performance_results21a$mse_for_all_coeffs_and_simulation, as.vector))
biasmc = unlist(lapply(performance_results21a$bias_for_all_coeffs_and_simulations, as.vector))
MC_errors21a = MC_errors_calc(msesmc,biasmc)
```


#### LP ERRORS

```{r}
LP_errorssecond21 = linear_predictor_err(datas2a, performance_results21a)

lp_errors_mc = unlist(lapply(LP_errorssecond21$lp_errors, as.vector))

MC_error_lp_second21=sd(lp_errors_mc)/sqrt(length(lp_errors_mc))
```


### Fit simulated datasets with model of rank R2=2

#### Get performance
```{r}
#set seed
set.seed(123)

R2 = 2 #rank of fitted model

performance_results22a = simulations_fit_and_preformance(datas2a,p,k,n_simulations,R2)

```



#### Get Average Beta Heatmap
```{r}
beta_heat_2_22=beta_heatmap("Fitted under rank 2",datas2a, performance_results22a,0.13, 1.02)
```


#### Plot performance
```{r}
r22a = generate_histograms(performance_results22a, 4,2)
```


#### ΜC ERRORS

```{r}
msesmc = unlist(lapply(performance_results22a$mse_for_all_coeffs_and_simulation, as.vector))
biasmc = unlist(lapply(performance_results22a$bias_for_all_coeffs_and_simulations, as.vector))
MC_errors22a = MC_errors_calc(msesmc,biasmc)
```


#### LP ERRORS

```{r}
LP_errorssecond22 = linear_predictor_err(datas2a, performance_results22a)

lp_errors_mc = unlist(lapply(LP_errorssecond22$lp_errors, as.vector))

MC_error_lp_second22=sd(lp_errors_mc)/sqrt(length(lp_errors_mc))
```


### Fit simulated datasets with model of rank R2=3

#### Get performance
```{r}
#set seed
set.seed(123)

R2 = 3 #rank of fitted model

performance_results23a = simulations_fit_and_preformance(datas2a,p,k,n_simulations,R2)

```

#### Get Average Beta Heatmap
```{r}
beta_heat_2_23=beta_heatmap("Fitted under rank 3",datas2a, performance_results23a,0.13, 1.02)
```

#### Plot performance
```{r}
r23a = generate_histograms(performance_results23a, 4,3)
```

#### ΜC ERRORS

```{r}
msesmc = unlist(lapply(performance_results23a$mse_for_all_coeffs_and_simulation, as.vector))
biasmc = unlist(lapply(performance_results23a$bias_for_all_coeffs_and_simulations, as.vector))
MC_errors23a = MC_errors_calc(msesmc,biasmc)
```


#### LP ERRORS

```{r}
LP_errorssecond23 = linear_predictor_err(datas2a, performance_results23a)

lp_errors_mc = unlist(lapply(LP_errorssecond23$lp_errors, as.vector))

MC_error_lp_second23=sd(lp_errors_mc)/sqrt(length(lp_errors_mc))
```


#### Plots

```{r}
plot_listf1 <- list()
plot_listf1[[1]]=r21a$pL2
plot_listf1[[2]]=r22a$pL2
plot_listf1[[3]]=r23a$pL2


top=paste('Histograms of mean absolute bias(left) and MSE(right) for 200 simulated under rank 2 datasets with 1000 individuals, 5 predictors and 4 outcomes when fitted with ranks 1,2 or 3')
top=''
combined_plotsf1 <- grid.arrange(grobs = plot_listf1, ncol = 3, nrow = 1,top=top)

# Save the combined plot grid as a PNG
ggsave("combined_plotss2p1.png", plot = combined_plotsf1, width = 23, height = 6, dpi = 300)


plot_listf1 <- list()
plot_listf1[[1]]=r21a$pL3
plot_listf1[[2]]=r22a$pL3
plot_listf1[[3]]=r23a$pL3


top=paste('Histograms of mean bias per outcome for 200 simulated under rank 2 datasets with 1000 individuals, 5 predictors and 4 outcomes when fitted with ranks 1,2 or 3')
top=''
combined_plotsf1 <- grid.arrange(grobs = plot_listf1, ncol = 3, nrow = 1,top=top)

# Save the combined plot grid as a PNG
ggsave("combined_plotss2p2.png", plot = combined_plotsf1, width = 23, height = 6, dpi = 300)


plot_listf1 <- list()
plot_listf1[[1]]=r21a$pL4
plot_listf1[[2]]=r22a$pL4
plot_listf1[[3]]=r23a$pL4


top=paste('Histograms of mean MSE per outcome for 200 simulated under rank 2 datasets with 1000 individuals, 5 predictors and 4 outcomes when fitted with ranks 1,2 or 3')
top=''
combined_plotsf1 <- grid.arrange(grobs = plot_listf1, ncol = 3, nrow = 1,top=top)

# Save the combined plot grid as a PNG
ggsave("combined_plotss2p3.png", plot = combined_plotsf1, width = 23, height = 6, dpi = 300)

```


### Heatmaps

```{r}
plot_listf1 <- list()
plot_listf1[[1]]=heatmap2_1_real
plot_listf1[[2]]=beta_heat_2_11
plot_listf1[[3]]=beta_heat_2_12
plot_listf1[[4]]=NULL
plot_listf1[[5]]=heatmap2_2_real
plot_listf1[[6]]=beta_heat_2_21
plot_listf1[[7]]=beta_heat_2_22
plot_listf1[[8]]=beta_heat_2_23

top=''

heatmaps_2 <- grid.arrange(grobs = plot_listf1, ncol = 4, nrow = 2,top=top)

# Save the combined plot grid as a PNG
ggsave("heatmaps_2.png", plot = heatmaps_2, width = 23, height = 6, dpi = 300)
```


#  Third Configuration (sample size:5000, p=5,K=4, n_simulations = 200)

## Call functions to simulate  n_simulations = 200 datasets using rank r=1 

```{r}
#set seed
set.seed(123)
#chose variables 
n = 5000   # Number of observations
p <- 5    # Number of covariates
k <- 4   # Number of outcomes
n_simulations = 200

r <- 1  # Rank of simulated data

datas1b = simulations_data(n,p,r,k,n_simulations)

Beta_matrix_1=datas1b$Beta_matrix
colnames(Beta_matrix_1) <- paste("k=", 1:ncol(Beta_matrix_1), sep="")
rownames(Beta_matrix_1) <- paste("X", 1:nrow(Beta_matrix_1), sep="")
beta_df <- reshape2::melt(Beta_matrix_1)
my_palette <- colorRampPalette(c("lightgrey", "orange", "black"))(100)
breaks <- seq(0.0050,  0.86, length.out = 101)
norm_breaks <- (breaks - min(breaks)) / (max(breaks) - min(breaks))
heatmap3_1_real <- ggplot(beta_df, aes(x = Var2, y = Var1, fill = value)) +
  geom_tile() +
  scale_fill_gradientn(colors = my_palette, 
                       values = norm_breaks, # specify normalized breaks here
                       limits = range(breaks)) + # Set limits to the min and max of your breaks
  theme_minimal() +
  labs(x = "", y = "", fill = "Value") +
  theme(     axis.text.x = element_text(angle = 90, hjust = 1, size = 15),       axis.text.y = element_text(size = 15),                              axis.title.x = element_text(size = 15),                            axis.title.y = element_text(size = 15),                            legend.text = element_text(size = 20),                              legend.title = element_text(size = 20),                            plot.title = element_text(size = 30),                strip.text = element_text(size = 20)                              )+
  ggtitle("Simulated under rank 1")
```

### Generated Data Statistics Plots

```{r}
third1=generate_data_plots(datas1b$dlong_list,5000,1,5,4)
```

### Fit simulated datasets with model of rank R2=1

#### Get performance
```{r}
#set seed
set.seed(123)

R2 = 1 #rank of fitted model

performance_results11b = simulations_fit_and_preformance(datas1b,p,k,n_simulations,R2)

```


#### Get Average Beta Heatmap
```{r}
beta_heat_3_11=beta_heatmap("Fitted under rank 1",datas1b, performance_results11b,0.0050,  0.86)
```


#### Plot performance
```{r}
r11b = generate_histograms(performance_results11b, 4,1)
```
#### ΜC ERRORS

```{r}
msesmc = unlist(lapply(performance_results11b$mse_for_all_coeffs_and_simulation, as.vector))
biasmc = unlist(lapply(performance_results11b$bias_for_all_coeffs_and_simulations, as.vector))
MC_errors11b = MC_errors_calc(msesmc,biasmc)
```

#### LP ERRORS

```{r}
LP_errorsthird11 = linear_predictor_err(datas1b, performance_results11b)

lp_errors_mc = unlist(lapply(LP_errorsthird11$lp_errors, as.vector))

MC_error_lp_third11=sd(lp_errors_mc)/sqrt(length(lp_errors_mc))
```

### Fit simulated datasets with model of rank R2=2

#### Get performance
```{r}
#set seed
set.seed(123)

R2 = 2 #rank of fitted model

performance_results12b = simulations_fit_and_preformance(datas1b,p,k,n_simulations,R2)

```


#### Get Average Beta Heatmap
```{r}
beta_heat_3_12=beta_heatmap("Fitted under rank 2",datas1b, performance_results12b,0.0050,  0.86)
```


#### Plot performance
```{r}
r12b = generate_histograms(performance_results12b, 4,2)
```
#### ΜC ERRORS

```{r}
msesmc = unlist(lapply(performance_results12b$mse_for_all_coeffs_and_simulation, as.vector))
biasmc = unlist(lapply(performance_results12b$bias_for_all_coeffs_and_simulations, as.vector))
MC_errors12b = MC_errors_calc(msesmc,biasmc)
```


#### LP ERRORS

```{r}
LP_errorsthird12 = linear_predictor_err(datas1b, performance_results12b)

lp_errors_mc = unlist(lapply(LP_errorsthird12$lp_errors, as.vector))

MC_error_lp_third12=sd(lp_errors_mc)/sqrt(length(lp_errors_mc))
```


### Fit simulated datasets with model of rank R2=3

#### Get performance
```{r}
#set seed
set.seed(123)

R2 = 3 #rank of fitted model

performance_results13b = simulations_fit_and_preformance(datas1b,p,k,n_simulations,R2)

```

#### Plot performance
```{r}
r13b = generate_histograms(performance_results13b, 4,3)
```


#### ΜC ERRORS

```{r}
msesmc = unlist(lapply(performance_results13b$mse_for_all_coeffs_and_simulation, as.vector))
biasmc = unlist(lapply(performance_results13b$bias_for_all_coeffs_and_simulations, as.vector))
MC_errors13b = MC_errors_calc(msesmc,biasmc)
```


#### LP ERRORS

```{r}
LP_errorsthird13 = linear_predictor_err(datas1b, performance_results13b)

lp_errors_mc = unlist(lapply(LP_errorsthird13$lp_errors, as.vector))

MC_error_lp_third13=sd(lp_errors_mc)/sqrt(length(lp_errors_mc))
```

#### Plots

```{r}
plot_listf1 <- list()
plot_listf1[[1]]=r11b$pL2
plot_listf1[[2]]=r12b$pL2
# plot_listf1[[3]]=r13b$pL2


top=paste('Histograms of mean absolute bias(left) and MSE(right) for 200 simulated under rank 1 datasets with 5000 individuals, 5 predictors and 4 outcomes when fitted with ranks 1,2 or 3')
top=''
combined_plotsf1 <- grid.arrange(grobs = plot_listf1, ncol = 2, nrow = 1,top=top)

# Save the combined plot grid as a PNG
ggsave("combined_plotst1p1.png", plot = combined_plotsf1, width = 23, height = 6, dpi = 300)


plot_listf1 <- list()
plot_listf1[[1]]=r11b$pL3
plot_listf1[[2]]=r12b$pL3
# plot_listf1[[3]]=r13b$pL3


top=paste('Histograms of mean bias per outcome for 200 simulated under rank 1 datasets with 5000 individuals, 5 predictors and 4 outcomes when fitted with ranks 1,2 or 3')
top=''
combined_plotsf1 <- grid.arrange(grobs = plot_listf1, ncol = 2, nrow = 1,top=top)

# Save the combined plot grid as a PNG
ggsave("combined_plotst1p2.png", plot = combined_plotsf1, width = 23, height = 6, dpi = 300)


plot_listf1 <- list()
plot_listf1[[1]]=r11b$pL4
plot_listf1[[2]]=r12b$pL4
# plot_listf1[[3]]=r13b$pL4


top=paste('Histograms of mean MSE per outcome for 200 simulated under rank 1 datasets with 5000 individuals, 5 predictors and 4 outcomes when fitted with ranks 1,2 or 3')
top=''
combined_plotsf1 <- grid.arrange(grobs = plot_listf1, ncol = 2, nrow = 1,top=top)

# Save the combined plot grid as a PNG
ggsave("combined_plotst1p3.png", plot = combined_plotsf1, width = 23, height = 6, dpi = 300)

```



## Call functions to simulate  n_simulations = 200 datasets using rank r=2 

```{r}
#set seed
set.seed(123)

r <- 2  # Rank of simulated data

datas2b = simulations_data(n,p,r,k,n_simulations)

Beta_matrix_1=datas2b$Beta_matrix
colnames(Beta_matrix_1) <- paste("k=", 1:ncol(Beta_matrix_1), sep="")
rownames(Beta_matrix_1) <- paste("X", 1:nrow(Beta_matrix_1), sep="")
beta_df <- reshape2::melt(Beta_matrix_1)
my_palette <- colorRampPalette(c("lightgrey", "orange", "black"))(100)
breaks <- seq(0.13, 1.02, length.out = 101)
norm_breaks <- (breaks - min(breaks)) / (max(breaks) - min(breaks))
heatmap3_2_real <- ggplot(beta_df, aes(x = Var2, y = Var1, fill = value)) +
  geom_tile() +
  scale_fill_gradientn(colors = my_palette, 
                       values = norm_breaks, # specify normalized breaks here
                       limits = range(breaks)) + # Set limits to the min and max of your breaks
  theme_minimal() +
  labs(x = "", y = "", fill = "Value") +
  theme(     axis.text.x = element_text(angle = 90, hjust = 1, size = 15),       axis.text.y = element_text(size = 15),                              axis.title.x = element_text(size = 15),                            axis.title.y = element_text(size = 15),                            legend.text = element_text(size = 20),                              legend.title = element_text(size = 20),                            plot.title = element_text(size = 30),                strip.text = element_text(size = 20)                              )+
  ggtitle("Simulated under rank 2")
```

### Generated Data Statistics Plots

```{r}
third2=generate_data_plots(datas2b$dlong_list,5000,2,5,4)
```

### Fit simulated datasets with model of rank R2=1

#### Get performance
```{r}
#set seed
set.seed(123)

R2 = 1 #rank of fitted model

performance_results21b = simulations_fit_and_preformance(datas2b,p,k,n_simulations,R2)

```

#### Get Average Beta Heatmap
```{r}
beta_heat_3_21=beta_heatmap("Fitted under rank 1",datas2b, performance_results21b,0.13, 1.02)
```


#### Plot performance
```{r}
r21b = generate_histograms(performance_results21b, 4,1)
```
#### ΜC ERRORS

```{r}
msesmc = unlist(lapply(performance_results21b$mse_for_all_coeffs_and_simulation, as.vector))
biasmc = unlist(lapply(performance_results21b$bias_for_all_coeffs_and_simulations, as.vector))
MC_errors21b = MC_errors_calc(msesmc,biasmc)
```


#### LP ERRORS

```{r}
LP_errorsthird21 = linear_predictor_err(datas2b, performance_results21b)

lp_errors_mc = unlist(lapply(LP_errorsthird21$lp_errors, as.vector))

MC_error_lp_third21=sd(lp_errors_mc)/sqrt(length(lp_errors_mc))
```


### Fit simulated datasets with model of rank R2=2

#### Get performance
```{r}
#set seed
set.seed(123)

R2 = 2 #rank of fitted model

performance_results22b = simulations_fit_and_preformance(datas2b, p,k,n_simulations,R2)

```


#### Get Average Beta Heatmap
```{r}
beta_heat_3_22=beta_heatmap("Fitted under rank 2",datas2b, performance_results22b,0.13, 1.02)
```


#### Plot performance
```{r}
r22b = generate_histograms(performance_results22b, 4,2)
```
#### ΜC ERRORS

```{r}
msesmc = unlist(lapply(performance_results22b$mse_for_all_coeffs_and_simulation, as.vector))
biasmc = unlist(lapply(performance_results22b$bias_for_all_coeffs_and_simulations, as.vector))
MC_errors22b = MC_errors_calc(msesmc,biasmc)
```


#### LP ERRORS

```{r}
LP_errorsthird22 = linear_predictor_err(datas2b, performance_results22b)

lp_errors_mc = unlist(lapply(LP_errorsthird22$lp_errors, as.vector))

MC_error_lp_third22=sd(lp_errors_mc)/sqrt(length(lp_errors_mc))
```


### Fit simulated datasets with model of rank R2=3

#### Get performance
```{r}
#set seed
set.seed(123)

R2 = 3 #rank of fitted model

performance_results23b = simulations_fit_and_preformance(datas2b, p,k,n_simulations,R2)

```


#### Get Average Beta Heatmap
```{r}
beta_heat_3_23=beta_heatmap("Fitted under rank 3",datas2b, performance_results23b,0.13, 1.02)
```


#### Plot performance
```{r}
r23b = generate_histograms(performance_results23b, 4,3)
```
#### ΜC ERRORS

```{r}
msesmc = unlist(lapply(performance_results23b$mse_for_all_coeffs_and_simulation, as.vector))
biasmc = unlist(lapply(performance_results23b$bias_for_all_coeffs_and_simulations, as.vector))
MC_errors23b = MC_errors_calc(msesmc,biasmc)
```


#### LP ERRORS

```{r}
LP_errorsthird23 = linear_predictor_err(datas2b, performance_results23b)

lp_errors_mc = unlist(lapply(LP_errorsthird23$lp_errors, as.vector))

MC_error_lp_third23=sd(lp_errors_mc)/sqrt(length(lp_errors_mc))
```

#### Plots

```{r}
plot_listf1 <- list()
plot_listf1[[1]]=r21b$pL2
plot_listf1[[2]]=r22b$pL2
plot_listf1[[3]]=r23b$pL2


top=paste('Histograms of mean absolute bias(left) and MSE(right) for 200 simulated under rank 2 datasets with 5000 individuals, 5 predictors and 4 outcomes when fitted with ranks 1,2 or 3')
top=''
combined_plotsf1 <- grid.arrange(grobs = plot_listf1, ncol = 3, nrow = 1,top=top)

# Save the combined plot grid as a PNG
ggsave("combined_plotst2p1.png", plot = combined_plotsf1, width = 16, height = 6, dpi = 300)


plot_listf1 <- list()
plot_listf1[[1]]=r21b$pL3
plot_listf1[[2]]=r22b$pL3
plot_listf1[[3]]=r23b$pL3


top=paste('Histograms of mean bias per outcome for 200 simulated under rank 2 datasets with 5000 individuals, 5 predictors and 4 outcomes when fitted with ranks 1,2 or 3')
top=''
combined_plotsf1 <- grid.arrange(grobs = plot_listf1, ncol = 3, nrow = 1,top=top)

# Save the combined plot grid as a PNG
ggsave("combined_plotst2p2.png", plot = combined_plotsf1, width = 16, height = 6, dpi = 300)


plot_listf1 <- list()
plot_listf1[[1]]=r21b$pL4
plot_listf1[[2]]=r22b$pL4
plot_listf1[[3]]=r23b$pL4


top=paste('Histograms of mean MSE per outcome for 200 simulated under rank 2 datasets with 5000 individuals, 5 predictors and 4 outcomes when fitted with ranks 1,2 or 3')
top=''
combined_plotsf1 <- grid.arrange(grobs = plot_listf1, ncol = 3, nrow = 1,top=top)

# Save the combined plot grid as a PNG
ggsave("combined_plotst2p3.png", plot = combined_plotsf1, width = 16, height = 6, dpi = 300)

```


### Heatmaps

```{r}
plot_listf1 <- list()
plot_listf1[[1]]=heatmap3_1_real
plot_listf1[[2]]=beta_heat_3_11
plot_listf1[[3]]=beta_heat_3_12
plot_listf1[[4]]=NULL
plot_listf1[[5]]=heatmap3_2_real
plot_listf1[[6]]=beta_heat_3_21
plot_listf1[[7]]=beta_heat_3_22
plot_listf1[[8]]=beta_heat_3_23

top=''

heatmaps_3 <- grid.arrange(grobs = plot_listf1, ncol = 4, nrow = 2,top=top)

# Save the combined plot grid as a PNG
ggsave("heatmaps_3.png", plot = heatmaps_3, width = 23, height = 6, dpi = 300)
```


#  Fourth Configuration (sample size:500, p=10,K=4, n_simulations = 200)

## Call functions to simulate  n_simulations = 200 datasets using rank r=1 

```{r}
#set seed
set.seed(123)
#chose variables 
n = 500 # Number of observations
p <- 10  # Number of covariates
k <- 4   # Number of outcomes
 n_simulations = 200

r <- 1  # Rank of simulated data

datas1c = simulations_data(n,p,r,k,n_simulations)

Beta_matrix_1=datas1c$Beta_matrix
colnames(Beta_matrix_1) <- paste("k=", 1:ncol(Beta_matrix_1), sep="")
rownames(Beta_matrix_1) <- paste("X", 1:nrow(Beta_matrix_1), sep="")
beta_df <- reshape2::melt(Beta_matrix_1)
my_palette <- colorRampPalette(c("lightgrey", "orange", "black"))(100)
breaks <- seq(0.017,0.914, length.out = 101)
norm_breaks <- (breaks - min(breaks)) / (max(breaks) - min(breaks))
heatmap4_1_real <- ggplot(beta_df, aes(x = Var2, y = Var1, fill = value)) +
  geom_tile() +
  scale_fill_gradientn(colors = my_palette, 
                       values = norm_breaks, # specify normalized breaks here
                       limits = range(breaks)) + # Set limits to the min and max of your breaks
  theme_minimal() +
  labs(x = "", y = "", fill = "Value") +
  theme(     axis.text.x = element_text(angle = 90, hjust = 1, size = 15),       axis.text.y = element_text(size = 15),                              axis.title.x = element_text(size = 15),                            axis.title.y = element_text(size = 15),                            legend.text = element_text(size = 20),                              legend.title = element_text(size = 20),                            plot.title = element_text(size = 30),                strip.text = element_text(size = 20)                              )+
  ggtitle("Simulated under rank 1")
```

### Generated Data Statistics Plots

```{r}
fourth1= generate_data_plots(datas1c$dlong_list,500,1,10,4)
```

### Fit simulated datasets with model of rank R2=1

#### Get performance
```{r}
#set seed
set.seed(123)

R2 = 1 #rank of fitted model

performance_results11c = simulations_fit_and_preformance(datas1c, p,k,n_simulations,R2)

```


#### Get Average Beta Heatmap
```{r}
beta_heat_4_11=beta_heatmap("Fitted under rank 1",datas1c, performance_results11c,0.017,0.914)
```


#### Plot performance
```{r}
r11c = generate_histograms(performance_results11c, 4,1)
```

#### ΜC ERRORS

```{r}
msesmc = unlist(lapply(performance_results11c$mse_for_all_coeffs_and_simulation, as.vector))
biasmc = unlist(lapply(performance_results11c$bias_for_all_coeffs_and_simulations, as.vector))
MC_errors11c = MC_errors_calc(msesmc,biasmc)
```



#### LP ERRORS

```{r}
LP_errorsfourth11 = linear_predictor_err(datas1c, performance_results11c)

lp_errors_mc = unlist(lapply(LP_errorsfourth11$lp_errors, as.vector))

MC_error_lp_fourth11=sd(lp_errors_mc)/sqrt(length(lp_errors_mc))
```

### Fit simulated datasets with model of rank R2=2

#### Get performance
```{r}
#set seed
set.seed(123)

R2 = 2 #rank of fitted model

performance_results12c = simulations_fit_and_preformance(datas1c, p,k,n_simulations,R2)

```


#### Get Average Beta Heatmap
```{r}
beta_heat_4_12=beta_heatmap("Fitted under rank 2",datas1c, performance_results12c,0.017,0.914)
```


#### Plot performance
```{r}
r12c = generate_histograms(performance_results12c, 4,2)
```


#### ΜC ERRORS

```{r}
msesmc = unlist(lapply(performance_results12c$mse_for_all_coeffs_and_simulation, as.vector))
biasmc = unlist(lapply(performance_results12c$bias_for_all_coeffs_and_simulations, as.vector))
MC_errors12c = MC_errors_calc(msesmc,biasmc)
```


#### LP ERRORS

```{r}
LP_errorsfourth12 = linear_predictor_err(datas1c, performance_results12c)

lp_errors_mc = unlist(lapply(LP_errorsfourth12$lp_errors, as.vector))

MC_error_lp_fourth12=sd(lp_errors_mc)/sqrt(length(lp_errors_mc))
```


### Fit simulated datasets with model of rank R2=3

#### Get performance
```{r}
#set seed
set.seed(123)

R2 = 3 #rank of fitted model

performance_results13c = simulations_fit_and_preformance(datas1c, p,k,n_simulations,R2)

```

#### Plot performance
```{r}
r13c = generate_histograms(performance_results13c, 4,3)
```


#### ΜC ERRORS

```{r}
msesmc = unlist(lapply(performance_results13c$mse_for_all_coeffs_and_simulation, as.vector))
biasmc = unlist(lapply(performance_results13c$bias_for_all_coeffs_and_simulations, as.vector))
MC_errors13c = MC_errors_calc(msesmc,biasmc)
```


#### LP ERRORS

```{r}
LP_errorsfourth13 = linear_predictor_err(datas1c, performance_results13c)

lp_errors_mc = unlist(lapply(LP_errorsfourth13$lp_errors, as.vector))

MC_error_lp_fourth13=sd(lp_errors_mc)/sqrt(length(lp_errors_mc))
```


#### Plots

```{r}
plot_listf1 <- list()
plot_listf1[[1]]=r11c$pL2
plot_listf1[[2]]=r12c$pL2
# plot_listf1[[3]]=r13c$pL2


top=paste('Histograms of mean absolute bias(left) and MSE(right) for 200 simulated under rank 1 datasets with 500 individuals, 10 predictors and 4 outcomes when fitted with ranks 1,2 or 3')
top=''
combined_plotsf1 <- grid.arrange(grobs = plot_listf1, ncol = 2, nrow = 1,top=top)

# Save the combined plot grid as a PNG
ggsave("combined_plotsfo1p1.png", plot = combined_plotsf1, width = 23, height = 6, dpi = 300)


plot_listf1 <- list()
plot_listf1[[1]]=r11c$pL3
plot_listf1[[2]]=r12c$pL3
# plot_listf1[[3]]=r13c$pL3


top=paste('Histograms of mean bias per outcome for 200 simulated under rank 1 datasets with 500 individuals, 10 predictors and 4 outcomes when fitted with ranks 1,2 or 3')
top=''
combined_plotsf1 <- grid.arrange(grobs = plot_listf1, ncol = 2, nrow = 1,top=top)

# Save the combined plot grid as a PNG
ggsave("combined_plotsfo1p2.png", plot = combined_plotsf1, width = 23, height = 6, dpi = 300)


plot_listf1 <- list()
plot_listf1[[1]]=r11c$pL4
plot_listf1[[2]]=r12c$pL4
# plot_listf1[[3]]=r13c$pL4


top=paste('Histograms of mean MSE per outcome for 200 simulated under rank 1 datasets with 500 individuals, 10 predictors and 4 outcomes when fitted with ranks 1,2 or 3')
top=''
combined_plotsf1 <- grid.arrange(grobs = plot_listf1, ncol = 2, nrow = 1,top=top)

# Save the combined plot grid as a PNG
ggsave("combined_plotsfo1p3.png", plot = combined_plotsf1, width = 23, height = 6, dpi = 300)

```


## Call functions to simulate  n_simulations = 200 datasets using rank r=2 

```{r}
#set seed
set.seed(123)

r <- 2  # Rank of simulated data

datas2c = simulations_data(n,p,r,k,n_simulations)

Beta_matrix_1=datas2c$Beta_matrix
colnames(Beta_matrix_1) <- paste("k=", 1:ncol(Beta_matrix_1), sep="")
rownames(Beta_matrix_1) <- paste("X", 1:nrow(Beta_matrix_1), sep="")
beta_df <- reshape2::melt(Beta_matrix_1)
my_palette <- colorRampPalette(c("lightgrey", "orange", "black"))(100)
breaks <- seq(0.470,1.247, length.out = 101)
norm_breaks <- (breaks - min(breaks)) / (max(breaks) - min(breaks))
heatmap4_2_real <- ggplot(beta_df, aes(x = Var2, y = Var1, fill = value)) +
  geom_tile() +
  scale_fill_gradientn(colors = my_palette, 
                       values = norm_breaks, # specify normalized breaks here
                       limits = range(breaks)) + # Set limits to the min and max of your breaks
  theme_minimal() +
  labs(x = "", y = "", fill = "Value") +
  theme(     axis.text.x = element_text(angle = 90, hjust = 1, size = 15),       axis.text.y = element_text(size = 15),                              axis.title.x = element_text(size = 15),                            axis.title.y = element_text(size = 15),                            legend.text = element_text(size = 20),                              legend.title = element_text(size = 20),                            plot.title = element_text(size = 30),                strip.text = element_text(size = 20)                              )+
  ggtitle("Simulated under rank 2")
```

### Generated Data Statistics Plots

```{r}
fourth2 = generate_data_plots(datas2c$dlong_list,500,2,10,4)
```

### Fit simulated datasets with model of rank R2=1

#### Get performance
```{r}
#set seed
set.seed(123)

R2 = 1 #rank of fitted model

performance_results21c = simulations_fit_and_preformance(datas2c, p,k,n_simulations,R2)

```


#### Get Average Beta Heatmap
```{r}
beta_heat_4_21=beta_heatmap("Fitted under rank 1",datas2c, performance_results21c,0.470,1.247)
```


#### Plot performance
```{r}
r21c = generate_histograms(performance_results21c, 4,1)
```


#### ΜC ERRORS

```{r}
msesmc = unlist(lapply(performance_results21c$mse_for_all_coeffs_and_simulation, as.vector))
biasmc = unlist(lapply(performance_results21c$bias_for_all_coeffs_and_simulations, as.vector))
MC_errors21c = MC_errors_calc(msesmc,biasmc)
```

#### LP ERRORS

```{r}
LP_errorsfourth21 = linear_predictor_err(datas2c, performance_results21c)

lp_errors_mc = unlist(lapply(LP_errorsfourth21$lp_errors, as.vector))

MC_error_lp_fourth21=sd(lp_errors_mc)/sqrt(length(lp_errors_mc))
```

### Fit simulated datasets with model of rank R2=2

#### Get performance
```{r}
#set seed
set.seed(123)

R2 = 2 #rank of fitted model

performance_results22c = simulations_fit_and_preformance(datas2c, p,k,n_simulations,R2)

```


#### Get Average Beta Heatmap
```{r}
beta_heat_4_22=beta_heatmap("Fitted under rank 2",datas2c, performance_results22c,0.470,1.247)
```


#### Plot performance
```{r}
r22c = generate_histograms(performance_results22c, 4,2)
```

#### ΜC ERRORS

```{r}
msesmc = unlist(lapply(performance_results22c$mse_for_all_coeffs_and_simulation, as.vector))
biasmc = unlist(lapply(performance_results22c$bias_for_all_coeffs_and_simulations, as.vector))
MC_errors22c = MC_errors_calc(msesmc,biasmc)
```


#### LP ERRORS

```{r}
LP_errorsfourth22 = linear_predictor_err(datas2c, performance_results22c)

lp_errors_mc = unlist(lapply(LP_errorsfourth22$lp_errors, as.vector))

MC_error_lp_fourth22=sd(lp_errors_mc)/sqrt(length(lp_errors_mc))
```

### Fit simulated datasets with model of rank R2=3

#### Get performance
```{r}
#set seed
set.seed(123)

R2 = 3 #rank of fitted model

performance_results23c = simulations_fit_and_preformance(datas2c, p,k,n_simulations,R2)

```


#### Get Average Beta Heatmap
```{r}
beta_heat_4_23=beta_heatmap("Fitted under rank 3",datas2c, performance_results23c,0.470,1.247)
```


#### Plot performance
```{r}
r23c = generate_histograms(performance_results23c, 4,3)
```


#### ΜC ERRORS

```{r}
msesmc = unlist(lapply(performance_results23c$mse_for_all_coeffs_and_simulation, as.vector))
biasmc = unlist(lapply(performance_results23c$bias_for_all_coeffs_and_simulations, as.vector))
MC_errors23c = MC_errors_calc(msesmc,biasmc)
```


#### LP ERRORS

```{r}
LP_errorsfourth23 = linear_predictor_err(datas2c, performance_results23c)

lp_errors_mc = unlist(lapply(LP_errorsfourth23$lp_errors, as.vector))

MC_error_lp_fourth23=sd(lp_errors_mc)/sqrt(length(lp_errors_mc))
```


#### Plots

```{r}
plot_listf1 <- list()
plot_listf1[[1]]=r21c$pL2
plot_listf1[[2]]=r22c$pL2
plot_listf1[[3]]=r23c$pL2


top=paste('Histograms of mean absolute bias(left) and MSE(right) for 200 simulated under rank 2 datasets with 500 individuals, 10 predictors and 4 outcomes when fitted with ranks 1,2 or 3')
top=''
combined_plotsf1 <- grid.arrange(grobs = plot_listf1, ncol = 3, nrow = 1,top=top)

# Save the combined plot grid as a PNG
ggsave("combined_plotsfo2p1.png", plot = combined_plotsf1, width = 23, height = 6, dpi = 300)


plot_listf1 <- list()
plot_listf1[[1]]=r21c$pL3
plot_listf1[[2]]=r22c$pL3
plot_listf1[[3]]=r23c$pL3


top=paste('Histograms of mean bias per outcome for 200 simulated under rank 2 datasets with 500 individuals, 10 predictors and 4 outcomes when fitted with ranks 1,2 or 3')
top=''
combined_plotsf1 <- grid.arrange(grobs = plot_listf1, ncol = 3, nrow = 1,top=top)

# Save the combined plot grid as a PNG
ggsave("combined_plotsfo2p2.png", plot = combined_plotsf1, width = 23, height = 6, dpi = 300)


plot_listf1 <- list()
plot_listf1[[1]]=r21c$pL4
plot_listf1[[2]]=r22c$pL4
plot_listf1[[3]]=r23c$pL4


top=paste('Histograms of mean MSE per outcome for 200 simulated under rank 2 datasets with 500 individuals, 10 predictors and 4 outcomes when fitted with ranks 1,2 or 3')
top=''
combined_plotsf1 <- grid.arrange(grobs = plot_listf1, ncol = 3, nrow = 1,top=top)

# Save the combined plot grid as a PNG
ggsave("combined_plotsfo2p3.png", plot = combined_plotsf1, width = 23, height = 6, dpi = 300)

```


### Heatmaps

```{r}
plot_listf1 <- list()
plot_listf1[[1]]=heatmap4_1_real
plot_listf1[[2]]=beta_heat_4_11
plot_listf1[[3]]=beta_heat_4_12
plot_listf1[[4]]=NULL
plot_listf1[[5]]=heatmap4_2_real
plot_listf1[[6]]=beta_heat_4_21
plot_listf1[[7]]=beta_heat_4_22
plot_listf1[[8]]=beta_heat_4_23

top=''

heatmaps_4 <- grid.arrange(grobs = plot_listf1, ncol = 4, nrow = 2,top=top)

# Save the combined plot grid as a PNG
ggsave("heatmaps_4.png", plot = heatmaps_4, width = 23, height = 6, dpi = 300)
```


# Fifth Configuration (sample size:1000, p=10,K=4, n_simulations = 200)

## Call functions to simulate  n_simulations = 200 datasets using rank r=1 

```{r}
#set seed
set.seed(123)
#chose variables 
n = 1000   # Number of observations
p <- 10    # Number of covariates
k <- 4   # Number of outcomes
 n_simulations = 200

r <- 1  # Rank of simulated data

datas1d = simulations_data(n,p,r,k,n_simulations)

Beta_matrix_1=datas1d$Beta_matrix
colnames(Beta_matrix_1) <- paste("k=", 1:ncol(Beta_matrix_1), sep="")
rownames(Beta_matrix_1) <- paste("X", 1:nrow(Beta_matrix_1), sep="")
beta_df <- reshape2::melt(Beta_matrix_1)
my_palette <- colorRampPalette(c("lightgrey", "orange", "black"))(100)
breaks <- seq(0.017,0.914, length.out = 101)
norm_breaks <- (breaks - min(breaks)) / (max(breaks) - min(breaks))
heatmap5_1_real <- ggplot(beta_df, aes(x = Var2, y = Var1, fill = value)) +
  geom_tile() +
  scale_fill_gradientn(colors = my_palette, 
                       values = norm_breaks, # specify normalized breaks here
                       limits = range(breaks)) + # Set limits to the min and max of your breaks
  theme_minimal() +
  labs(x = "", y = "", fill = "Value") +
  theme(     axis.text.x = element_text(angle = 90, hjust = 1, size = 15),       axis.text.y = element_text(size = 15),                              axis.title.x = element_text(size = 15),                            axis.title.y = element_text(size = 15),                            legend.text = element_text(size = 20),                              legend.title = element_text(size = 20),                            plot.title = element_text(size = 30),                strip.text = element_text(size = 20)                              )+
  ggtitle("Simulated under rank 1")
```

### Generated Data Statistics Plots

```{r}
fifth1 = generate_data_plots(datas1d$dlong_list,1000,1,10,4)
```

### Fit simulated datasets with model of rank R2=1

#### Get performance
```{r}
#set seed
set.seed(123)

R2 = 1 #rank of fitted model

performance_results11d = simulations_fit_and_preformance(datas1d, p,k,n_simulations,R2)

```


#### Get Average Beta Heatmap
```{r}
beta_heat_5_11=beta_heatmap("Fitted under rank 1",datas1d, performance_results11d,0.017,0.914)
```


#### Plot performance
```{r}
r11d = generate_histograms(performance_results11d, 4,1)
```


#### ΜC ERRORS

```{r}
msesmc = unlist(lapply(performance_results11d$mse_for_all_coeffs_and_simulation, as.vector))
biasmc = unlist(lapply(performance_results11d$bias_for_all_coeffs_and_simulations, as.vector))
MC_errors11d = MC_errors_calc(msesmc,biasmc)
```



#### LP ERRORS

```{r}
LP_errorsfifth11 = linear_predictor_err(datas1d, performance_results11d)

lp_errors_mc = unlist(lapply(LP_errorsfifth11$lp_errors, as.vector))

MC_error_lp_fifth11=sd(lp_errors_mc)/sqrt(length(lp_errors_mc))
```


### Fit simulated datasets with model of rank R2=2

#### Get performance
```{r}
#set seed
set.seed(123)

R2 = 2 #rank of fitted model

performance_results12d = simulations_fit_and_preformance(datas1d, p,k,n_simulations,R2)

```


#### Get Average Beta Heatmap
```{r}
beta_heat_5_12=beta_heatmap("Fitted under rank 2",datas1d, performance_results12d,0.017,0.914)
```

#### Plot performance
```{r}
r12d = generate_histograms(performance_results12d,4,2)
```

#### ΜC ERRORS

```{r}
msesmc = unlist(lapply(performance_results12d$mse_for_all_coeffs_and_simulation, as.vector))
biasmc = unlist(lapply(performance_results12d$bias_for_all_coeffs_and_simulations, as.vector))
MC_errors12d = MC_errors_calc(msesmc,biasmc)
```


#### LP ERRORS

```{r}
LP_errorsfifth12 = linear_predictor_err(datas1d, performance_results12d)

lp_errors_mc = unlist(lapply(LP_errorsfifth12$lp_errors, as.vector))

MC_error_lp_fifth12=sd(lp_errors_mc)/sqrt(length(lp_errors_mc))
```


### Fit simulated datasets with model of rank R2=3

#### Get performance
```{r}
#set seed
set.seed(123)

R2 = 3 #rank of fitted model

performance_results13d = simulations_fit_and_preformance(datas1d, p,k,n_simulations,R2)

```

#### Plot performance
```{r}
r13d = generate_histograms(performance_results13d, 4,3)
```


#### ΜC ERRORS

```{r}
msesmc = unlist(lapply(performance_results13d$mse_for_all_coeffs_and_simulation, as.vector))
biasmc = unlist(lapply(performance_results13d$bias_for_all_coeffs_and_simulations, as.vector))
MC_errors13d = MC_errors_calc(msesmc,biasmc)
```

#### LP ERRORS

```{r}
LP_errorsfifth13 = linear_predictor_err(datas1d, performance_results13d)

lp_errors_mc = unlist(lapply(LP_errorsfifth13$lp_errors, as.vector))

MC_error_lp_fifth13=sd(lp_errors_mc)/sqrt(length(lp_errors_mc))
```

#### Plots

```{r}
plot_listf1 <- list()
plot_listf1[[1]]=r11d$pL2
plot_listf1[[2]]=r12d$pL2
# plot_listf1[[3]]=r13d$pL2


top=paste('Histograms of mean absolute bias(left) and MSE(right) for 200 simulated under rank 1 datasets with 1000 individuals, 10 predictors and 4 outcomes when fitted with ranks 1,2 or 3')
top=''
combined_plotsf1 <- grid.arrange(grobs = plot_listf1, ncol = 2, nrow = 1,top=top)

# Save the combined plot grid as a PNG
ggsave("combined_plotsfi1p1.png", plot = combined_plotsf1, width = 23, height = 6, dpi = 300)


plot_listf1 <- list()
plot_listf1[[1]]=r11d$pL3
plot_listf1[[2]]=r12d$pL3
# plot_listf1[[3]]=r13d$pL3


top=paste('Histograms of mean bias per outcome for 200 simulated under rank 1 datasets with 1000 individuals, 10 predictors and 4 outcomes when fitted with ranks 1,2 or 3')
top=''
combined_plotsf1 <- grid.arrange(grobs = plot_listf1, ncol = 2, nrow = 1,top=top)

# Save the combined plot grid as a PNG
ggsave("combined_plotsfi1p2.png", plot = combined_plotsf1, width = 23, height = 6, dpi = 300)


plot_listf1 <- list()
plot_listf1[[1]]=r11d$pL4
plot_listf1[[2]]=r12d$pL4
# plot_listf1[[3]]=r13d$pL4


top=paste('Histograms of mean MSE per outcome for 200 simulated under rank 1 datasets with 1000 individuals, 10 predictors and 4 outcomes when fitted with ranks 1,2 or 3')
top=''
combined_plotsf1 <- grid.arrange(grobs = plot_listf1, ncol = 2, nrow = 1,top=top)

# Save the combined plot grid as a PNG
ggsave("combined_plotsfi1p3.png", plot = combined_plotsf1, width = 23, height = 6, dpi = 300)

```


## Call functions to simulate  n_simulations = 200 datasets using rank r=2 

```{r}
#set seed
set.seed(123)

r <- 2  # Rank of simulated data

datas2d = simulations_data(n,p,r,k,n_simulations)

Beta_matrix_1=datas2d$Beta_matrix
colnames(Beta_matrix_1) <- paste("k=", 1:ncol(Beta_matrix_1), sep="")
rownames(Beta_matrix_1) <- paste("X", 1:nrow(Beta_matrix_1), sep="")
beta_df <- reshape2::melt(Beta_matrix_1)
my_palette <- colorRampPalette(c("lightgrey", "orange", "black"))(100)
breaks <- seq(0.470,1.247, length.out = 101)
norm_breaks <- (breaks - min(breaks)) / (max(breaks) - min(breaks))
heatmap5_2_real <- ggplot(beta_df, aes(x = Var2, y = Var1, fill = value)) +
  geom_tile() +
  scale_fill_gradientn(colors = my_palette, 
                       values = norm_breaks, # specify normalized breaks here
                       limits = range(breaks)) + # Set limits to the min and max of your breaks
  theme_minimal() +
  labs(x = "", y = "", fill = "Value") +
  theme(     axis.text.x = element_text(angle = 90, hjust = 1, size = 15),       axis.text.y = element_text(size = 15),                              axis.title.x = element_text(size = 15),                            axis.title.y = element_text(size = 15),                            legend.text = element_text(size = 20),                              legend.title = element_text(size = 20),                            plot.title = element_text(size = 30),                strip.text = element_text(size = 20)                              )+
  ggtitle("Simulated under rank 2")
```

### Generated Data Statistics Plots

```{r}
fifth2 = generate_data_plots(datas2d$dlong_list,1000,2,10,4)
```

### Fit simulated datasets with model of rank R2=1

#### Get performance
```{r}
#set seed
set.seed(123)

R2 = 1 #rank of fitted model

performance_results21d = simulations_fit_and_preformance(datas2d, p,k,n_simulations,R2)

```



#### Get Average Beta Heatmap
```{r}
beta_heat_5_21=beta_heatmap("Fitted under rank 1",datas2d, performance_results21d,0.470,1.247)
```


#### Plot performance
```{r}
r21d = generate_histograms(performance_results21d, 4,1)
```


#### ΜC ERRORS

```{r}
msesmc = unlist(lapply(performance_results21d$mse_for_all_coeffs_and_simulation, as.vector))
biasmc = unlist(lapply(performance_results21d$bias_for_all_coeffs_and_simulations, as.vector))
MC_errors21d = MC_errors_calc(msesmc,biasmc)
```


#### LP ERRORS

```{r}
LP_errorsfifth21 = linear_predictor_err(datas2d, performance_results21d)

lp_errors_mc = unlist(lapply(LP_errorsfifth21$lp_errors, as.vector))

MC_error_lp_fifth21=sd(lp_errors_mc)/sqrt(length(lp_errors_mc))
```

### Fit simulated datasets with model of rank R2=2

#### Get performance
```{r}
#set seed
set.seed(123)

R2 = 2 #rank of fitted model

performance_results22d = simulations_fit_and_preformance(datas2d, p,k,n_simulations,R2)

```

#### Get Average Beta Heatmap
```{r}
beta_heat_5_22=beta_heatmap("Fitted under rank 2",datas2d, performance_results22d,0.470,1.247)
```

#### Plot performance
```{r}
r22d = generate_histograms(performance_results22d,4,2)
```

#### ΜC ERRORS

```{r}
msesmc = unlist(lapply(performance_results22d$mse_for_all_coeffs_and_simulation, as.vector))
biasmc = unlist(lapply(performance_results22d$bias_for_all_coeffs_and_simulations, as.vector))
MC_errors22d = MC_errors_calc(msesmc,biasmc)
```


#### LP ERRORS

```{r}
LP_errorsfifth22 = linear_predictor_err(datas2d, performance_results22d)

lp_errors_mc = unlist(lapply(LP_errorsfifth22$lp_errors, as.vector))

MC_error_lp_fifth22=sd(lp_errors_mc)/sqrt(length(lp_errors_mc))
```

### Fit simulated datasets with model of rank R2=3

#### Get performance
```{r}
#set seed
set.seed(123)

R2 = 3 #rank of fitted model

performance_results23d = simulations_fit_and_preformance(datas2d, p,k,n_simulations,R2)

```

#### Get Average Beta Heatmap
```{r}
beta_heat_5_23=beta_heatmap("Fitted under rank 3",datas2d, performance_results23d,0.470,1.247)
```

#### Plot performance
```{r}
r23d = generate_histograms(performance_results23d, 4,3)
```


#### ΜC ERRORS

```{r}
msesmc = unlist(lapply(performance_results23d$mse_for_all_coeffs_and_simulation, as.vector))
biasmc = unlist(lapply(performance_results23d$bias_for_all_coeffs_and_simulations, as.vector))
MC_errors23d = MC_errors_calc(msesmc,biasmc)
```


#### LP ERRORS

```{r}
LP_errorsfifth23 = linear_predictor_err(datas2d, performance_results23d)

lp_errors_mc = unlist(lapply(LP_errorsfifth23$lp_errors, as.vector))

MC_error_lp_fifth23=sd(lp_errors_mc)/sqrt(length(lp_errors_mc))
```

#### Plots

```{r}
plot_listf1 <- list()
plot_listf1[[1]]=r21d$pL2
plot_listf1[[2]]=r22d$pL2
plot_listf1[[3]]=r23d$pL2


top=paste('Histograms of mean absolute bias(left) and MSE(right) for 200 simulated under rank 2 datasets with 1000 individuals, 10 predictors and 4 outcomes when fitted with ranks 1,2 or 3')
top=''
combined_plotsf1 <- grid.arrange(grobs = plot_listf1, ncol = 3, nrow = 1,top=top)

# Save the combined plot grid as a PNG
ggsave("combined_plotsfi2p1.png", plot = combined_plotsf1, width = 23, height = 6, dpi = 300)


plot_listf1 <- list()
plot_listf1[[1]]=r21d$pL3
plot_listf1[[2]]=r22d$pL3
plot_listf1[[3]]=r23d$pL3


top=paste('Histograms of mean bias per outcome for 200 simulated under rank 2 datasets with 1000 individuals, 10 predictors and 4 outcomes when fitted with ranks 1,2 or 3')
top=''
combined_plotsf1 <- grid.arrange(grobs = plot_listf1, ncol = 3, nrow = 1,top=top)

# Save the combined plot grid as a PNG
ggsave("combined_plotsfi2p2.png", plot = combined_plotsf1, width = 23, height = 6, dpi = 300)


plot_listf1 <- list()
plot_listf1[[1]]=r21d$pL4
plot_listf1[[2]]=r22d$pL4
plot_listf1[[3]]=r23d$pL4


top=paste('Histograms of mean MSE per outcome for 200 simulated under rank 2 datasets with 1000 individuals, 10 predictors and 4 outcomes when fitted with ranks 1,2 or 3')
top=''
combined_plotsf1 <- grid.arrange(grobs = plot_listf1, ncol = 3, nrow = 1,top=top)

# Save the combined plot grid as a PNG
ggsave("combined_plotsfi2p3.png", plot = combined_plotsf1, width = 23, height = 6, dpi = 300)

```

### Heatmaps

```{r}
plot_listf1 <- list()
plot_listf1[[1]]=heatmap5_1_real
plot_listf1[[2]]=beta_heat_5_11
plot_listf1[[3]]=beta_heat_5_12
plot_listf1[[4]]=NULL
plot_listf1[[5]]=heatmap5_2_real
plot_listf1[[6]]=beta_heat_5_21
plot_listf1[[7]]=beta_heat_5_22
plot_listf1[[8]]=beta_heat_5_23

top=''

heatmaps_5 <- grid.arrange(grobs = plot_listf1, ncol = 4, nrow = 2,top=top)

# Save the combined plot grid as a PNG
ggsave("heatmaps_5.png", plot = heatmaps_5, width = 23, height = 6, dpi = 300)
```

# Sixth Configuration (sample size:5000, p=10,K=4, n_simulations = 200)

## Call functions to simulate  n_simulations = 200 datasets using rank r=1 


```{r}
#set seed
set.seed(123)
#chose variables 
n = 5000  # Number of observations
p <- 10    # Number of covariates
k <- 4  # Number of outcomes
 n_simulations = 200

r <- 1  # Rank of simulated data

datas1e = simulations_data(n,p,r,k,n_simulations)

Beta_matrix_1=datas1e$Beta_matrix
colnames(Beta_matrix_1) <- paste("k=", 1:ncol(Beta_matrix_1), sep="")
rownames(Beta_matrix_1) <- paste("X", 1:nrow(Beta_matrix_1), sep="")
beta_df <- reshape2::melt(Beta_matrix_1)
my_palette <- colorRampPalette(c("lightgrey", "orange", "black"))(100)
breaks <- seq(0.017,0.914, length.out = 101)
norm_breaks <- (breaks - min(breaks)) / (max(breaks) - min(breaks))
heatmap6_1_real <- ggplot(beta_df, aes(x = Var2, y = Var1, fill = value)) +
  geom_tile() +
  scale_fill_gradientn(colors = my_palette, 
                       values = norm_breaks, # specify normalized breaks here
                       limits = range(breaks)) + # Set limits to the min and max of your breaks
  theme_minimal() +
  labs(x = "", y = "", fill = "Value") +
  theme(     axis.text.x = element_text(angle = 90, hjust = 1, size = 15),       axis.text.y = element_text(size = 15),                              axis.title.x = element_text(size = 15),                            axis.title.y = element_text(size = 15),                            legend.text = element_text(size = 20),                              legend.title = element_text(size = 20),                            plot.title = element_text(size = 30),                strip.text = element_text(size = 20)                              )+
  ggtitle("Simulated under rank 1")
```

### Generated Data Statistics Plots

```{r}
sixth1 = generate_data_plots(datas1e$dlong_list,5000,1,10,4)
```

### Fit simulated datasets with model of rank R2=1

#### Get performance
```{r}
#set seed
set.seed(123)

R2 = 1 #rank of fitted model

performance_results11e = simulations_fit_and_preformance(datas1e, p,k,n_simulations,R2)

```


#### Get Average Beta Heatmap
```{r}
beta_heat_6_11=beta_heatmap("Fitted under rank 1",datas1e, performance_results11e,0.017,0.914)
```

#### Plot performance
```{r}
r11e = generate_histograms(performance_results11e,4,1)
```

#### ΜC ERRORS

```{r}
msesmc = unlist(lapply(performance_results11e$mse_for_all_coeffs_and_simulation, as.vector))
biasmc = unlist(lapply(performance_results11e$bias_for_all_coeffs_and_simulations, as.vector))
MC_errors11e = MC_errors_calc(msesmc,biasmc)
```


#### LP ERRORS

```{r}
LP_errorssix11 = linear_predictor_err(datas1e, performance_results11e)

lp_errors_mc = unlist(lapply(LP_errorssix11$lp_errors, as.vector))

MC_error_lp_six11=sd(lp_errors_mc)/sqrt(length(lp_errors_mc))
```


### Fit simulated datasets with model of rank R2=2

#### Get performance
```{r}
#set seed
set.seed(123)

R2 = 2 #rank of fitted model

performance_results12e = simulations_fit_and_preformance(datas1e, p,k,n_simulations,R2)

```

#### Get Average Beta Heatmap
```{r}
beta_heat_6_12=beta_heatmap("Fitted under rank 2",datas1e, performance_results12e,0.017,0.914)
```


#### Plot performance
```{r}
r12e = generate_histograms(performance_results12e, 4,2)
```

#### ΜC ERRORS

```{r}
msesmc = unlist(lapply(performance_results12e$mse_for_all_coeffs_and_simulation, as.vector))
biasmc = unlist(lapply(performance_results12e$bias_for_all_coeffs_and_simulations, as.vector))
MC_errors12e = MC_errors_calc(msesmc,biasmc)
```


#### LP ERRORS

```{r}
LP_errorssix12 = linear_predictor_err(datas1e, performance_results12e)

lp_errors_mc = unlist(lapply(LP_errorssix12$lp_errors, as.vector))

MC_error_lp_six12=sd(lp_errors_mc)/sqrt(length(lp_errors_mc))
```


### Fit simulated datasets with model of rank R2=3

#### Get performance
```{r}
#set seed
set.seed(123)

R2 = 3 #rank of fitted model

performance_results13e = simulations_fit_and_preformance(datas1e, p,k,n_simulations,R2)

```

#### Plot performance
```{r}
r13e = generate_histograms(performance_results13e, 4,3)
```


#### ΜC ERRORS

```{r}
msesmc = unlist(lapply(performance_results13e$mse_for_all_coeffs_and_simulation, as.vector))
biasmc = unlist(lapply(performance_results13e$bias_for_all_coeffs_and_simulations, as.vector))
MC_errors13e = MC_errors_calc(msesmc,biasmc)
```


#### LP ERRORS

```{r}
LP_errorssix13 = linear_predictor_err(datas1e, performance_results12e)

lp_errors_mc = unlist(lapply(LP_errorssix13$lp_errors, as.vector))

MC_error_lp_six13=sd(lp_errors_mc)/sqrt(length(lp_errors_mc))
```


#### Plots

```{r}
plot_listf1 <- list()
plot_listf1[[1]]=r11e$pL2
plot_listf1[[2]]=r12e$pL2
# plot_listf1[[3]]=r13e$pL2


top=paste('Histograms of mean absolute bias(left) and MSE(right) for 200 simulated under rank 1 datasets with 5000 individuals, 10 predictors and 4 outcomes when fitted with ranks 1,2 or 3')
top=''
combined_plotsf1 <- grid.arrange(grobs = plot_listf1, ncol = 2, nrow = 1,top=top)

# Save the combined plot grid as a PNG
ggsave("combined_plotssix1p1.png", plot = combined_plotsf1, width = 23, height = 6, dpi = 300)


plot_listf1 <- list()
plot_listf1[[1]]=r11e$pL3
plot_listf1[[2]]=r12e$pL3
# plot_listf1[[3]]=r13e$pL3


top=paste('Histograms of mean bias per outcome for 200 simulated under rank 1 datasets with 5000 individuals, 10 predictors and 4 outcomes when fitted with ranks 1,2 or 3')
top=''
combined_plotsf1 <- grid.arrange(grobs = plot_listf1, ncol = 2, nrow = 1,top=top)

# Save the combined plot grid as a PNG
ggsave("combined_plotssix1p2.png", plot = combined_plotsf1, width = 23, height = 6, dpi = 300)


plot_listf1 <- list()
plot_listf1[[1]]=r11e$pL4
plot_listf1[[2]]=r12e$pL4
# plot_listf1[[3]]=r13e$pL4


top=paste('Histograms of mean MSE per outcome for 200 simulated under rank 1 datasets with 5000 individuals, 10 predictors and 4 outcomes when fitted with ranks 1,2 or 3')
top=''
combined_plotsf1 <- grid.arrange(grobs = plot_listf1, ncol = 2, nrow = 1,top=top)

# Save the combined plot grid as a PNG
ggsave("combined_plotssix1p3.png", plot = combined_plotsf1, width = 23, height = 6, dpi = 300)

```


## Call functions to simulate  n_simulations = 200 datasets using rank r=2 

```{r}
#set seed
set.seed(123)

r <- 2  # Rank of simulated data

datas2e = simulations_data(n,p,r,k,n_simulations)

Beta_matrix_1=datas2e$Beta_matrix
colnames(Beta_matrix_1) <- paste("k=", 1:ncol(Beta_matrix_1), sep="")
rownames(Beta_matrix_1) <- paste("X", 1:nrow(Beta_matrix_1), sep="")
beta_df <- reshape2::melt(Beta_matrix_1)
my_palette <- colorRampPalette(c("lightgrey", "orange", "black"))(100)
breaks <- seq(0.470,1.247, length.out = 101)
norm_breaks <- (breaks - min(breaks)) / (max(breaks) - min(breaks))
heatmap6_2_real <- ggplot(beta_df, aes(x = Var2, y = Var1, fill = value)) +
  geom_tile() +
  scale_fill_gradientn(colors = my_palette, 
                       values = norm_breaks, # specify normalized breaks here
                       limits = range(breaks)) + # Set limits to the min and max of your breaks
  theme_minimal() +
  labs(x = "", y = "", fill = "Value") +
  theme(     axis.text.x = element_text(angle = 90, hjust = 1, size = 15),       axis.text.y = element_text(size = 15),                              axis.title.x = element_text(size = 15),                            axis.title.y = element_text(size = 15),                            legend.text = element_text(size = 20),                              legend.title = element_text(size = 20),                            plot.title = element_text(size = 30),                strip.text = element_text(size = 20)                              )+
  ggtitle("Simulated under rank 2")
```

### Generated Data Statistics Plots

```{r}
sixth2 = generate_data_plots(datas2e$dlong_list,5000,2,10,4)
```

### Fit simulated datasets with model of rank R2=1

#### Get performance
```{r}
#set seed
set.seed(123)

R2 = 1 #rank of fitted model

performance_results21e = simulations_fit_and_preformance(datas2e, p,k,n_simulations,R2)

```


#### Get Average Beta Heatmap
```{r}
beta_heat_6_21=beta_heatmap("Fitted under rank 1",datas2e, performance_results21e,0.470,1.247)
```


#### Plot performance
```{r}
r21e = generate_histograms(performance_results21e, 4,1)
```


#### ΜC ERRORS

```{r}
msesmc = unlist(lapply(performance_results21e$mse_for_all_coeffs_and_simulation, as.vector))
biasmc = unlist(lapply(performance_results21e$bias_for_all_coeffs_and_simulations, as.vector))
MC_errors21e = MC_errors_calc(msesmc,biasmc)
```


#### LP ERRORS

```{r}
LP_errorssix21 = linear_predictor_err(datas2e, performance_results21e)

lp_errors_mc = unlist(lapply(LP_errorssix21$lp_errors, as.vector))

MC_error_lp_six21=sd(lp_errors_mc)/sqrt(length(lp_errors_mc))
```


### Fit simulated datasets with model of rank R2=2

#### Get performance
```{r}
#set seed
set.seed(123)

R2 = 2 #rank of fitted model

performance_results22e = simulations_fit_and_preformance(datas2e, p,k,n_simulations,R2)

```

#### Get Average Beta Heatmap
```{r}
beta_heat_6_22=beta_heatmap("Fitted under rank 2",datas2e, performance_results22e,0.470,1.247)
```



#### Plot performance
```{r}
r22e = generate_histograms(performance_results22e,4,2)
```

#### ΜC ERRORS

```{r}
msesmc = unlist(lapply(performance_results22e$mse_for_all_coeffs_and_simulation, as.vector))
biasmc = unlist(lapply(performance_results22e$bias_for_all_coeffs_and_simulations, as.vector))
MC_errors22e = MC_errors_calc(msesmc,biasmc)
```

#### LP ERRORS

```{r}
LP_errorssix22 = linear_predictor_err(datas2e, performance_results22e)

lp_errors_mc = unlist(lapply(LP_errorssix22$lp_errors, as.vector))

MC_error_lp_six22=sd(lp_errors_mc)/sqrt(length(lp_errors_mc))
```

### Fit simulated datasets with model of rank R2=3

#### Get performance
```{r}
#set seed
set.seed(123)

R2 = 3 #rank of fitted model

performance_results23e = simulations_fit_and_preformance(datas2e, p,k,n_simulations,R2)

```


#### Get Average Beta Heatmap
```{r}
beta_heat_6_23=beta_heatmap("Fitted under rank 3",datas2e, performance_results23e,0.470,1.247)
```


#### Plot performance
```{r}
r23e = generate_histograms(performance_results23e, 4,3)
```


#### ΜC ERRORS

```{r}
msesmc = unlist(lapply(performance_results23e$mse_for_all_coeffs_and_simulation, as.vector))
biasmc = unlist(lapply(performance_results23e$bias_for_all_coeffs_and_simulations, as.vector))
MC_errors23e = MC_errors_calc(msesmc,biasmc)
```


#### LP ERRORS

```{r}
LP_errorssix23 = linear_predictor_err(datas2e, performance_results23e)

lp_errors_mc = unlist(lapply(LP_errorssix23$lp_errors, as.vector))

MC_error_lp_six23=sd(lp_errors_mc)/sqrt(length(lp_errors_mc))
```


#### Plots

```{r}
plot_listf1 <- list()
plot_listf1[[1]]=r21e$pL2
plot_listf1[[2]]=r22e$pL2
plot_listf1[[3]]=r23e$pL2


top=paste('Histograms of mean absolute bias(left) and MSE(right) for 200 simulated under rank 2 datasets with 5000 individuals, 10 predictors and 4 outcomes when fitted with ranks 1,2 or 3')
top=''
combined_plotsf1 <- grid.arrange(grobs = plot_listf1, ncol = 3, nrow = 1,top=top)

# Save the combined plot grid as a PNG
ggsave("combined_plotssix2p1.png", plot = combined_plotsf1, width = 23, height = 6, dpi = 300)


plot_listf1 <- list()
plot_listf1[[1]]=r21e$pL3
plot_listf1[[2]]=r22e$pL3
plot_listf1[[3]]=r23e$pL3


top=paste('Histograms of mean bias per outcome for 200 simulated under rank 2 datasets with 5000 individuals, 10 predictors and 4 outcomes when fitted with ranks 1,2 or 3')
top=''
combined_plotsf1 <- grid.arrange(grobs = plot_listf1, ncol = 3, nrow = 1,top=top)

# Save the combined plot grid as a PNG
ggsave("combined_plotssix2p2.png", plot = combined_plotsf1, width = 23, height = 6, dpi = 300)


plot_listf1 <- list()
plot_listf1[[1]]=r21e$pL4
plot_listf1[[2]]=r22e$pL4
plot_listf1[[3]]=r23e$pL4


top=paste('Histograms of mean MSE per outcome for 200 simulated under rank 2 datasets with 5000 individuals, 10 predictors and 4 outcomes when fitted with ranks 1,2 or 3')
top=''
combined_plotsf1 <- grid.arrange(grobs = plot_listf1, ncol = 3, nrow = 1,top=top)

# Save the combined plot grid as a PNG
ggsave("combined_plotssix2p3.png", plot = combined_plotsf1, width = 23, height = 6, dpi = 300)

```


### Heatmaps

```{r}
plot_listf1 <- list()
plot_listf1[[1]]=heatmap6_1_real
plot_listf1[[2]]=beta_heat_6_11
plot_listf1[[3]]=beta_heat_6_12
plot_listf1[[4]]=NULL
plot_listf1[[5]]=heatmap6_2_real
plot_listf1[[6]]=beta_heat_6_21
plot_listf1[[7]]=beta_heat_6_22
plot_listf1[[8]]=beta_heat_6_23

top=''

heatmaps_6 <- grid.arrange(grobs = plot_listf1, ncol = 4, nrow = 2,top=top)

# Save the combined plot grid as a PNG
ggsave("heatmaps_6.png", plot = heatmaps_6, width = 23, height = 6, dpi = 300)
```


# Seventh Configuration (sample size:500, p=5,K=8, n_simulations = 200)

## Call functions to simulate  n_simulations =200 datasets using rank r=1 


```{r}
#set seed
set.seed(123)
#chose variables 
n = 500 # Number of observations
p <- 5    # Number of covariates
k <- 8  # Number of outcomes
n_simulations = 200

r <- 1  # Rank of simulated data

datas1f = simulations_data(n,p,r,k,n_simulations)

Beta_matrix_1=datas1f$Beta_matrix
colnames(Beta_matrix_1) <- paste("k=", 1:ncol(Beta_matrix_1), sep="")
rownames(Beta_matrix_1) <- paste("X", 1:nrow(Beta_matrix_1), sep="")
beta_df <- reshape2::melt(Beta_matrix_1)
my_palette <- colorRampPalette(c("lightgrey", "orange", "black"))(100)
breaks <- seq(0.012,0.907, length.out = 101)
norm_breaks <- (breaks - min(breaks)) / (max(breaks) - min(breaks))
heatmap7_1_real <- ggplot(beta_df, aes(x = Var2, y = Var1, fill = value)) +
  geom_tile() +
  scale_fill_gradientn(colors = my_palette, 
                       values = norm_breaks, # specify normalized breaks here
                       limits = range(breaks)) + # Set limits to the min and max of your breaks
  theme_minimal() +
  labs(x = "", y = "", fill = "Value") +
  theme(     axis.text.x = element_text(angle = 90, hjust = 1, size = 15),       axis.text.y = element_text(size = 15),                              axis.title.x = element_text(size = 15),                            axis.title.y = element_text(size = 15),                            legend.text = element_text(size = 20),                              legend.title = element_text(size = 20),                            plot.title = element_text(size = 30),                strip.text = element_text(size = 20)                              )+
  ggtitle("Simulated under rank 1")
```

### Generated Data Statistics Plots

```{r}
seventh1 = generate_data_plots(datas1f$dlong_list,500,1,5,8)
```

### Fit simulated datasets with model of rank R2=1

#### Get performance
```{r}
#set seed
set.seed(123)

R2 = 1 #rank of fitted model

performance_results11f = simulations_fit_and_preformance(datas1f, p,k,n_simulations,R2)

```


#### Get Average Beta Heatmap
```{r}
beta_heat_7_11=beta_heatmap("Fitted under rank 1",datas1f, performance_results11f,0.012,0.907)
```


#### Plot performance
```{r}
r11f = generate_histograms(performance_results11f, 8,1)
```


#### ΜC ERRORS

```{r}
msesmc = unlist(lapply(performance_results11f$mse_for_all_coeffs_and_simulation, as.vector))
biasmc = unlist(lapply(performance_results11f$bias_for_all_coeffs_and_simulations, as.vector))
MC_errors11f = MC_errors_calc(msesmc,biasmc)
```


#### LP ERRORS

```{r}
LP_errorssev11 = linear_predictor_err(datas1f, performance_results11f)

lp_errors_mc = unlist(lapply(LP_errorssev11$lp_errors, as.vector))

MC_error_lp_sev11=sd(lp_errors_mc)/sqrt(length(lp_errors_mc))
```


### Fit simulated datasets with model of rank R2=2

#### Get performance
```{r}
#set seed
set.seed(123)

R2 = 2 #rank of fitted model

performance_results12f = simulations_fit_and_preformance(datas1f, p,k,n_simulations,R2)

```


#### Get Average Beta Heatmap
```{r}
beta_heat_7_12=beta_heatmap("Fitted under rank 2",datas1f, performance_results12f,0.012,0.907)
```


#### Plot performance
```{r}
r12f = generate_histograms(performance_results12f, 8,2)
```

#### ΜC ERRORS

```{r}
msesmc = unlist(lapply(performance_results12f$mse_for_all_coeffs_and_simulation, as.vector))
biasmc = unlist(lapply(performance_results12f$bias_for_all_coeffs_and_simulations, as.vector))
MC_errors12f = MC_errors_calc(msesmc,biasmc)
```


#### LP ERRORS

```{r}
LP_errorssev12 = linear_predictor_err(datas1f, performance_results12f)

lp_errors_mc = unlist(lapply(LP_errorssev12$lp_errors, as.vector))

MC_error_lp_sev12=sd(lp_errors_mc)/sqrt(length(lp_errors_mc))
```

### Fit simulated datasets with model of rank R2=3

#### Get performance
```{r}
#set seed
set.seed(123)

R2 = 3 #rank of fitted model

performance_results13f = simulations_fit_and_preformance(datas1f, p,k,n_simulations,R2)

```

#### Plot performance
```{r}
r13f = generate_histograms(performance_results13f, 8,3)
```

#### ΜC ERRORS

```{r}
msesmc = unlist(lapply(performance_results13f$mse_for_all_coeffs_and_simulation, as.vector))
biasmc = unlist(lapply(performance_results13f$bias_for_all_coeffs_and_simulations, as.vector))
MC_errors13f = MC_errors_calc(msesmc,biasmc)
```


#### LP ERRORS

```{r}
LP_errorssev13 = linear_predictor_err(datas1f, performance_results13f)

lp_errors_mc = unlist(lapply(LP_errorssev13$lp_errors, as.vector))

MC_error_lp_sev13=sd(lp_errors_mc)/sqrt(length(lp_errors_mc))
```


#### Plots

```{r}
plot_listf1 <- list()
plot_listf1[[1]]=r11f$pL2
plot_listf1[[2]]=r12f$pL2
# plot_listf1[[3]]=r13f$pL2


top=paste('Histograms of mean absolute bias(left) and MSE(right) for 200 simulated under rank 1 datasets with 500 individuals, 5 predictors and 8 outcomes when fitted with ranks 1,2 or 3')
top=''
combined_plotsf1 <- grid.arrange(grobs = plot_listf1, ncol = 2, nrow = 1,top=top)

# Save the combined plot grid as a PNG
ggsave("combined_plotssev1p1.png", plot = combined_plotsf1, width = 23, height = 6, dpi = 300)


plot_listf1 <- list()
plot_listf1[[1]]=r11f$pL3
plot_listf1[[2]]=r12f$pL3
# plot_listf1[[3]]=r13f$pL3


top=paste('Histograms of mean bias per outcome for 200 simulated under rank 1 datasets with 500 individuals, 5 predictors and 8 outcomes when fitted with ranks 1,2 or 3')
top=''
combined_plotsf1 <- grid.arrange(grobs = plot_listf1, ncol = 2, nrow = 1,top=top)

# Save the combined plot grid as a PNG
ggsave("combined_plotssev1p2.png", plot = combined_plotsf1, width = 23, height = 6, dpi = 300)


plot_listf1 <- list()
plot_listf1[[1]]=r11f$pL4
plot_listf1[[2]]=r12f$pL4
# plot_listf1[[3]]=r13f$pL4


top=paste('Histograms of mean MSE per outcome for 200 simulated under rank 1 datasets with 500 individuals, 5 predictors and 8 outcomes when fitted with ranks 1,2 or 3')
top=''
combined_plotsf1 <- grid.arrange(grobs = plot_listf1, ncol = 2, nrow = 1,top=top)

# Save the combined plot grid as a PNG
ggsave("combined_plotssev1p3.png", plot = combined_plotsf1, width = 23, height = 6, dpi = 300)

```




## Call functions to simulate  n_simulations = 200 datasets using rank r=2 

```{r}
#set seed
set.seed(123)

r <- 2  # Rank of simulated data

datas2f = simulations_data(n,p,r,k,n_simulations)

Beta_matrix_1=datas2f$Beta_matrix
colnames(Beta_matrix_1) <- paste("k=", 1:ncol(Beta_matrix_1), sep="")
rownames(Beta_matrix_1) <- paste("X", 1:nrow(Beta_matrix_1), sep="")
beta_df <- reshape2::melt(Beta_matrix_1)
my_palette <- colorRampPalette(c("lightgrey", "orange", "black"))(100)
breaks <- seq(0.043,1.361, length.out = 101)
norm_breaks <- (breaks - min(breaks)) / (max(breaks) - min(breaks))
heatmap7_2_real <- ggplot(beta_df, aes(x = Var2, y = Var1, fill = value)) +
  geom_tile() +
  scale_fill_gradientn(colors = my_palette, 
                       values = norm_breaks, # specify normalized breaks here
                       limits = range(breaks)) + # Set limits to the min and max of your breaks
  theme_minimal() +
  labs(x = "", y = "", fill = "Value") +
  theme(     axis.text.x = element_text(angle = 90, hjust = 1, size = 15),       axis.text.y = element_text(size = 15),                              axis.title.x = element_text(size = 15),                            axis.title.y = element_text(size = 15),                            legend.text = element_text(size = 20),                              legend.title = element_text(size = 20),                            plot.title = element_text(size = 30),                strip.text = element_text(size = 20)                              )+
  ggtitle("Simulated under rank 2")
```

### Generated Data Statistics Plots

```{r}
seventh2 = generate_data_plots(datas2f$dlong_list,500,2,5,8)
```

### Fit simulated datasets with model of rank R2=1

#### Get performance
```{r}
#set seed
set.seed(123)

R2 = 1 #rank of fitted model

performance_results21f = simulations_fit_and_preformance(datas2f, p,k,n_simulations,R2)

```


#### Get Average Beta Heatmap
```{r}
beta_heat_7_21=beta_heatmap("Fitted under rank 1",datas2f, performance_results21f,0.043,1.361)
```

#### Plot performance
```{r}
r21f = generate_histograms(performance_results21f, 8,1)
```


#### ΜC ERRORS

```{r}
msesmc = unlist(lapply(performance_results21f$mse_for_all_coeffs_and_simulation, as.vector))
biasmc = unlist(lapply(performance_results21f$bias_for_all_coeffs_and_simulations, as.vector))
MC_errors21f = MC_errors_calc(msesmc,biasmc)
```


#### LP ERRORS

```{r}
LP_errorssev21 = linear_predictor_err(datas2f, performance_results21f)

lp_errors_mc = unlist(lapply(LP_errorssev21$lp_errors, as.vector))

MC_error_lp_sev21=sd(lp_errors_mc)/sqrt(length(lp_errors_mc))
```


### Fit simulated datasets with model of rank R2=2

#### Get performance
```{r}
#set seed
set.seed(123)

R2 = 2 #rank of fitted model

performance_results22f = simulations_fit_and_preformance(datas2f, p,k,n_simulations,R2)

```


#### Get Average Beta Heatmap
```{r}
beta_heat_7_22=beta_heatmap("Fitted under rank 2",datas2f, performance_results22f,0.043,1.361)
```


#### Plot performance
```{r}
r22f = generate_histograms(performance_results22f, 8,2)
```

#### ΜC ERRORS

```{r}
msesmc = unlist(lapply(performance_results22f$mse_for_all_coeffs_and_simulation, as.vector))
biasmc = unlist(lapply(performance_results22f$bias_for_all_coeffs_and_simulations, as.vector))
MC_errors22f = MC_errors_calc(msesmc,biasmc)
```


#### LP ERRORS

```{r}
LP_errorssev22 = linear_predictor_err(datas2f, performance_results22f)

lp_errors_mc = unlist(lapply(LP_errorssev22$lp_errors, as.vector))

MC_error_lp_sev22=sd(lp_errors_mc)/sqrt(length(lp_errors_mc))
```


### Fit simulated datasets with model of rank R2=3

#### Get performance
```{r}
#set seed
set.seed(123)

R2 = 3 #rank of fitted model

performance_results23f = simulations_fit_and_preformance(datas2f, p,k,n_simulations,R2)

```



#### Get Average Beta Heatmap
```{r}
beta_heat_7_23=beta_heatmap("Fitted under rank 3",datas2f, performance_results23f,0.043,1.361)
```


#### Plot performance
```{r}
r23f = generate_histograms(performance_results23f,8,3)
```

#### ΜC ERRORS

```{r}
msesmc = unlist(lapply(performance_results23f$mse_for_all_coeffs_and_simulation, as.vector))
biasmc = unlist(lapply(performance_results23f$bias_for_all_coeffs_and_simulations, as.vector))
MC_errors23f = MC_errors_calc(msesmc,biasmc)
```


#### LP ERRORS

```{r}
LP_errorssev23 = linear_predictor_err(datas2f, performance_results23f)

lp_errors_mc = unlist(lapply(LP_errorssev23$lp_errors, as.vector))

MC_error_lp_sev23=sd(lp_errors_mc)/sqrt(length(lp_errors_mc))
```


#### Plots

```{r}
plot_listf1 <- list()
plot_listf1[[1]]=r21f$pL2
plot_listf1[[2]]=r22f$pL2
plot_listf1[[3]]=r23f$pL2


top=paste('Histograms of mean absolute bias(left) and MSE(right) for 200 simulated under rank 2 datasets with 500 individuals, 5 predictors and 8 outcomes when fitted with ranks 1,2 or 3')
top=''
combined_plotsf1 <- grid.arrange(grobs = plot_listf1, ncol = 3, nrow = 1,top=top)

# Save the combined plot grid as a PNG
ggsave("combined_plotssev2p1.png", plot = combined_plotsf1, width = 23, height = 6, dpi = 300)


plot_listf1 <- list()
plot_listf1[[1]]=r21f$pL3
plot_listf1[[2]]=r22f$pL3
plot_listf1[[3]]=r23f$pL3


top=paste('Histograms of mean bias per outcome for 200 simulated under rank 2 datasets with 500 individuals, 5 predictors and 8 outcomes when fitted with ranks 1,2 or 3')
top=''
combined_plotsf1 <- grid.arrange(grobs = plot_listf1, ncol = 3, nrow = 1,top=top)

# Save the combined plot grid as a PNG
ggsave("combined_plotssev2p2.png", plot = combined_plotsf1, width = 23, height = 6, dpi = 300)


plot_listf1 <- list()
plot_listf1[[1]]=r21f$pL4
plot_listf1[[2]]=r22f$pL4
plot_listf1[[3]]=r23f$pL4


top=paste('Histograms of mean MSE per outcome for 200 simulated under rank 2 datasets with 500 individuals, 5 predictors and 8 outcomes when fitted with ranks 1,2 or 3')
top=''
combined_plotsf1 <- grid.arrange(grobs = plot_listf1, ncol = 3, nrow = 1,top=top)

# Save the combined plot grid as a PNG
ggsave("combined_plotssev2p3.png", plot = combined_plotsf1, width = 23, height = 6, dpi = 300)

```


### Heatmaps

```{r}
plot_listf1 <- list()
plot_listf1[[1]]=heatmap7_1_real
plot_listf1[[2]]=beta_heat_7_11
plot_listf1[[3]]=beta_heat_7_12
plot_listf1[[4]]=NULL
plot_listf1[[5]]=heatmap7_2_real
plot_listf1[[6]]=beta_heat_7_21
plot_listf1[[7]]=beta_heat_7_22
plot_listf1[[8]]=beta_heat_7_23

top=''

heatmaps_7 <- grid.arrange(grobs = plot_listf1, ncol = 4, nrow = 2,top=top)

# Save the combined plot grid as a PNG
ggsave("heatmaps_7.png", plot = heatmaps_7, width = 23, height = 6, dpi = 300)
```


# Eighth Configuration (sample size:1000, p=5,K=8, n_simulations = 200)

## Call functions to simulate  n_simulations = 200 datasets using rank r=1 

```{r}
#set seed
set.seed(123)
#chose variables 
n = 1000 # Number of observations
p <- 5    # Number of covariates
k <- 8  # Number of outcomes
 n_simulations = 200

r <- 1  # Rank of simulated data

datas1g = simulations_data(n,p,r,k,n_simulations)

Beta_matrix_1=datas1g$Beta_matrix
colnames(Beta_matrix_1) <- paste("k=", 1:ncol(Beta_matrix_1), sep="")
rownames(Beta_matrix_1) <- paste("X", 1:nrow(Beta_matrix_1), sep="")
beta_df <- reshape2::melt(Beta_matrix_1)
my_palette <- colorRampPalette(c("lightgrey", "orange", "black"))(100)
breaks <- seq(0.012,0.907, length.out = 101)
norm_breaks <- (breaks - min(breaks)) / (max(breaks) - min(breaks))
heatmap8_1_real <- ggplot(beta_df, aes(x = Var2, y = Var1, fill = value)) +
  geom_tile() +
  scale_fill_gradientn(colors = my_palette, 
                       values = norm_breaks, # specify normalized breaks here
                       limits = range(breaks)) + # Set limits to the min and max of your breaks
  theme_minimal() +
  labs(x = "", y = "", fill = "Value") +
  theme(     axis.text.x = element_text(angle = 90, hjust = 1, size = 15),       axis.text.y = element_text(size = 15),                              axis.title.x = element_text(size = 15),                            axis.title.y = element_text(size = 15),                            legend.text = element_text(size = 20),                              legend.title = element_text(size = 20),                            plot.title = element_text(size = 30),                strip.text = element_text(size = 20)                              )+
  ggtitle("Simulated under rank 1")
```

### Generated Data Statistics Plots

```{r}
eight1 = generate_data_plots(datas1g$dlong_list,1000,1,5,8)
```

### Fit simulated datasets with model of rank R2=1

#### Get performance
```{r}
#set seed
set.seed(123)

R2 = 1 #rank of fitted model

performance_results11g = simulations_fit_and_preformance(datas1g, p,k,n_simulations,R2)

```


#### Get Average Beta Heatmap
```{r}
beta_heat_8_11=beta_heatmap("Fitted under rank 1",datas1g, performance_results11g,0.012,0.907)
```


#### Plot performance
```{r}
r11g = generate_histograms(performance_results11g, 8,1)
```


#### ΜC ERRORS

```{r}
msesmc = unlist(lapply(performance_results11g$mse_for_all_coeffs_and_simulation, as.vector))
biasmc = unlist(lapply(performance_results11g$bias_for_all_coeffs_and_simulations, as.vector))
MC_errors11g = MC_errors_calc(msesmc,biasmc)
```



#### LP ERRORS

```{r}
LP_errorseig11 = linear_predictor_err(datas1g, performance_results11g)

lp_errors_mc = unlist(lapply(LP_errorseig11$lp_errors, as.vector))

MC_error_lp_eig11=sd(lp_errors_mc)/sqrt(length(lp_errors_mc))
```


### Fit simulated datasets with model of rank R2=2

#### Get performance
```{r}
#set seed
set.seed(123)

R2 = 2 #rank of fitted model

performance_results12g = simulations_fit_and_preformance(datas1g, p,k,n_simulations,R2)

```



#### Get Average Beta Heatmap
```{r}
beta_heat_8_12=beta_heatmap("Fitted under rank 2",datas1g, performance_results12g,0.012,0.907)
```


#### Plot performance
```{r}
r12g = generate_histograms(performance_results12g, 8,2)
```

#### ΜC ERRORS

```{r}
msesmc = unlist(lapply(performance_results12g$mse_for_all_coeffs_and_simulation, as.vector))
biasmc = unlist(lapply(performance_results12g$bias_for_all_coeffs_and_simulations, as.vector))
MC_errors12g = MC_errors_calc(msesmc,biasmc)
```


#### LP ERRORS

```{r}
LP_errorseig12 = linear_predictor_err(datas1g, performance_results12g)

lp_errors_mc = unlist(lapply(LP_errorseig12$lp_errors, as.vector))

MC_error_lp_eig12=sd(lp_errors_mc)/sqrt(length(lp_errors_mc))
```

### Fit simulated datasets with model of rank R2=3

#### Get performance
```{r}
#set seed
set.seed(123)

R2 = 3 #rank of fitted model

performance_results13g = simulations_fit_and_preformance(datas1g, p,k,n_simulations,R2)

```

#### Plot performance
```{r}
r13g = generate_histograms(performance_results13g, 8,3)
```

#### ΜC ERRORS

```{r}
msesmc = unlist(lapply(performance_results13g$mse_for_all_coeffs_and_simulation, as.vector))
biasmc = unlist(lapply(performance_results13g$bias_for_all_coeffs_and_simulations, as.vector))
MC_errors13g = MC_errors_calc(msesmc,biasmc)
```

#### LP ERRORS

```{r}
LP_errorseig13 = linear_predictor_err(datas1g, performance_results13g)

lp_errors_mc = unlist(lapply(LP_errorseig13$lp_errors, as.vector))

MC_error_lp_eig13=sd(lp_errors_mc)/sqrt(length(lp_errors_mc))
```

#### Plots

```{r}
plot_listf1 <- list()
plot_listf1[[1]]=r11g$pL2
plot_listf1[[2]]=r12g$pL2
# plot_listf1[[3]]=r13g$pL2


top=paste('Histograms of mean absolute bias(left) and MSE(right) for 200 simulated under rank 1 datasets with 1000 individuals, 5 predictors and 8 outcomes when fitted with ranks 1,2 or 3')
top=''
combined_plotsf1 <- grid.arrange(grobs = plot_listf1, ncol = 2, nrow = 1,top=top)

# Save the combined plot grid as a PNG
ggsave("combined_plotseig1p1.png", plot = combined_plotsf1, width = 23, height = 6, dpi = 300)


plot_listf1 <- list()
plot_listf1[[1]]=r11g$pL3
plot_listf1[[2]]=r12g$pL3
# plot_listf1[[3]]=r13g$pL3


top=paste('Histograms of mean bias per outcome for 200 simulated under rank 1 datasets with 1000 individuals, 5 predictors and 8 outcomes when fitted with ranks 1,2 or 3')
top=''
combined_plotsf1 <- grid.arrange(grobs = plot_listf1, ncol = 2, nrow = 1,top=top)

# Save the combined plot grid as a PNG
ggsave("combined_plotseig1p2.png", plot = combined_plotsf1, width = 23, height = 6, dpi = 300)


plot_listf1 <- list()
plot_listf1[[1]]=r11g$pL4
plot_listf1[[2]]=r12g$pL4
# plot_listf1[[3]]=r13g$pL4


top=paste('Histograms of mean MSE per outcome for 200 simulated under rank 1 datasets with 1000 individuals, 5 predictors and 8 outcomes when fitted with ranks 1,2 or 3')
top=''
combined_plotsf1 <- grid.arrange(grobs = plot_listf1, ncol = 2, nrow = 1,top=top)

# Save the combined plot grid as a PNG
ggsave("combined_plotseig1p3.png", plot = combined_plotsf1, width = 23, height = 6, dpi = 300)

```




## Call functions to simulate  n_simulations = 200 datasets using rank r=2 

```{r}
#set seed
set.seed(123)

r <- 2  # Rank of simulated data

datas2g = simulations_data(n,p,r,k,n_simulations)

Beta_matrix_1=datas2g$Beta_matrix
colnames(Beta_matrix_1) <- paste("k=", 1:ncol(Beta_matrix_1), sep="")
rownames(Beta_matrix_1) <- paste("X", 1:nrow(Beta_matrix_1), sep="")
beta_df <- reshape2::melt(Beta_matrix_1)
my_palette <- colorRampPalette(c("lightgrey", "orange", "black"))(100)
breaks <- seq(0.043,1.361, length.out = 101)
norm_breaks <- (breaks - min(breaks)) / (max(breaks) - min(breaks))
heatmap8_2_real <- ggplot(beta_df, aes(x = Var2, y = Var1, fill = value)) +
  geom_tile() +
  scale_fill_gradientn(colors = my_palette, 
                       values = norm_breaks, # specify normalized breaks here
                       limits = range(breaks)) + # Set limits to the min and max of your breaks
  theme_minimal() +
  labs(x = "", y = "", fill = "Value") +
  theme(     axis.text.x = element_text(angle = 90, hjust = 1, size = 15),       axis.text.y = element_text(size = 15),                              axis.title.x = element_text(size = 15),                            axis.title.y = element_text(size = 15),                            legend.text = element_text(size = 20),                              legend.title = element_text(size = 20),                            plot.title = element_text(size = 30),                strip.text = element_text(size = 20)                              )+
  ggtitle("Simulated under rank 2")
```

### Generated Data Statistics Plots

```{r}
eight2 = generate_data_plots(datas2g$dlong_list,1000,2,5,8)
```

### Fit simulated datasets with model of rank R2=1

#### Get performance
```{r}
#set seed
set.seed(123)

R2 = 1 #rank of fitted model

performance_results21g = simulations_fit_and_preformance(datas2g, p,k,n_simulations,R2)

```

#### Get Average Beta Heatmap
```{r}
beta_heat_8_21=beta_heatmap("Fitted under rank 1",datas2g, performance_results21g,0.043,1.361)
```

#### Plot performance
```{r}
r21g = generate_histograms(performance_results21g, 8,1)
```

#### ΜC ERRORS

```{r}
msesmc = unlist(lapply(performance_results21g$mse_for_all_coeffs_and_simulation, as.vector))
biasmc = unlist(lapply(performance_results21g$bias_for_all_coeffs_and_simulations, as.vector))
MC_errors21g = MC_errors_calc(msesmc,biasmc)
```


#### LP ERRORS

```{r}
LP_errorseig21 = linear_predictor_err(datas2g, performance_results21g)

lp_errors_mc = unlist(lapply(LP_errorseig21$lp_errors, as.vector))

MC_error_lp_eig21=sd(lp_errors_mc)/sqrt(length(lp_errors_mc))
```

### Fit simulated datasets with model of rank R2=2

#### Get performance
```{r}
#set seed
set.seed(123)

R2 = 2 #rank of fitted model

performance_results22g= simulations_fit_and_preformance(datas2g, p,k,n_simulations,R2)

```


#### Get Average Beta Heatmap
```{r}
beta_heat_8_22=beta_heatmap("Fitted under rank 2",datas2g, performance_results22g,0.043,1.361)
```


#### Plot performance
```{r}
r22g = generate_histograms(performance_results22g, 8,2)
```

#### ΜC ERRORS

```{r}
msesmc = unlist(lapply(performance_results22g$mse_for_all_coeffs_and_simulation, as.vector))
biasmc = unlist(lapply(performance_results22g$bias_for_all_coeffs_and_simulations, as.vector))
MC_errors22g = MC_errors_calc(msesmc,biasmc)
```


#### LP ERRORS

```{r}
LP_errorseig22 = linear_predictor_err(datas2g, performance_results22g)

lp_errors_mc = unlist(lapply(LP_errorseig22$lp_errors, as.vector))

MC_error_lp_eig22=sd(lp_errors_mc)/sqrt(length(lp_errors_mc))
```

### Fit simulated datasets with model of rank R2=3

#### Get performance
```{r}
#set seed
set.seed(123)

R2 = 3 #rank of fitted model

performance_results23g= simulations_fit_and_preformance(datas2g, p,k,n_simulations,R2)

```


#### Get Average Beta Heatmap
```{r}
beta_heat_8_23=beta_heatmap("Fitted under rank 3",datas2g, performance_results23g,0.043,1.361)
```


#### Plot performance
```{r}
r23g = generate_histograms(performance_results23g, 8,3)
```


#### ΜC ERRORS

```{r}
msesmc = unlist(lapply(performance_results23g$mse_for_all_coeffs_and_simulation, as.vector))
biasmc = unlist(lapply(performance_results23g$bias_for_all_coeffs_and_simulations, as.vector))
MC_errors23g = MC_errors_calc(msesmc,biasmc)
```


#### LP ERRORS

```{r}
LP_errorseig23 = linear_predictor_err(datas2g, performance_results23g)

lp_errors_mc = unlist(lapply(LP_errorseig23$lp_errors, as.vector))

MC_error_lp_eig23=sd(lp_errors_mc)/sqrt(length(lp_errors_mc))
```


#### Plots

```{r}
plot_listf1 <- list()
plot_listf1[[1]]=r21g$pL2
plot_listf1[[2]]=r22g$pL2
plot_listf1[[3]]=r23g$pL2


top=paste('Histograms of mean absolute bias(left) and MSE(right) for 200 simulated under rank 2 datasets with 1000 individuals, 5 predictors and 8 outcomes when fitted with ranks 1,2 or 3')
top=''
combined_plotsf1 <- grid.arrange(grobs = plot_listf1, ncol = 3, nrow = 1,top=top)

# Save the combined plot grid as a PNG
ggsave("combined_plotseig2p1.png", plot = combined_plotsf1, width = 23, height = 6, dpi = 300)


plot_listf1 <- list()
plot_listf1[[1]]=r21g$pL3
plot_listf1[[2]]=r22g$pL3
plot_listf1[[3]]=r23g$pL3


top=paste('Histograms of mean bias per outcome for 200 simulated under rank 2 datasets with 1000 individuals, 5 predictors and 8 outcomes when fitted with ranks 1,2 or 3')
top=''
combined_plotsf1 <- grid.arrange(grobs = plot_listf1, ncol = 3, nrow = 1,top=top)

# Save the combined plot grid as a PNG
ggsave("combined_plotseig2p2.png", plot = combined_plotsf1, width = 23, height = 6, dpi = 300)


plot_listf1 <- list()
plot_listf1[[1]]=r21g$pL4
plot_listf1[[2]]=r22g$pL4
plot_listf1[[3]]=r23g$pL4


top=paste('Histograms of mean MSE per outcome for 200 simulated under rank 2 datasets with 1000 individuals, 5 predictors and 8 outcomes when fitted with ranks 1,2 or 3')
top=''
combined_plotsf1 <- grid.arrange(grobs = plot_listf1, ncol = 3, nrow = 1,top=top)

# Save the combined plot grid as a PNG
ggsave("combined_plotseig2p3.png", plot = combined_plotsf1, width = 23, height = 6, dpi = 300)

```


### Heatmaps

```{r}
plot_listf1 <- list()
plot_listf1[[1]]=heatmap8_1_real
plot_listf1[[2]]=beta_heat_8_11
plot_listf1[[3]]=beta_heat_8_12
plot_listf1[[4]]=NULL
plot_listf1[[5]]=heatmap8_2_real
plot_listf1[[6]]=beta_heat_8_21
plot_listf1[[7]]=beta_heat_8_22
plot_listf1[[8]]=beta_heat_8_23

top=''

heatmaps_8 <- grid.arrange(grobs = plot_listf1, ncol = 4, nrow = 2,top=top)

# Save the combined plot grid as a PNG
ggsave("heatmaps_8.png", plot = heatmaps_8, width = 23, height = 6, dpi = 300)
```


# Nineth Configuration (sample size:5000, p=5,K=8, n_simulations = 200)

## Call functions to simulate  n_simulations = 200 datasets using rank r=1 

```{r}
#set seed
set.seed(123)
#chose variables 
n = 5000  # Number of observations
p <- 5    # Number of covariates
k <- 8  # Number of outcomes
 n_simulations = 200

r <- 1  # Rank of simulated data

datas1h = simulations_data(n,p,r,k,n_simulations)

Beta_matrix_1=datas1h$Beta_matrix
colnames(Beta_matrix_1) <- paste("k=", 1:ncol(Beta_matrix_1), sep="")
rownames(Beta_matrix_1) <- paste("X", 1:nrow(Beta_matrix_1), sep="")
beta_df <- reshape2::melt(Beta_matrix_1)
my_palette <- colorRampPalette(c("lightgrey", "orange", "black"))(100)
breaks <- seq(0.012,0.907, length.out = 101)
norm_breaks <- (breaks - min(breaks)) / (max(breaks) - min(breaks))
heatmap9_1_real <- ggplot(beta_df, aes(x = Var2, y = Var1, fill = value)) +
  geom_tile() +
  scale_fill_gradientn(colors = my_palette, 
                       values = norm_breaks, # specify normalized breaks here
                       limits = range(breaks)) + # Set limits to the min and max of your breaks
  theme_minimal() +
  labs(x = "", y = "", fill = "Value") +
  theme(     axis.text.x = element_text(angle = 90, hjust = 1, size = 15),       axis.text.y = element_text(size = 15),                              axis.title.x = element_text(size = 15),                            axis.title.y = element_text(size = 15),                            legend.text = element_text(size = 20),                              legend.title = element_text(size = 20),                            plot.title = element_text(size = 30),                strip.text = element_text(size = 20)                              )+
  ggtitle("Simulated under rank 1")
```

### Generated Data Statistics Plots

```{r}
nine1 = generate_data_plots(datas1h$dlong_list,5000,1,5,8)
```

### Fit simulated datasets with model of rank R2=1

#### Get performance
```{r}
#set seed
set.seed(123)

R2 = 1 #rank of fitted model

performance_results11h = simulations_fit_and_preformance(datas1h, p,k,n_simulations,R2)

```



#### Get Average Beta Heatmap
```{r}
beta_heat_9_11=beta_heatmap("Fitted under rank 1",datas1h, performance_results11h,0.012,0.907)
```

#### Plot performance
```{r}
r11h = generate_histograms(performance_results11h, 8,1)
```

#### ΜC ERRORS

```{r}
msesmc = unlist(lapply(performance_results11h$mse_for_all_coeffs_and_simulation, as.vector))
biasmc = unlist(lapply(performance_results11h$bias_for_all_coeffs_and_simulations, as.vector))
MC_errors11h = MC_errors_calc(msesmc,biasmc)
```


#### LP ERRORS

```{r}
LP_errorsnine11 = linear_predictor_err(datas1h, performance_results11h)

lp_errors_mc = unlist(lapply(LP_errorsnine11$lp_errors, as.vector))

MC_error_lp_nine11=sd(lp_errors_mc)/sqrt(length(lp_errors_mc))
```


### Fit simulated datasets with model of rank R2=2

#### Get performance
```{r}
#set seed
set.seed(123)

R2 = 2 #rank of fitted model

performance_results12h = simulations_fit_and_preformance(datas1h, p,k,n_simulations,R2)

```


#### Get Average Beta Heatmap
```{r}
beta_heat_9_12=beta_heatmap("Fitted under rank 2",datas1h, performance_results12h,0.012,0.907)
```


#### Plot performance
```{r}
r12h = generate_histograms(performance_results12h,8,2)
```

#### ΜC ERRORS

```{r}
msesmc = unlist(lapply(performance_results12h$mse_for_all_coeffs_and_simulation, as.vector))
biasmc = unlist(lapply(performance_results12h$bias_for_all_coeffs_and_simulations, as.vector))
MC_errors12h = MC_errors_calc(msesmc,biasmc)
```



#### LP ERRORS

```{r}
LP_errorsnine12 = linear_predictor_err(datas1h, performance_results12h)

lp_errors_mc = unlist(lapply(LP_errorsnine12$lp_errors, as.vector))

MC_error_lp_nine12=sd(lp_errors_mc)/sqrt(length(lp_errors_mc))
```


### Fit simulated datasets with model of rank R2=3

#### Get performance
```{r}
#set seed
set.seed(123)

R2 = 3 #rank of fitted model

performance_results13h = simulations_fit_and_preformance(datas1h, p,k,n_simulations,R2)

```

#### Plot performance
```{r}
r13h = generate_histograms(performance_results13h, 8,3)
```

#### ΜC ERRORS

```{r}
msesmc = unlist(lapply(performance_results13h$mse_for_all_coeffs_and_simulation, as.vector))
biasmc = unlist(lapply(performance_results13h$bias_for_all_coeffs_and_simulations, as.vector))
MC_errors13h = MC_errors_calc(msesmc,biasmc)
```


#### LP ERRORS

```{r}
LP_errorsnine13 = linear_predictor_err(datas1h, performance_results13h)

lp_errors_mc = unlist(lapply(LP_errorsnine13$lp_errors, as.vector))

MC_error_lp_nine13=sd(lp_errors_mc)/sqrt(length(lp_errors_mc))
```

#### Plots

```{r}
plot_listf1 <- list()
plot_listf1[[1]]=r11h$pL2
plot_listf1[[2]]=r12h$pL2
# plot_listf1[[3]]=r13h$pL2


top=paste('Histograms of mean absolute bias(left) and MSE(right) for 200 simulated under rank 1 datasets with 5000 individuals, 5 predictors and 8 outcomes when fitted with ranks 1,2 or 3')
top=''
combined_plotsf1 <- grid.arrange(grobs = plot_listf1, ncol = 2, nrow = 1,top=top)

# Save the combined plot grid as a PNG
ggsave("combined_plotsnine1p1.png", plot = combined_plotsf1, width = 23, height = 6, dpi = 300)


plot_listf1 <- list()
plot_listf1[[1]]=r11h$pL3
plot_listf1[[2]]=r12h$pL3
# plot_listf1[[3]]=r13h$pL3


top=paste('Histograms of mean bias per outcome for 200 simulated under rank 1 datasets with 5000 individuals, 5 predictors and 8 outcomes when fitted with ranks 1,2 or 3')
top=''
combined_plotsf1 <- grid.arrange(grobs = plot_listf1, ncol = 2, nrow = 1,top=top)

# Save the combined plot grid as a PNG
ggsave("combined_plotsnine1p2.png", plot = combined_plotsf1, width = 23, height = 6, dpi = 300)


plot_listf1 <- list()
plot_listf1[[1]]=r11h$pL4
plot_listf1[[2]]=r12h$pL4
# plot_listf1[[3]]=r13h$pL4


top=paste('Histograms of mean MSE per outcome for 200 simulated under rank 1 datasets with 5000 individuals, 5 predictors and 8 outcomes when fitted with ranks 1,2 or 3')
top=''
combined_plotsf1 <- grid.arrange(grobs = plot_listf1, ncol = 2, nrow = 1,top=top)

# Save the combined plot grid as a PNG
ggsave("combined_plotsnine1p3.png", plot = combined_plotsf1, width = 23, height = 6, dpi = 300)

```


## Call functions to simulate  n_simulations = 200 datasets using rank r=2 

```{r}
#set seed
set.seed(123)

r <- 2  # Rank of simulated data

datas2h = simulations_data(n,p,r,k,n_simulations)

Beta_matrix_1=datas2h$Beta_matrix
colnames(Beta_matrix_1) <- paste("k=", 1:ncol(Beta_matrix_1), sep="")
rownames(Beta_matrix_1) <- paste("X", 1:nrow(Beta_matrix_1), sep="")
beta_df <- reshape2::melt(Beta_matrix_1)
my_palette <- colorRampPalette(c("lightgrey", "orange", "black"))(100)
breaks <- seq(0.043,1.361, length.out = 101)
norm_breaks <- (breaks - min(breaks)) / (max(breaks) - min(breaks))
heatmap9_2_real <- ggplot(beta_df, aes(x = Var2, y = Var1, fill = value)) +
  geom_tile() +
  scale_fill_gradientn(colors = my_palette, 
                       values = norm_breaks, # specify normalized breaks here
                       limits = range(breaks)) + # Set limits to the min and max of your breaks
  theme_minimal() +
  labs(x = "", y = "", fill = "Value") +
  theme(     axis.text.x = element_text(angle = 90, hjust = 1, size = 15),       axis.text.y = element_text(size = 15),                              axis.title.x = element_text(size = 15),                            axis.title.y = element_text(size = 15),                            legend.text = element_text(size = 20),                              legend.title = element_text(size = 20),                            plot.title = element_text(size = 30),                strip.text = element_text(size = 20)                              )+
  ggtitle("Simulated under rank 2")
```

### Generated Data Statistics Plots

```{r}
nine2= generate_data_plots(datas2h$dlong_list,5000,2,5,8)
```

### Fit simulated datasets with model of rank R2=1

#### Get performance
```{r}
#set seed
set.seed(123)

R2 = 1 #rank of fitted model

performance_results21h = simulations_fit_and_preformance(datas2h, p,k,n_simulations,R2)

```

#### Get Average Beta Heatmap
```{r}
beta_heat_9_21=beta_heatmap("Fitted under rank 1",datas2h, performance_results21h,0.043,1.361)
```


#### Plot performance
```{r}
r21h = generate_histograms(performance_results21h, 8,1)
```

#### ΜC ERRORS

```{r}
msesmc = unlist(lapply(performance_results21h$mse_for_all_coeffs_and_simulation, as.vector))
biasmc = unlist(lapply(performance_results21h$bias_for_all_coeffs_and_simulations, as.vector))
MC_errors21h = MC_errors_calc(msesmc,biasmc)
```



#### LP ERRORS

```{r}
LP_errorsnine21 = linear_predictor_err(datas2h, performance_results21h)

lp_errors_mc = unlist(lapply(LP_errorsnine21$lp_errors, as.vector))

MC_error_lp_nine21=sd(lp_errors_mc)/sqrt(length(lp_errors_mc))
```

### Fit simulated datasets with model of rank R2=2

#### Get performance
```{r}
#set seed
set.seed(123)

R2 = 2 #rank of fitted model

performance_results22h= simulations_fit_and_preformance(datas2h, p,k,n_simulations,R2)

```


#### Get Average Beta Heatmap
```{r}
beta_heat_9_22=beta_heatmap("Fitted under rank 2",datas2h, performance_results22h,0.043,1.361)
```


#### Plot performance
```{r}
r22h = generate_histograms(performance_results22h , 8,2)
```

#### ΜC ERRORS

```{r}
msesmc = unlist(lapply(performance_results22h$mse_for_all_coeffs_and_simulation, as.vector))
biasmc = unlist(lapply(performance_results22h$bias_for_all_coeffs_and_simulations, as.vector))
MC_errors22h = MC_errors_calc(msesmc,biasmc)
```


#### LP ERRORS

```{r}
LP_errorsnine22 = linear_predictor_err(datas2h, performance_results22h)

lp_errors_mc = unlist(lapply(LP_errorsnine22$lp_errors, as.vector))

MC_error_lp_nine22=sd(lp_errors_mc)/sqrt(length(lp_errors_mc))
```


### Fit simulated datasets with model of rank R2=3

#### Get performance
```{r}
#set seed
set.seed(123)

R2 = 3 #rank of fitted model

performance_results23h= simulations_fit_and_preformance(datas2h, p,k,n_simulations,R2)

```


#### Get Average Beta Heatmap
```{r}
beta_heat_9_23=beta_heatmap("Fitted under rank 3",datas2h, performance_results23h,0.043,1.361)
```


#### Plot performance
```{r}
r23h = generate_histograms(performance_results23h, 8, 3)
```

#### ΜC ERRORS

```{r}
msesmc = unlist(lapply(performance_results23h$mse_for_all_coeffs_and_simulation, as.vector))
biasmc = unlist(lapply(performance_results23h$bias_for_all_coeffs_and_simulations, as.vector))
MC_errors23h = MC_errors_calc(msesmc,biasmc)
```


#### LP ERRORS

```{r}
LP_errorsnine23 = linear_predictor_err(datas2h, performance_results23h)

lp_errors_mc = unlist(lapply(LP_errorsnine23$lp_errors, as.vector))

MC_error_lp_nine23=sd(lp_errors_mc)/sqrt(length(lp_errors_mc))
```


#### Plots

```{r}
plot_listf1 <- list()
plot_listf1[[1]]=r21h$pL2
plot_listf1[[2]]=r22h$pL2
plot_listf1[[3]]=r23h$pL2


top=paste('Histograms of mean absolute bias(left) and MSE(right) for 200 simulated under rank 2 datasets with 5000 individuals, 5 predictors and 8 outcomes when fitted with ranks 1,2 or 3')
top=''
combined_plotsf1 <- grid.arrange(grobs = plot_listf1, ncol = 3, nrow = 1,top=top)

# Save the combined plot grid as a PNG
ggsave("combined_plotsnine2p1.png", plot = combined_plotsf1, width = 23, height = 6, dpi = 300)


plot_listf1 <- list()
plot_listf1[[1]]=r21h$pL3
plot_listf1[[2]]=r22h$pL3
plot_listf1[[3]]=r23h$pL3


top=paste('Histograms of mean bias per outcome for 200 simulated under rank 2 datasets with 5000 individuals, 5 predictors and 8 outcomes when fitted with ranks 1,2 or 3')
top=''
combined_plotsf1 <- grid.arrange(grobs = plot_listf1, ncol = 3, nrow = 1,top=top)

# Save the combined plot grid as a PNG
ggsave("combined_plotsnine2p2.png", plot = combined_plotsf1, width = 23, height = 6, dpi = 300)


plot_listf1 <- list()
plot_listf1[[1]]=r21h$pL4
plot_listf1[[2]]=r22h$pL4
plot_listf1[[3]]=r23h$pL4


top=paste('Histograms of mean MSE per outcome for 200 simulated under rank 2 datasets with 5000 individuals, 5 predictors and 8 outcomes when fitted with ranks 1,2 or 3')
top=''
combined_plotsf1 <- grid.arrange(grobs = plot_listf1, ncol = 3, nrow = 1,top=top)

# Save the combined plot grid as a PNG
ggsave("combined_plotsnine2p3.png", plot = combined_plotsf1, width = 23, height = 6, dpi = 300)

```


### Heatmaps

```{r}
plot_listf1 <- list()
plot_listf1[[1]]=heatmap9_1_real
plot_listf1[[2]]=beta_heat_9_11
plot_listf1[[3]]=beta_heat_9_12
plot_listf1[[4]]=NULL
plot_listf1[[5]]=heatmap9_2_real
plot_listf1[[6]]=beta_heat_9_21
plot_listf1[[7]]=beta_heat_9_22
plot_listf1[[8]]=beta_heat_9_23

top=''

heatmaps_9 <- grid.arrange(grobs = plot_listf1, ncol = 4, nrow = 2,top=top)

# Save the combined plot grid as a PNG
ggsave("heatmaps_9.png", plot = heatmaps_9, width = 23, height = 6, dpi = 300)
```

# Tenth Configuration (sample size:500, p=10,K=8, n_simulations = 200)

## Call functions to simulate  n_simulations = 200 datasets using rank r=1 

```{r}
#set seed
set.seed(123)
#chose variables 
n = 500  # Number of observations
p <- 10    # Number of covariates
k <- 8  # Number of outcomes
 n_simulations = 200

r <- 1  # Rank of simulated data

datas1i = simulations_data(n,p,r,k,n_simulations)

Beta_matrix_1=datas1i$Beta_matrix
colnames(Beta_matrix_1) <- paste("k=", 1:ncol(Beta_matrix_1), sep="")
rownames(Beta_matrix_1) <- paste("X", 1:nrow(Beta_matrix_1), sep="")
beta_df <- reshape2::melt(Beta_matrix_1)
my_palette <- colorRampPalette(c("lightgrey", "orange", "black"))(100)
breaks <- seq(0.000283,0.911, length.out = 101)
norm_breaks <- (breaks - min(breaks)) / (max(breaks) - min(breaks))
heatmap10_1_real <- ggplot(beta_df, aes(x = Var2, y = Var1, fill = value)) +
  geom_tile() +
  scale_fill_gradientn(colors = my_palette, 
                       values = norm_breaks, # specify normalized breaks here
                       limits = range(breaks)) + # Set limits to the min and max of your breaks
  theme_minimal() +
  labs(x = "", y = "", fill = "Value") +
  theme(     axis.text.x = element_text(angle = 90, hjust = 1, size = 15),       axis.text.y = element_text(size = 15),                              axis.title.x = element_text(size = 15),                            axis.title.y = element_text(size = 15),                            legend.text = element_text(size = 20),                              legend.title = element_text(size = 20),                            plot.title = element_text(size = 30),                strip.text = element_text(size = 20)                              )+
  ggtitle("Simulated under rank 1")
```

### Generated Data Statistics Plots

```{r}
ten1=generate_data_plots(datas1i$dlong_list,500,1,10,8)
```

### Fit simulated datasets with model of rank R2=1

#### Get performance
```{r}
#set seed
set.seed(123)

R2 = 1 #rank of fitted model

performance_results11i = simulations_fit_and_preformance(datas1i, p,k,n_simulations,R2)

```


#### Get Average Beta Heatmap
```{r}
beta_heat_10_11=beta_heatmap("Fitted under rank 1",datas1i, performance_results11i,0.000283,0.911)
```


#### Plot performance
```{r}
r11i = generate_histograms(performance_results11i, 8,1)
```


#### ΜC ERRORS

```{r}
msesmc = unlist(lapply(performance_results11i$mse_for_all_coeffs_and_simulation, as.vector))
biasmc = unlist(lapply(performance_results11i$bias_for_all_coeffs_and_simulations, as.vector))
MC_errors11i = MC_errors_calc(msesmc,biasmc)
```


#### LP ERRORS

```{r}
LP_errorsten11 = linear_predictor_err(datas1i, performance_results11i)

lp_errors_mc = unlist(lapply(LP_errorsten11$lp_errors, as.vector))

MC_error_lp_ten11=sd(lp_errors_mc)/sqrt(length(lp_errors_mc))
```

### Fit simulated datasets with model of rank R2=2

#### Get performance
```{r}
#set seed
set.seed(123)

R2 = 2 #rank of fitted model

performance_results12i = simulations_fit_and_preformance(datas1i, p,k,n_simulations,R2)

```

#### Get Average Beta Heatmap
```{r}
beta_heat_10_12=beta_heatmap("Fitted under rank 2",datas1i, performance_results12i,0.000283,0.911)
```



#### Plot performance
```{r}
r12i = generate_histograms(performance_results12i, 8,2)
```

#### ΜC ERRORS

```{r}
msesmc = unlist(lapply(performance_results12i$mse_for_all_coeffs_and_simulation, as.vector))
biasmc = unlist(lapply(performance_results12i$bias_for_all_coeffs_and_simulations, as.vector))
MC_errors12i = MC_errors_calc(msesmc,biasmc)
```


#### LP ERRORS

```{r}
LP_errorsten12 = linear_predictor_err(datas1i, performance_results12i)

lp_errors_mc = unlist(lapply(LP_errorsten12$lp_errors, as.vector))

MC_error_lp_ten12=sd(lp_errors_mc)/sqrt(length(lp_errors_mc))
```

### Fit simulated datasets with model of rank R2=3

#### Get performance
```{r}
#set seed
set.seed(123)

R2 = 3 #rank of fitted model

performance_results13i = simulations_fit_and_preformance(datas1i, p,k,n_simulations,R2)

```

#### Plot performance
```{r}
r13i = generate_histograms(performance_results13i, 8,3)
```


#### ΜC ERRORS

```{r}
msesmc = unlist(lapply(performance_results13i$mse_for_all_coeffs_and_simulation, as.vector))
biasmc = unlist(lapply(performance_results13i$bias_for_all_coeffs_and_simulations, as.vector))
MC_errors13i = MC_errors_calc(msesmc,biasmc)
```


#### LP ERRORS

```{r}
LP_errorsten13= linear_predictor_err(datas1i, performance_results13i)

lp_errors_mc = unlist(lapply(LP_errorsten13$lp_errors, as.vector))

MC_error_lp_ten13=sd(lp_errors_mc)/sqrt(length(lp_errors_mc))
```

#### Plots

```{r}
plot_listf1 <- list()
plot_listf1[[1]]=r11i$pL2
plot_listf1[[2]]=r12i$pL2
# plot_listf1[[3]]=r13i$pL2


top=paste('Histograms of mean absolute bias(left) and MSE(right) for 200 simulated under rank 1 datasets with 500 individuals, 10 predictors and 8 outcomes when fitted with ranks 1,2 or 3')
top=''
combined_plotsf1 <- grid.arrange(grobs = plot_listf1, ncol = 2, nrow = 1,top=top)

# Save the combined plot grid as a PNG
ggsave("combined_plotsten1p1.png", plot = combined_plotsf1, width = 23, height = 6, dpi = 300)


plot_listf1 <- list()
plot_listf1[[1]]=r11i$pL3
plot_listf1[[2]]=r12i$pL3
# plot_listf1[[3]]=r13i$pL3


top=paste('Histograms of mean bias per outcome for 200 simulated under rank 1 datasets with 500 individuals, 10 predictors and 8 outcomes when fitted with ranks 1,2 or 3')
top=''
combined_plotsf1 <- grid.arrange(grobs = plot_listf1, ncol = 2, nrow = 1,top=top)

# Save the combined plot grid as a PNG
ggsave("combined_plotsten1p2.png", plot = combined_plotsf1, width = 23, height = 6, dpi = 300)


plot_listf1 <- list()
plot_listf1[[1]]=r11i$pL4
plot_listf1[[2]]=r12i$pL4
# plot_listf1[[3]]=r13i$pL4


top=paste('Histograms of mean MSE per outcome for 200 simulated under rank 1 datasets with 500 individuals, 10 predictors and 8 outcomes when fitted with ranks 1,2 or 3')
top=''
combined_plotsf1 <- grid.arrange(grobs = plot_listf1, ncol = 2, nrow = 1,top=top)

# Save the combined plot grid as a PNG
ggsave("combined_plotsten1p3.png", plot = combined_plotsf1, width = 23, height = 6, dpi = 300)

```



## Call functions to simulate  n_simulations = 200 datasets using rank r=2 

```{r}
#set seed
set.seed(123)

r <- 2  # Rank of simulated data

datas2i = simulations_data(n,p,r,k,n_simulations)

Beta_matrix_1=datas2i$Beta_matrix
colnames(Beta_matrix_1) <- paste("k=", 1:ncol(Beta_matrix_1), sep="")
rownames(Beta_matrix_1) <- paste("X", 1:nrow(Beta_matrix_1), sep="")
beta_df <- reshape2::melt(Beta_matrix_1)
my_palette <- colorRampPalette(c("lightgrey", "orange", "black"))(100)
breaks <- seq(0.045,1.4192, length.out = 101)
norm_breaks <- (breaks - min(breaks)) / (max(breaks) - min(breaks))
heatmap10_2_real <- ggplot(beta_df, aes(x = Var2, y = Var1, fill = value)) +
  geom_tile() +
  scale_fill_gradientn(colors = my_palette, 
                       values = norm_breaks, # specify normalized breaks here
                       limits = range(breaks)) + # Set limits to the min and max of your breaks
  theme_minimal() +
  labs(x = "", y = "", fill = "Value") +
  theme(     axis.text.x = element_text(angle = 90, hjust = 1, size = 15),       axis.text.y = element_text(size = 15),                              axis.title.x = element_text(size = 15),                            axis.title.y = element_text(size = 15),                            legend.text = element_text(size = 20),                              legend.title = element_text(size = 20),                            plot.title = element_text(size = 30),                strip.text = element_text(size = 20)                              )+
  ggtitle("Simulated under rank 2")
```

### Generated Data Statistics Plots

```{r}
ten2=generate_data_plots(datas2i$dlong_list,500,2,10,8)
```

### Fit simulated datasets with model of rank R2=1

#### Get performance
```{r}
#set seed
set.seed(123)

R2 = 1 #rank of fitted model

performance_results21i = simulations_fit_and_preformance(datas2i, p,k,n_simulations,R2)

```


#### Get Average Beta Heatmap
```{r}
beta_heat_10_21=beta_heatmap("Fitted under rank 1",datas2i, performance_results21i,0.045,1.4192)
```


#### Plot performance
```{r}
r21i = generate_histograms(performance_results21i, 8,1)
```

#### ΜC ERRORS

```{r}
msesmc = unlist(lapply(performance_results21i$mse_for_all_coeffs_and_simulation, as.vector))
biasmc = unlist(lapply(performance_results21i$bias_for_all_coeffs_and_simulations, as.vector))
MC_errors21i = MC_errors_calc(msesmc,biasmc)
```


#### LP ERRORS

```{r}
LP_errorsten21= linear_predictor_err(datas2i, performance_results21i)

lp_errors_mc = unlist(lapply(LP_errorsten21$lp_errors, as.vector))

MC_error_lp_ten21=sd(lp_errors_mc)/sqrt(length(lp_errors_mc))
```


### Fit simulated datasets with model of rank R2=2

#### Get performance
```{r}
#set seed
set.seed(123)

R2 = 2 #rank of fitted model

performance_results22i= simulations_fit_and_preformance(datas2i, p,k,n_simulations,R2)

```


#### Get Average Beta Heatmap
```{r}
beta_heat_10_22=beta_heatmap("Fitted under rank 2",datas2i, performance_results22i,0.045,1.4192)
```


#### Plot performance
```{r}
r22i = generate_histograms(performance_results22i, 8,2)
```

#### ΜC ERRORS

```{r}
msesmc = unlist(lapply(performance_results22i$mse_for_all_coeffs_and_simulation, as.vector))
biasmc = unlist(lapply(performance_results22i$bias_for_all_coeffs_and_simulations, as.vector))
MC_errors22i = MC_errors_calc(msesmc,biasmc)
```


#### LP ERRORS

```{r}
LP_errorsten22= linear_predictor_err(datas2i, performance_results22i)


lp_errors_mc = unlist(lapply(LP_errorsten22$lp_errors, as.vector))

MC_error_lp_ten22=sd(lp_errors_mc)/sqrt(length(lp_errors_mc))
```

### Fit simulated datasets with model of rank R2=3

#### Get performance
```{r}
#set seed
set.seed(123)

R2 = 3 #rank of fitted model

performance_results23i= simulations_fit_and_preformance(datas2i, p,k,n_simulations,R2)

```


#### Get Average Beta Heatmap
```{r}
beta_heat_10_23=beta_heatmap("Fitted under rank 3",datas2i, performance_results23i,0.045,1.4192)
```


#### Plot performance
```{r}
r23i = generate_histograms(performance_results23i, 8,3)
```

#### ΜC ERRORS

```{r}
msesmc = unlist(lapply(performance_results23i$mse_for_all_coeffs_and_simulation, as.vector))
biasmc = unlist(lapply(performance_results23i$bias_for_all_coeffs_and_simulations, as.vector))
MC_errors23i = MC_errors_calc(msesmc,biasmc)
```

#### LP ERRORS

```{r}
LP_errorsten23= linear_predictor_err(datas2i, performance_results23i)

lp_errors_mc = unlist(lapply(LP_errorsten23$lp_errors, as.vector))

MC_error_lp_ten23=sd(lp_errors_mc)/sqrt(length(lp_errors_mc))
```

#### Plots

```{r}
plot_listf1 <- list()
plot_listf1[[1]]=r21i$pL2
plot_listf1[[2]]=r22i$pL2
plot_listf1[[3]]=r23i$pL2


top=paste('Histograms of mean absolute bias(left) and MSE(right) for 200 simulated under rank 2 datasets with 500 individuals, 10 predictors and 8 outcomes when fitted with ranks 1,2 or 3')
top=''
combined_plotsf1 <- grid.arrange(grobs = plot_listf1, ncol = 3, nrow = 1,top=top)

# Save the combined plot grid as a PNG
ggsave("combined_plotsten2p1.png", plot = combined_plotsf1, width = 23, height = 6, dpi = 300)


plot_listf1 <- list()
plot_listf1[[1]]=r21i$pL3
plot_listf1[[2]]=r22i$pL3
plot_listf1[[3]]=r23i$pL3


top=paste('Histograms of mean bias per outcome for 200 simulated under rank 2 datasets with 500 individuals, 10 predictors and 8 outcomes when fitted with ranks 1,2 or 3')
top=''
combined_plotsf1 <- grid.arrange(grobs = plot_listf1, ncol = 3, nrow = 1,top=top)

# Save the combined plot grid as a PNG
ggsave("combined_plotsten2p2.png", plot = combined_plotsf1, width = 23, height = 6, dpi = 300)


plot_listf1 <- list()
plot_listf1[[1]]=r21i$pL4
plot_listf1[[2]]=r22i$pL4
plot_listf1[[3]]=r23i$pL4


top=paste('Histograms of mean MSE per outcome for 200 simulated under rank 2 datasets with 500 individuals, 10 predictors and 8 outcomes when fitted with ranks 1,2 or 3')
top=''
combined_plotsf1 <- grid.arrange(grobs = plot_listf1, ncol = 3, nrow = 1,top=top)

# Save the combined plot grid as a PNG
ggsave("combined_plotsten2p3.png", plot = combined_plotsf1, width = 23, height = 6, dpi = 300)

```


### Heatmaps

```{r}
plot_listf1 <- list()
plot_listf1[[1]]=heatmap10_1_real
plot_listf1[[2]]=beta_heat_10_11
plot_listf1[[3]]=beta_heat_10_12
plot_listf1[[4]]=NULL
plot_listf1[[5]]=heatmap10_2_real
plot_listf1[[6]]=beta_heat_10_21
plot_listf1[[7]]=beta_heat_10_22
plot_listf1[[8]]=beta_heat_10_23

top=''

heatmaps_10 <- grid.arrange(grobs = plot_listf1, ncol = 4, nrow = 2,top=top)

# Save the combined plot grid as a PNG
ggsave("heatmaps_10.png", plot = heatmaps_10, width = 23, height = 6, dpi = 300)
```


# Eleventh Configuration (sample size:1000, p=10,K=8, n_simulations = 200)

## Call functions to simulate  n_simulations = 200 datasets using rank r=1 

```{r}
#set seed
set.seed(123)
#chose variables 
n = 1000  # Number of observations
p <- 10   # Number of covariates
k <- 8  # Number of outcomes
 n_simulations = 200

r <- 1  # Rank of simulated data

datas1j = simulations_data(n,p,r,k,n_simulations)

Beta_matrix_1=datas1j$Beta_matrix
colnames(Beta_matrix_1) <- paste("k=", 1:ncol(Beta_matrix_1), sep="")
rownames(Beta_matrix_1) <- paste("X", 1:nrow(Beta_matrix_1), sep="")
beta_df <- reshape2::melt(Beta_matrix_1)
my_palette <- colorRampPalette(c("lightgrey", "orange", "black"))(100)
breaks <- seq(0.000283,0.911, length.out = 101)
norm_breaks <- (breaks - min(breaks)) / (max(breaks) - min(breaks))
heatmap11_1_real <- ggplot(beta_df, aes(x = Var2, y = Var1, fill = value)) +
  geom_tile() +
  scale_fill_gradientn(colors = my_palette, 
                       values = norm_breaks, # specify normalized breaks here
                       limits = range(breaks)) + # Set limits to the min and max of your breaks
  theme_minimal() +
  labs(x = "", y = "", fill = "Value") +
  theme(     axis.text.x = element_text(angle = 90, hjust = 1, size = 15),       axis.text.y = element_text(size = 15),                              axis.title.x = element_text(size = 15),                            axis.title.y = element_text(size = 15),                            legend.text = element_text(size = 20),                              legend.title = element_text(size = 20),                            plot.title = element_text(size = 30),                strip.text = element_text(size = 20)                              )+
  ggtitle("Simulated under rank 1")
```

### Generated Data Statistics Plots

```{r}
eleven1 = generate_data_plots(datas1j$dlong_list,1000,1,10,8)
```

### Fit simulated datasets with model of rank R2=1

#### Get performance
```{r}
#set seed
set.seed(123)

R2 = 1 #rank of fitted model

performance_results11j = simulations_fit_and_preformance(datas1j, p,k,n_simulations,R2)

```


#### Get Average Beta Heatmap
```{r}
beta_heat_11_11=beta_heatmap("Fitted under rank 1",datas1j, performance_results11j,0.000283,0.911)
```


#### Plot performance
```{r}
r11j = generate_histograms(performance_results11j, 8,1)
```

#### ΜC ERRORS

```{r}
msesmc = unlist(lapply(performance_results11j$mse_for_all_coeffs_and_simulation, as.vector))
biasmc = unlist(lapply(performance_results11j$bias_for_all_coeffs_and_simulations, as.vector))
MC_errors11j = MC_errors_calc(msesmc,biasmc)
```


#### LP ERRORS

```{r}
LP_errorsele11= linear_predictor_err(datas1j, performance_results11j)

lp_errors_mc = unlist(lapply(LP_errorsele11$lp_errors, as.vector))

MC_error_lp_ele11=sd(lp_errors_mc)/sqrt(length(lp_errors_mc))
```


### Fit simulated datasets with model of rank R2=2

#### Get performance
```{r}
#set seed
set.seed(123)

R2 = 2 #rank of fitted model

performance_results12j = simulations_fit_and_preformance(datas1j, p,k,n_simulations,R2)

```


#### Get Average Beta Heatmap
```{r}
beta_heat_11_12=beta_heatmap("Fitted under rank 2",datas1j, performance_results12j,0.000283,0.911)
```


#### Plot performance
```{r}
r12j = generate_histograms(performance_results12j, 8,2)
```

#### ΜC ERRORS

```{r}
msesmc = unlist(lapply(performance_results12j$mse_for_all_coeffs_and_simulation, as.vector))
biasmc = unlist(lapply(performance_results12j$bias_for_all_coeffs_and_simulations, as.vector))
MC_errors12j = MC_errors_calc(msesmc,biasmc)
```

#### LP ERRORS

```{r}
LP_errorsele12= linear_predictor_err(datas1j, performance_results12j)

lp_errors_mc = unlist(lapply(LP_errorsele12$lp_errors, as.vector))

MC_error_lp_ele12=sd(lp_errors_mc)/sqrt(length(lp_errors_mc))
```



### Fit simulated datasets with model of rank R2=3

#### Get performance
```{r}
#set seed
set.seed(123)

R2 = 3 #rank of fitted model

performance_results13j = simulations_fit_and_preformance(datas1j, p,k,n_simulations,R2)

```

#### Plot performance
```{r}
r13j = generate_histograms(performance_results13j, 8,3)
```


#### ΜC ERRORS

```{r}
msesmc = unlist(lapply(performance_results13j$mse_for_all_coeffs_and_simulation, as.vector))
biasmc = unlist(lapply(performance_results13j$bias_for_all_coeffs_and_simulations, as.vector))
MC_errors13j = MC_errors_calc(msesmc,biasmc)
```


#### LP ERRORS

```{r}
LP_errorsele13= linear_predictor_err(datas1j, performance_results13j)

lp_errors_mc = unlist(lapply(LP_errorsele13$lp_errors, as.vector))

MC_error_lp_ele13=sd(lp_errors_mc)/sqrt(length(lp_errors_mc))
```


#### Plots

```{r}
plot_listf1 <- list()
plot_listf1[[1]]=r11j$pL2
plot_listf1[[2]]=r12j$pL2
# plot_listf1[[3]]=r13j$pL2


top=paste('Histograms of mean absolute bias(left) and MSE(right) for 200 simulated under rank 1 datasets with 1000 individuals, 10 predictors and 8 outcomes when fitted with ranks 1,2 or 3')
top=''
combined_plotsf1 <- grid.arrange(grobs = plot_listf1, ncol = 2, nrow = 1,top=top)

# Save the combined plot grid as a PNG
ggsave("combined_plotselev1p1.png", plot = combined_plotsf1, width = 23, height = 6, dpi = 300)


plot_listf1 <- list()
plot_listf1[[1]]=r11j$pL3
plot_listf1[[2]]=r12j$pL3
# plot_listf1[[3]]=r13j$pL3


top=paste('Histograms of mean bias per outcome for 200 simulated under rank 1 datasets with 1000 individuals, 10 predictors and 8 outcomes when fitted with ranks 1,2 or 3')
top=''
combined_plotsf1 <- grid.arrange(grobs = plot_listf1, ncol = 2, nrow = 1,top=top)

# Save the combined plot grid as a PNG
ggsave("combined_plotselev1p2.png", plot = combined_plotsf1, width = 23, height = 6, dpi = 300)


plot_listf1 <- list()
plot_listf1[[1]]=r11j$pL4
plot_listf1[[2]]=r12j$pL4
# plot_listf1[[3]]=r13j$pL4


top=paste('Histograms of mean MSE per outcome for 200 simulated under rank 1 datasets with 1000 individuals, 10 predictors and 8 outcomes when fitted with ranks 1,2 or 3')
top=''
combined_plotsf1 <- grid.arrange(grobs = plot_listf1, ncol = 2, nrow = 1,top=top)

# Save the combined plot grid as a PNG
ggsave("combined_plotselev1p3.png", plot = combined_plotsf1, width = 23, height = 6, dpi = 300)

```

## Call functions to simulate  n_simulations = 200 datasets using rank r=2 

```{r}
#set seed
set.seed(123)

r <- 2  # Rank of simulated data

datas2j = simulations_data(n,p,r,k,n_simulations)

Beta_matrix_1=datas2j$Beta_matrix
colnames(Beta_matrix_1) <- paste("k=", 1:ncol(Beta_matrix_1), sep="")
rownames(Beta_matrix_1) <- paste("X", 1:nrow(Beta_matrix_1), sep="")
beta_df <- reshape2::melt(Beta_matrix_1)
my_palette <- colorRampPalette(c("lightgrey", "orange", "black"))(100)
breaks <- seq(0.045,1.4192, length.out = 101)
norm_breaks <- (breaks - min(breaks)) / (max(breaks) - min(breaks))
heatmap11_2_real <- ggplot(beta_df, aes(x = Var2, y = Var1, fill = value)) +
  geom_tile() +
  scale_fill_gradientn(colors = my_palette, 
                       values = norm_breaks, # specify normalized breaks here
                       limits = range(breaks)) + # Set limits to the min and max of your breaks
  theme_minimal() +
  labs(x = "", y = "", fill = "Value") +
  theme(     axis.text.x = element_text(angle = 90, hjust = 1, size = 15),       axis.text.y = element_text(size = 15),                              axis.title.x = element_text(size = 15),                            axis.title.y = element_text(size = 15),                            legend.text = element_text(size = 20),                              legend.title = element_text(size = 20),                            plot.title = element_text(size = 30),                strip.text = element_text(size = 20)                              )+
  ggtitle("Simulated under rank 2")
```

### Generated Data Statistics Plots

```{r}
eleven2 = generate_data_plots(datas2j$dlong_list,1000,2,10,8)
```

### Fit simulated datasets with model of rank R2=1

#### Get performance
```{r}
#set seed
set.seed(123)

R2 = 1 #rank of fitted model

performance_results21j = simulations_fit_and_preformance(datas2j, p,k,n_simulations,R2)

```


#### Get Average Beta Heatmap
```{r}
beta_heat_11_21=beta_heatmap("Fitted under rank 1",datas2j, performance_results21j,0.045,1.4192)
```


#### Plot performance
```{r}
r21j = generate_histograms(performance_results21j, 8,1)
```

#### ΜC ERRORS

```{r}
msesmc = unlist(lapply(performance_results21j$mse_for_all_coeffs_and_simulation, as.vector))
biasmc = unlist(lapply(performance_results21j$bias_for_all_coeffs_and_simulations, as.vector))
MC_errors21j = MC_errors_calc(msesmc,biasmc)
```


#### LP ERRORS

```{r}
LP_errorsele21= linear_predictor_err(datas2j, performance_results21j)

lp_errors_mc = unlist(lapply(LP_errorsele21$lp_errors, as.vector))

MC_error_lp_ele21=sd(lp_errors_mc)/sqrt(length(lp_errors_mc))
```


### Fit simulated datasets with model of rank R2=2

#### Get performance
```{r}
#set seed
set.seed(123)

R2 = 2 #rank of fitted model

performance_results22j= simulations_fit_and_preformance(datas2j, p,k,n_simulations,R2)

```


#### Get Average Beta Heatmap
```{r}
beta_heat_11_22=beta_heatmap("Fitted under rank 2",datas2j, performance_results22j,0.045,1.4192)
```


#### Plot performance
```{r}
r22j = generate_histograms(performance_results22j, 8,2)
```

#### ΜC ERRORS

```{r}
msesmc = unlist(lapply(performance_results22j$mse_for_all_coeffs_and_simulation, as.vector))
biasmc = unlist(lapply(performance_results22j$bias_for_all_coeffs_and_simulations, as.vector))
MC_errors22j = MC_errors_calc(msesmc,biasmc)
```


#### LP ERRORS

```{r}
LP_errorsele22= linear_predictor_err(datas2j, performance_results22j)

lp_errors_mc = unlist(lapply(LP_errorsele22$lp_errors, as.vector))

MC_error_lp_ele22=sd(lp_errors_mc)/sqrt(length(lp_errors_mc))
```


### Fit simulated datasets with model of rank R2=3

#### Get performance
```{r}
#set seed
set.seed(123)

R2 = 3 #rank of fitted model

performance_results23j= simulations_fit_and_preformance(datas2j, p,k,n_simulations,R2)

```


#### Get Average Beta Heatmap
```{r}
beta_heat_11_23=beta_heatmap("Fitted under rank 3",datas2j, performance_results23j,0.045,1.41928)
```


#### Plot performance
```{r}
r23j = generate_histograms(performance_results23j, 8,3)
```

#### ΜC ERRORS

```{r}
msesmc = unlist(lapply(performance_results23j$mse_for_all_coeffs_and_simulation, as.vector))
biasmc = unlist(lapply(performance_results23j$bias_for_all_coeffs_and_simulations, as.vector))
MC_errors23j = MC_errors_calc(msesmc,biasmc)
```


#### LP ERRORS

```{r}
LP_errorsele23= linear_predictor_err(datas2j, performance_results23j)

lp_errors_mc = unlist(lapply(LP_errorsele23$lp_errors, as.vector))

MC_error_lp_ele23=sd(lp_errors_mc)/sqrt(length(lp_errors_mc))
```

#### Plots

```{r}
plot_listf1 <- list()
plot_listf1[[1]]=r21j$pL2
plot_listf1[[2]]=r22j$pL2
plot_listf1[[3]]=r23j$pL2


top=paste('Histograms of mean absolute bias(left) and MSE(right) for 200 simulated under rank 2 datasets with 1000 individuals, 10 predictors and 8 outcomes when fitted with ranks 1,2 or 3')
top=''
combined_plotsf1 <- grid.arrange(grobs = plot_listf1, ncol = 3, nrow = 1,top=top)

# Save the combined plot grid as a PNG
ggsave("combined_plotselev2p1.png", plot = combined_plotsf1, width = 23, height = 6, dpi = 300)


plot_listf1 <- list()
plot_listf1[[1]]=r21j$pL3
plot_listf1[[2]]=r22j$pL3
plot_listf1[[3]]=r23j$pL3


top=paste('Histograms of mean bias per outcome for 200 simulated under rank 2 datasets with 1000 individuals, 10 predictors and 8 outcomes when fitted with ranks 1,2 or 3')
top=''
combined_plotsf1 <- grid.arrange(grobs = plot_listf1, ncol = 3, nrow = 1,top=top)

# Save the combined plot grid as a PNG
ggsave("combined_plotselev2p2.png", plot = combined_plotsf1, width = 23, height = 6, dpi = 300)


plot_listf1 <- list()
plot_listf1[[1]]=r21j$pL4
plot_listf1[[2]]=r22j$pL4
plot_listf1[[3]]=r23j$pL4


top=paste('Histograms of mean MSE per outcome for 200 simulated under rank 2 datasets with 1000 individuals, 10 predictors and 8 outcomes when fitted with ranks 1,2 or 3')
top=''
combined_plotsf1 <- grid.arrange(grobs = plot_listf1, ncol = 3, nrow = 1,top=top)

# Save the combined plot grid as a PNG
ggsave("combined_plotselev2p3.png", plot = combined_plotsf1, width = 23, height = 6, dpi = 300)

```


### Heatmaps

```{r}
plot_listf1 <- list()
plot_listf1[[1]]=heatmap11_1_real
plot_listf1[[2]]=beta_heat_11_11
plot_listf1[[3]]=beta_heat_11_12
plot_listf1[[4]]=NULL
plot_listf1[[5]]=heatmap11_2_real
plot_listf1[[6]]=beta_heat_11_21
plot_listf1[[7]]=beta_heat_11_22
plot_listf1[[8]]=beta_heat_11_23

top=''

heatmaps_11 <- grid.arrange(grobs = plot_listf1, ncol = 4, nrow = 2,top=top)

# Save the combined plot grid as a PNG
ggsave("heatmaps_11.png", plot = heatmaps_11, width = 23, height = 6, dpi = 300)
```


# Twelvth Configuration (sample size:5000, p=10,K=8, n_simulations = 200)

## Call functions to simulate  n_simulations = 200 datasets using rank r=1 

```{r}
#set seed
set.seed(123)
#chose variables 
n = 5000  # Number of observations
p <- 10   # Number of covariates
k <- 8  # Number of outcomes
 n_simulations = 200

r <- 1  # Rank of simulated data

datas1k = simulations_data(n,p,r,k,n_simulations)

Beta_matrix_1=datas1k$Beta_matrix
colnames(Beta_matrix_1) <- paste("k=", 1:ncol(Beta_matrix_1), sep="")
rownames(Beta_matrix_1) <- paste("X", 1:nrow(Beta_matrix_1), sep="")
beta_df <- reshape2::melt(Beta_matrix_1)
my_palette <- colorRampPalette(c("lightgrey", "orange", "black"))(100)
breaks <- seq(0.000283,0.911, length.out = 101)
norm_breaks <- (breaks - min(breaks)) / (max(breaks) - min(breaks))
heatmap12_1_real <- ggplot(beta_df, aes(x = Var2, y = Var1, fill = value)) +
  geom_tile() +
  scale_fill_gradientn(colors = my_palette, 
                       values = norm_breaks, # specify normalized breaks here
                       limits = range(breaks)) + # Set limits to the min and max of your breaks
  theme_minimal() +
  labs(x = "", y = "", fill = "Value") +
  theme(     axis.text.x = element_text(angle = 90, hjust = 1, size = 15),       axis.text.y = element_text(size = 15),                              axis.title.x = element_text(size = 15),                            axis.title.y = element_text(size = 15),                            legend.text = element_text(size = 20),                              legend.title = element_text(size = 20),                            plot.title = element_text(size = 30),                strip.text = element_text(size = 20)                              )+
  ggtitle("Simulated under rank 1")
```

### Generated Data Statistics Plots

```{r}
twelve1 = generate_data_plots(datas1k$dlong_list,5000,1,10,8,datas1k$T_sim_list,datas1k$T_obs_list)
```

### Fit simulated datasets with model of rank R2=1

#### Get performance
```{r}
#set seed
set.seed(123)

R2 = 1 #rank of fitted model

performance_results11k = simulations_fit_and_preformance(datas1k, p,k,n_simulations,R2)

```

#### Get Average Beta Heatmap
```{r}
beta_heat_12_11=beta_heatmap("Fitted under rank 1",datas1k, performance_results11k,0.000283,0.911)
```


#### Plot performance
```{r}
r11k = generate_histograms(performance_results11k, 8,1)
```
#### ΜC ERRORS

```{r}
msesmc = unlist(lapply(performance_results11k$mse_for_all_coeffs_and_simulation, as.vector))
biasmc = unlist(lapply(performance_results11k$bias_for_all_coeffs_and_simulations, as.vector))
MC_errors11k = MC_errors_calc(msesmc,biasmc)
```


#### LP ERRORS

```{r}
LP_errorstwe11= linear_predictor_err(datas1k, performance_results11k)

lp_errors_mc = unlist(lapply(LP_errorstwe11$lp_errors, as.vector))

MC_error_lp_twe11=sd(lp_errors_mc)/sqrt(length(lp_errors_mc))
```


### Fit simulated datasets with model of rank R2=2

#### Get performance
```{r}
#set seed
set.seed(123)

R2 = 2 #rank of fitted model

performance_results12k = simulations_fit_and_preformance(datas1k, p,k,n_simulations,R2)

```


#### Get Average Beta Heatmap
```{r}
beta_heat_12_12=beta_heatmap("Fitted under rank 2",datas1k, performance_results12k,0.000283,0.911)
```

#### Plot performance
```{r}
r12k = generate_histograms(performance_results12k, 8,2)
```

#### ΜC ERRORS

```{r}
msesmc = unlist(lapply(performance_results12k$mse_for_all_coeffs_and_simulation, as.vector))
biasmc = unlist(lapply(performance_results12k$bias_for_all_coeffs_and_simulations, as.vector))
MC_errors12k = MC_errors_calc(msesmc,biasmc)
```


#### LP ERRORS

```{r}
LP_errorstwe12= linear_predictor_err(datas1k, performance_results12k)

lp_errors_mc = unlist(lapply(LP_errorstwe12$lp_errors, as.vector))

MC_error_lp_twe12=sd(lp_errors_mc)/sqrt(length(lp_errors_mc))
```


### Fit simulated datasets with model of rank R2=3

#### Get performance
```{r}
#set seed
set.seed(123)

R2 = 3 #rank of fitted model

performance_results13k = simulations_fit_and_preformance(datas1k, p,k,n_simulations,R2)

```

#### Plot performance
```{r}
r13k = generate_histograms(performance_results13k, 8,3)
```


#### ΜC ERRORS

```{r}
msesmc = unlist(lapply(performance_results13k$mse_for_all_coeffs_and_simulation, as.vector))
biasmc = unlist(lapply(performance_results13k$bias_for_all_coeffs_and_simulations, as.vector))
MC_errors13k = MC_errors_calc(msesmc,biasmc)
```


#### LP ERRORS

```{r}
LP_errorstwe13= linear_predictor_err(datas1k, performance_results13k)

lp_errors_mc = unlist(lapply(LP_errorstwe13$lp_errors, as.vector))

MC_error_lp_twe13=sd(lp_errors_mc)/sqrt(length(lp_errors_mc))
```


#### Plots

```{r}
plot_listf1 <- list()
plot_listf1[[1]]=r11k$pL2
plot_listf1[[2]]=r12k$pL2
# plot_listf1[[3]]=r13k$pL2


top=paste('Histograms of mean absolute bias(left) and MSE(right) for 200 simulated under rank 1 datasets with 5000 individuals, 10 predictors and 8 outcomes when fitted with ranks 1,2 or 3')
top=''
combined_plotsf1 <- grid.arrange(grobs = plot_listf1, ncol = 2, nrow = 1,top=top)

# Save the combined plot grid as a PNG
ggsave("combined_plotstwelve1p1.png", plot = combined_plotsf1, width = 23, height = 6, dpi = 300)


plot_listf1 <- list()
plot_listf1[[1]]=r11k$pL3
plot_listf1[[2]]=r12k$pL3
# plot_listf1[[3]]=r13k$pL3


top=paste('Histograms of mean bias per outcome for 200 simulated under rank 1 datasets with 5000 individuals, 10 predictors and 8 outcomes when fitted with ranks 1,2 or 3')
top=''
combined_plotsf1 <- grid.arrange(grobs = plot_listf1, ncol = 2, nrow = 1,top=top)

# Save the combined plot grid as a PNG
ggsave("combined_plotstwelve1p2.png", plot = combined_plotsf1, width = 23, height = 6, dpi = 300)


plot_listf1 <- list()
plot_listf1[[1]]=r11k$pL4
plot_listf1[[2]]=r12k$pL4
# plot_listf1[[3]]=r13k$pL4


top=paste('Histograms of mean MSE per outcome for 200 simulated under rank 1 datasets with 5000 individuals, 10 predictors and 8 outcomes when fitted with ranks 1,2 or 3')
top=''
combined_plotsf1 <- grid.arrange(grobs = plot_listf1, ncol = 2, nrow = 1,top=top)

# Save the combined plot grid as a PNG
ggsave("combined_plotstwelve1p3.png", plot = combined_plotsf1, width = 23, height = 6, dpi = 300)

```


## Call functions to simulate  n_simulations = 200 datasets using rank r=2 

```{r}
#set seed
set.seed(123)

r <- 2  # Rank of simulated data

datas2k = simulations_data(n,p,r,k,n_simulations)

Beta_matrix_1=datas2k$Beta_matrix
colnames(Beta_matrix_1) <- paste("k=", 1:ncol(Beta_matrix_1), sep="")
rownames(Beta_matrix_1) <- paste("X", 1:nrow(Beta_matrix_1), sep="")
beta_df <- reshape2::melt(Beta_matrix_1)
my_palette <- colorRampPalette(c("lightgrey", "orange", "black"))(100)
breaks <- seq(0.045,1.4192, length.out = 101)
norm_breaks <- (breaks - min(breaks)) / (max(breaks) - min(breaks))
heatmap12_2_real <- ggplot(beta_df, aes(x = Var2, y = Var1, fill = value)) +
  geom_tile() +
  scale_fill_gradientn(colors = my_palette, 
                       values = norm_breaks, # specify normalized breaks here
                       limits = range(breaks)) + # Set limits to the min and max of your breaks
  theme_minimal() +
  labs(x = "", y = "", fill = "Value") +
  theme(     axis.text.x = element_text(angle = 90, hjust = 1, size = 15),       axis.text.y = element_text(size = 15),                              axis.title.x = element_text(size = 15),                            axis.title.y = element_text(size = 15),                            legend.text = element_text(size = 20),                              legend.title = element_text(size = 20),                            plot.title = element_text(size = 30),                strip.text = element_text(size = 20)                              )+
  ggtitle("Simulated under rank 2")
```

### Generated Data Statistics Plots

```{r}
twelve2= generate_data_plots(datas2k$dlong_list,5000,2,10,8,datas2k$T_sim_list,datas2k$T_obs_list)
```

### Fit simulated datasets with model of rank R2=1

#### Get performance
```{r}
#set seed
set.seed(123)

R2 = 1 #rank of fitted model

performance_results21k = simulations_fit_and_preformance(datas2k, p,k,n_simulations,R2)

```


#### Get Average Beta Heatmap
```{r}
beta_heat_12_21=beta_heatmap("Fitted under rank 1",datas2k, performance_results21k,0.045,1.4192)
```

#### Plot performance
```{r}
r21k = generate_histograms(performance_results21k, 8,1)
```
#### ΜC ERRORS

```{r}
msesmc = unlist(lapply(performance_results21k$mse_for_all_coeffs_and_simulation, as.vector))
biasmc = unlist(lapply(performance_results21k$bias_for_all_coeffs_and_simulations, as.vector))
MC_errors21k = MC_errors_calc(msesmc,biasmc)
```


#### LP ERRORS

```{r}
LP_errorstwe21= linear_predictor_err(datas2k, performance_results21k)

lp_errors_mc = unlist(lapply(LP_errorstwe21$lp_errors, as.vector))

MC_error_lp_twe21=sd(lp_errors_mc)/sqrt(length(lp_errors_mc))
```


### Fit simulated datasets with model of rank R2=2

#### Get performance
```{r}
#set seed
set.seed(123)

R2 = 2 #rank of fitted model

performance_results22k= simulations_fit_and_preformance(datas2k, p,k,n_simulations,R2)

```



#### Get Average Beta Heatmap
```{r}
beta_heat_12_22=beta_heatmap("Fitted under rank 2",datas2k, performance_results22k,0.045,1.4192)
```

#### Plot performance
```{r}
r22k = generate_histograms(performance_results22k, 8,2)
```

#### ΜC ERRORS

```{r}
msesmc = unlist(lapply(performance_results22k$mse_for_all_coeffs_and_simulation, as.vector))
biasmc = unlist(lapply(performance_results22k$bias_for_all_coeffs_and_simulations, as.vector))
MC_errors22k = MC_errors_calc(msesmc,biasmc)
```


#### LP ERRORS

```{r}
LP_errorstwe22= linear_predictor_err(datas2k, performance_results22k)

lp_errors_mc = unlist(lapply(LP_errorstwe22$lp_errors, as.vector))

MC_error_lp_twe22=sd(lp_errors_mc)/sqrt(length(lp_errors_mc))
```


### Fit simulated datasets with model of rank R2=3

#### Get performance
```{r}
#set seed
set.seed(123)

R2 = 3 #rank of fitted model

performance_results23k= simulations_fit_and_preformance(datas2k, p,k,n_simulations,R2)

```


#### Get Average Beta Heatmap
```{r}
beta_heat_12_23=beta_heatmap("Fitted under rank 3",datas2k, performance_results23k,0.045,1.4192)
```


#### Plot performance
```{r}
r23k = generate_histograms(performance_results23k, 8,3)
```
#### ΜC ERRORS

```{r}
msesmc = unlist(lapply(performance_results23k$mse_for_all_coeffs_and_simulation, as.vector))
biasmc = unlist(lapply(performance_results23k$bias_for_all_coeffs_and_simulations, as.vector))
MC_errors23k = MC_errors_calc(msesmc,biasmc)
```



#### LP ERRORS

```{r}
LP_errorstwe23= linear_predictor_err(datas2k, performance_results23k)

lp_errors_mc = unlist(lapply(LP_errorstwe23$lp_errors, as.vector))

MC_error_lp_twe23=sd(lp_errors_mc)/sqrt(length(lp_errors_mc))
```


#### Plots

```{r}
plot_listf1 <- list()
plot_listf1[[1]]=r21k$pL2
plot_listf1[[2]]=r22k$pL2
plot_listf1[[3]]=r23k$pL2


top=paste('Histograms of mean absolute bias(left) and MSE(right) for 200 simulated under rank 2 datasets with 5000 individuals, 10 predictors and 8 outcomes when fitted with ranks 1,2 or 3')
top=''
combined_plotsf1 <- grid.arrange(grobs = plot_listf1, ncol = 3, nrow = 1,top=top)

# Save the combined plot grid as a PNG
ggsave("combined_plotstwelve2p1.png", plot = combined_plotsf1, width = 23, height = 6, dpi = 300)


plot_listf1 <- list()
plot_listf1[[1]]=r21k$pL3
plot_listf1[[2]]=r22k$pL3
plot_listf1[[3]]=r23k$pL3


top=paste('Histograms of mean bias per outcome for 200 simulated under rank 2 datasets with 5000 individuals, 10 predictors and 8 outcomes when fitted with ranks 1,2 or 3')
top=''
combined_plotsf1 <- grid.arrange(grobs = plot_listf1, ncol = 3, nrow = 1,top=top)

# Save the combined plot grid as a PNG
ggsave("combined_plotstwelve2p2.png", plot = combined_plotsf1, width = 23, height = 6, dpi = 300)


plot_listf1 <- list()
plot_listf1[[1]]=r21k$pL4
plot_listf1[[2]]=r22k$pL4
plot_listf1[[3]]=r23k$pL4


top=paste('Histograms of mean MSE per outcome for 200 simulated under rank 2 datasets with 5000 individuals, 10 predictors and 8 outcomes when fitted with ranks 1,2 or 3')
top=''
combined_plotsf1 <- grid.arrange(grobs = plot_listf1, ncol = 3, nrow = 1,top=top)

# Save the combined plot grid as a PNG
ggsave("combined_plotstwelve2p3.png", plot = combined_plotsf1, width = 23, height = 6, dpi = 300)

```


### Heatmaps

```{r}
plot_listf1 <- list()
plot_listf1[[1]]=heatmap12_1_real
plot_listf1[[2]]=beta_heat_12_11
plot_listf1[[3]]=beta_heat_12_12
plot_listf1[[4]]=NULL
plot_listf1[[5]]=heatmap12_2_real
plot_listf1[[6]]=beta_heat_12_21
plot_listf1[[7]]=beta_heat_12_22
plot_listf1[[8]]=beta_heat_12_23

top=''

heatmaps_12 <- grid.arrange(grobs = plot_listf1, ncol = 4, nrow = 2,top=top)

# Save the combined plot grid as a PNG
ggsave("heatmaps_12.png", plot = heatmaps_12, width = 23, height = 6, dpi = 300)
```

# Final plots


## Data
```{r}
plot_list1 <- list()
plot_list1[[1]]=first1
plot_list1[[2]]=second1
plot_list1[[3]]=third1
plot_list1[[4]]=fourth1

plot_list1[[5]]=fifth1
plot_list1[[6]]=sixth1
plot_list1[[7]]=seventh1
plot_list1[[8]]=eight1

plot_list1[[9]]=nine1
plot_list1[[10]]=ten1
plot_list1[[11]]=eleven1
plot_list1[[12]]=twelve1


top=paste('Simulation study 1, 200 datasets simulated per scenario using case 1 and rank 1')
top=''
combined_plots1 <- grid.arrange(grobs = plot_list1, ncol = 3, nrow = 4,top=top)

# Save the combined plot grid as a PNG
ggsave("combined_plots_1.png", plot = combined_plots1, width = 20, height = 15, dpi = 300)


plot_list2 <- list()
plot_list2[[1]]=first2
plot_list2[[2]]=second2
plot_list2[[3]]=third2
plot_list2[[4]]=fourth2

plot_list2[[5]]=fifth2
plot_list2[[6]]=sixth2
plot_list2[[7]]=seventh2
plot_list2[[8]]=eight2

plot_list2[[9]]=nine2
plot_list2[[10]]=ten2
plot_list2[[11]]=eleven2
plot_list2[[12]]=twelve2

top=paste('Simulation study 1, 200 datasets simulated per scenario using case 1 and rank 2')
top=''
combined_plots2 <- grid.arrange(grobs = plot_list2, ncol = 3, nrow = 4,top=top)

# Save the combined plot grid as a PNG
ggsave("combined_plots_2.png", plot = combined_plots2, width = 20, height = 15, dpi = 300)
```

## MSE

```{r}
f_mean_mse = c(mean(performance_results11$mse1),
               mean(performance_results12$mse1),
               # mean(performance_results13$mse1),
               mean(performance_results21$mse1),
               mean(performance_results22$mse1),
               mean(performance_results23$mse1),

               mean(performance_results11a$mse1),
               mean(performance_results12a$mse1),
               # mean(performance_results13a$mse1),
               mean(performance_results21a$mse1),
               mean(performance_results22a$mse1),
               mean(performance_results23a$mse1),

               mean(performance_results11b$mse1),
               mean(performance_results12b$mse1),
               # mean(performance_results13b$mse1),
               mean(performance_results21b$mse1),
               mean(performance_results22b$mse1),
               mean(performance_results23b$mse1),

               mean(performance_results11c$mse1),
               mean(performance_results12c$mse1),
               # mean(performance_results13c$mse1),
               mean(performance_results21c$mse1),
               mean(performance_results22c$mse1),
               mean(performance_results23c$mse1),

               mean(performance_results11d$mse1),
               mean(performance_results12d$mse1),
               # mean(performance_results13d$mse1),
               mean(performance_results21d$mse1),
               mean(performance_results22d$mse1),
               mean(performance_results23d$mse1),

               mean(performance_results11e$mse1),
               mean(performance_results12e$mse1),
               # mean(performance_results13e$mse1),
               mean(performance_results21e$mse1),
               mean(performance_results22e$mse1),
               mean(performance_results23e$mse1),
               
               mean(performance_results11f$mse1),
               mean(performance_results12f$mse1),
               # mean(performance_results13f$mse1),
               mean(performance_results21f$mse1),
               mean(performance_results22f$mse1),
               mean(performance_results23f$mse1),
               
               mean(performance_results11g$mse1),
               mean(performance_results12g$mse1),
               # mean(performance_results13g$mse1),
               mean(performance_results21g$mse1),
               mean(performance_results22g$mse1),
               mean(performance_results23g$mse1),
               
               
               mean(performance_results11h$mse1),
               mean(performance_results12h$mse1),
               # mean(performance_results13h$mse1),
               mean(performance_results21h$mse1),
               mean(performance_results22h$mse1),
               mean(performance_results23h$mse1),
               
               
               mean(performance_results11i$mse1),
               mean(performance_results12i$mse1),
               # mean(performance_results13i$mse1),
               mean(performance_results21i$mse1),
               mean(performance_results22i$mse1),
               mean(performance_results23i$mse1),
               
               mean(performance_results11j$mse1),
               mean(performance_results12j$mse1),
               # mean(performance_results13j$mse1),
               mean(performance_results21j$mse1),
               mean(performance_results22j$mse1),
               mean(performance_results23j$mse1),
              
               mean(performance_results11k$mse1),
               mean(performance_results12k$mse1),
               # mean(performance_results13k$mse1),
               mean(performance_results21k$mse1),
               mean(performance_results22k$mse1),
               mean(performance_results23k$mse1))

f_mc_error_mse = c(MC_errors11$MC_error_mse,
                   MC_errors12$MC_error_mse,
                   # MC_errors13$MC_error_mse,
                   MC_errors21$MC_error_mse,
                   MC_errors22$MC_error_mse,
                   MC_errors23$MC_error_mse,
    
                   MC_errors11a$MC_error_mse,
                   MC_errors12a$MC_error_mse,
                   # MC_errors13a$MC_error_mse,
                   MC_errors21a$MC_error_mse,
                   MC_errors22a$MC_error_mse,
                   MC_errors23a$MC_error_mse,
    
                   MC_errors11b$MC_error_mse,
                   MC_errors12b$MC_error_mse,
                   # MC_errors13b$MC_error_mse,
                   MC_errors21b$MC_error_mse,
                   MC_errors22b$MC_error_mse,
                   MC_errors23b$MC_error_mse,
    
                   MC_errors11c$MC_error_mse,
                   MC_errors12c$MC_error_mse,
                   # MC_errors13c$MC_error_mse,
                   MC_errors21c$MC_error_mse,
                   MC_errors22c$MC_error_mse,
                   MC_errors23c$MC_error_mse,
    
                   MC_errors11d$MC_error_mse,
                   MC_errors12d$MC_error_mse,
                   # MC_errors13d$MC_error_mse,
                   MC_errors21d$MC_error_mse,
                   MC_errors22d$MC_error_mse,
                   MC_errors23d$MC_error_mse,
                   
                   MC_errors11e$MC_error_mse,
                   MC_errors12e$MC_error_mse,
                   # MC_errors13e$MC_error_mse,
                   MC_errors21e$MC_error_mse,
                   MC_errors22e$MC_error_mse,
                   MC_errors23e$MC_error_mse,
                   
                   MC_errors11f$MC_error_mse,
                   MC_errors12f$MC_error_mse,
                   # MC_errors13f$MC_error_mse,
                   MC_errors21f$MC_error_mse,
                   MC_errors22f$MC_error_mse,
                   MC_errors23f$MC_error_mse,
                   
                   MC_errors11g$MC_error_mse,
                   MC_errors12g$MC_error_mse,
                   # MC_errors13g$MC_error_mse,
                   MC_errors21g$MC_error_mse,
                   MC_errors22g$MC_error_mse,
                   MC_errors23g$MC_error_mse,
                   
                   MC_errors11h$MC_error_mse,
                   MC_errors12h$MC_error_mse,
                   # MC_errors13h$MC_error_mse,
                   MC_errors21h$MC_error_mse,
                   MC_errors22h$MC_error_mse,
                   MC_errors23h$MC_error_mse,
                   
                   
                   MC_errors11i$MC_error_mse,
                   MC_errors12i$MC_error_mse,
                   # MC_errors13i$MC_error_mse,
                   MC_errors21i$MC_error_mse,
                   MC_errors22i$MC_error_mse,
                   MC_errors23i$MC_error_mse,
                   
                   MC_errors11j$MC_error_mse,
                   MC_errors12j$MC_error_mse,
                   # MC_errors13j$MC_error_mse,
                   MC_errors21j$MC_error_mse,
                   MC_errors22j$MC_error_mse,
                   MC_errors23j$MC_error_mse,
                  
                   MC_errors11k$MC_error_mse,
                   MC_errors12k$MC_error_mse,
                   # MC_errors13k$MC_error_mse,
                   MC_errors21k$MC_error_mse,
                   MC_errors22k$MC_error_mse,
                   MC_errors23k$MC_error_mse)


f_ci_lower_mse = f_mean_mse  - 1.96*f_mc_error_mse
f_ci_upper_mse = f_mean_mse  + 1.96*f_mc_error_mse
```

## BIAS

```{r}
f_mean_bias =  c(mean(performance_results11$bias1),
                 mean(performance_results12$bias1),
                 # mean(performance_results13$bias1),
                 mean(performance_results21$bias1),
                 mean(performance_results22$bias1),
                 mean(performance_results23$bias1),
  
                 mean(performance_results11a$bias1),
                 mean(performance_results12a$bias1),
                 # mean(performance_results13a$bias1),
                 mean(performance_results21a$bias1),
                 mean(performance_results22a$bias1),
                 mean(performance_results23a$bias1),
  
                 mean(performance_results11b$bias1),
                 mean(performance_results12b$bias1),
                 # mean(performance_results13b$bias1),
                 mean(performance_results21b$bias1),
                 mean(performance_results22b$bias1),
                 mean(performance_results23b$bias1),
  
                 mean(performance_results11c$bias1),
                 mean(performance_results12c$bias1),
                 # mean(performance_results13c$bias1),
                 mean(performance_results21c$bias1),
                 mean(performance_results22c$bias1),
                 mean(performance_results23c$bias1),
  
                 mean(performance_results11d$bias1),
                 mean(performance_results12d$bias1),
                 # mean(performance_results13d$bias1),
                 mean(performance_results21d$bias1),
                 mean(performance_results22d$bias1),
                 mean(performance_results23d$bias1),
  
                 mean(performance_results11e$bias1),
                 mean(performance_results12e$bias1),
                 # mean(performance_results13e$bias1),
                 mean(performance_results21e$bias1),
                 mean(performance_results22e$bias1),
                 mean(performance_results23e$bias1),
                 
                 mean(performance_results11f$bias1),
                 mean(performance_results12f$bias1),
                 # mean(performance_results13f$bias1),
                 mean(performance_results21f$bias1),
                 mean(performance_results22f$bias1),
                 mean(performance_results23f$bias1),
                 
                 mean(performance_results11g$bias1),
                 mean(performance_results12g$bias1),
                 # mean(performance_results13g$bias1),
                 mean(performance_results21g$bias1),
                 mean(performance_results22g$bias1),
                 mean(performance_results23g$bias1),
                 
                 
                 mean(performance_results11h$bias1),
                 mean(performance_results12h$bias1),
                 # mean(performance_results13h$bias1),
                 mean(performance_results21h$bias1),
                 mean(performance_results22h$bias1),
                 mean(performance_results23h$bias1),
                 
                 
                 mean(performance_results11i$bias1),
                 mean(performance_results12i$bias1),
                 # mean(performance_results13i$bias1),
                 mean(performance_results21i$bias1),
                 mean(performance_results22i$bias1),
                 mean(performance_results23i$bias1),
                 
                 mean(performance_results11j$bias1),
                 mean(performance_results12j$bias1),
                 # mean(performance_results13j$bias1),
                 mean(performance_results21j$bias1),
                 mean(performance_results22j$bias1),
                 mean(performance_results23j$bias1),
                
                 mean(performance_results11k$bias1),
                 mean(performance_results12k$bias1),
                 # mean(performance_results13k$bias1),
                 mean(performance_results21k$bias1),
                 mean(performance_results22k$bias1),
                 mean(performance_results23k$bias1)
                 )

f_mc_error_bias =  c(MC_errors11$MC_error_bias,
                     MC_errors12$MC_error_bias,
                     # MC_errors13$MC_error_bias,
                     MC_errors21$MC_error_bias,
                     MC_errors22$MC_error_bias,
                     MC_errors23$MC_error_bias,
      
                     MC_errors11a$MC_error_bias,
                     MC_errors12a$MC_error_bias,
                     # MC_errors13a$MC_error_bias,
                     MC_errors21a$MC_error_bias,
                     MC_errors22a$MC_error_bias,
                     MC_errors23a$MC_error_bias,
      
                     MC_errors11b$MC_error_bias,
                     MC_errors12b$MC_error_bias,
                     # MC_errors13b$MC_error_bias,
                     MC_errors21b$MC_error_bias,
                     MC_errors22b$MC_error_bias,
                     MC_errors23b$MC_error_bias,
      
                     MC_errors11c$MC_error_bias,
                     MC_errors12c$MC_error_bias,
                     # MC_errors13c$MC_error_bias,
                     MC_errors21c$MC_error_bias,
                     MC_errors22c$MC_error_bias,
                     MC_errors23c$MC_error_bias,
      
                     MC_errors11d$MC_error_bias,
                     MC_errors12d$MC_error_bias,
                     # MC_errors13d$MC_error_bias,
                     MC_errors21d$MC_error_bias,
                     MC_errors22d$MC_error_bias,
                     MC_errors23d$MC_error_bias,
                     
                     MC_errors11e$MC_error_bias,
                     MC_errors12e$MC_error_bias,
                     # MC_errors13e$MC_error_bias,
                     MC_errors21e$MC_error_bias,
                     MC_errors22e$MC_error_bias,
                     MC_errors23e$MC_error_bias,
                     
                     MC_errors11f$MC_error_bias,
                     MC_errors12f$MC_error_bias,
                     # MC_errors13f$MC_error_bias,
                     MC_errors21f$MC_error_bias,
                     MC_errors22f$MC_error_bias,
                     MC_errors23f$MC_error_bias,
                     
                     MC_errors11g$MC_error_bias,
                     MC_errors12g$MC_error_bias,
                     # MC_errors13g$MC_error_bias,
                     MC_errors21g$MC_error_bias,
                     MC_errors22g$MC_error_bias,
                     MC_errors23g$MC_error_bias,
                     
                     MC_errors11h$MC_error_bias,
                     MC_errors12h$MC_error_bias,
                     # MC_errors13h$MC_error_bias,
                     MC_errors21h$MC_error_bias,
                     MC_errors22h$MC_error_bias,
                     MC_errors23h$MC_error_bias,
                     
                     
                     MC_errors11i$MC_error_bias,
                     MC_errors12i$MC_error_bias,
                     # MC_errors13i$MC_error_bias,
                     MC_errors21i$MC_error_bias,
                     MC_errors22i$MC_error_bias,
                     MC_errors23i$MC_error_bias,
                     
                     MC_errors11j$MC_error_bias,
                     MC_errors12j$MC_error_bias,
                     # MC_errors13j$MC_error_bias,
                     MC_errors21j$MC_error_bias,
                     MC_errors22j$MC_error_bias,
                     MC_errors23j$MC_error_bias,
                    
                     MC_errors11k$MC_error_bias,
                     MC_errors12k$MC_error_bias,
                     # MC_errors13k$MC_error_bias,
                     MC_errors21k$MC_error_bias,
                     MC_errors22k$MC_error_bias,
                     MC_errors23k$MC_error_bias)

f_ci_lower_bias = f_mean_bias  - 1.96*f_mc_error_bias
f_ci_upper_bias = f_mean_bias  + 1.96*f_mc_error_bias
```



## LP error

```{r}
f_mean_lp_error =  c(mean(unlist(lapply(LP_errorsfirst11$ave_lp_errors, as.vector))),
                 mean(unlist(lapply(LP_errorsfirst12$ave_lp_errors, as.vector))),
                 # mean(unlist(lapply(LP_errorsfirst13$ave_lp_errors, as.vector))),
                 mean(unlist(lapply(LP_errorsfirst21$ave_lp_errors, as.vector))),
                 mean(unlist(lapply(LP_errorsfirst22$ave_lp_errors, as.vector))),
                 mean(unlist(lapply(LP_errorsfirst23$ave_lp_errors, as.vector))),
  
                 mean(unlist(lapply(LP_errorssecond11$ave_lp_errors, as.vector))),
                 mean(unlist(lapply(LP_errorssecond12$ave_lp_errors, as.vector))),
                 # mean(unlist(lapply(LP_errorssecond13$ave_lp_errors, as.vector))),
                 mean(unlist(lapply(LP_errorssecond21$ave_lp_errors, as.vector))),
                 mean(unlist(lapply(LP_errorssecond22$ave_lp_errors, as.vector))),
                 mean(unlist(lapply(LP_errorssecond23$ave_lp_errors, as.vector))),
  
                 mean(unlist(lapply(LP_errorsthird11$ave_lp_errors, as.vector))),
                 mean(unlist(lapply(LP_errorsthird12$ave_lp_errors, as.vector))),
                 # mean(unlist(lapply(LP_errorsthird13$ave_lp_errors, as.vector))),
                 mean(unlist(lapply(LP_errorsthird21$ave_lp_errors, as.vector))),
                 mean(unlist(lapply(LP_errorsthird22$ave_lp_errors, as.vector))),
                 mean(unlist(lapply(LP_errorsthird23$ave_lp_errors, as.vector))),
  
                 mean(unlist(lapply(LP_errorsfourth11$ave_lp_errors, as.vector))),
                 mean(unlist(lapply(LP_errorsfourth12$ave_lp_errors, as.vector))),
                 # mean(unlist(lapply(LP_errorsfourth13$ave_lp_errors, as.vector))),
                 mean(unlist(lapply(LP_errorsfourth21$ave_lp_errors, as.vector))),
                 mean(unlist(lapply(LP_errorsfourth22$ave_lp_errors, as.vector))),
                 mean(unlist(lapply(LP_errorsfourth23$ave_lp_errors, as.vector))),
  
               
                mean(unlist(lapply(LP_errorsfifth11$ave_lp_errors, as.vector))),
                mean(unlist(lapply(LP_errorsfifth12$ave_lp_errors, as.vector))),
                # mean(unlist(lapply(LP_errorsfifth13$ave_lp_errors, as.vector))),
                mean(unlist(lapply(LP_errorsfifth21$ave_lp_errors, as.vector))),
                mean(unlist(lapply(LP_errorsfifth22$ave_lp_errors, as.vector))),
                mean(unlist(lapply(LP_errorsfifth23$ave_lp_errors, as.vector))),
                
                
                
                mean(unlist(lapply(LP_errorssix11$ave_lp_errors, as.vector))),
                mean(unlist(lapply(LP_errorssix12$ave_lp_errors, as.vector))),
                # mean(unlist(lapply(LP_errorssix13$ave_lp_errors, as.vector))),
                mean(unlist(lapply(LP_errorssix21$ave_lp_errors, as.vector))),
                mean(unlist(lapply(LP_errorssix22$ave_lp_errors, as.vector))),
                mean(unlist(lapply(LP_errorssix23$ave_lp_errors, as.vector))),

                
                
  
                mean(unlist(lapply(LP_errorssev11$ave_lp_errors, as.vector))),
                mean(unlist(lapply(LP_errorssev12$ave_lp_errors, as.vector))),
                # mean(unlist(lapply(LP_errorssev13$ave_lp_errors, as.vector))),
                mean(unlist(lapply(LP_errorssev21$ave_lp_errors, as.vector))),
                mean(unlist(lapply(LP_errorssev22$ave_lp_errors, as.vector))),
                mean(unlist(lapply(LP_errorssev23$ave_lp_errors, as.vector))),
                 
                mean(unlist(lapply(LP_errorseig11$ave_lp_errors, as.vector))),
                mean(unlist(lapply(LP_errorseig12$ave_lp_errors, as.vector))),
                # mean(unlist(lapply(LP_errorseig13$ave_lp_errors, as.vector))),
                mean(unlist(lapply(LP_errorseig21$ave_lp_errors, as.vector))),
                mean(unlist(lapply(LP_errorseig22$ave_lp_errors, as.vector))),
                mean(unlist(lapply(LP_errorseig23$ave_lp_errors, as.vector))),
                 
                mean(unlist(lapply(LP_errorsnine11$ave_lp_errors, as.vector))),
                mean(unlist(lapply(LP_errorsnine12$ave_lp_errors, as.vector))),
                # mean(unlist(lapply(LP_errorsnine13$ave_lp_errors, as.vector))),
                mean(unlist(lapply(LP_errorsnine21$ave_lp_errors, as.vector))),
                mean(unlist(lapply(LP_errorsnine22$ave_lp_errors, as.vector))),
                mean(unlist(lapply(LP_errorsnine23$ave_lp_errors, as.vector))),
                 
                 
                mean(unlist(lapply(LP_errorsten11$ave_lp_errors, as.vector))),
                mean(unlist(lapply(LP_errorsten12$ave_lp_errors, as.vector))),
                # mean(unlist(lapply(LP_errorsten13$ave_lp_errors, as.vector))),
                mean(unlist(lapply(LP_errorsten21$ave_lp_errors, as.vector))),
                mean(unlist(lapply(LP_errorsten22$ave_lp_errors, as.vector))),
                mean(unlist(lapply(LP_errorsten23$ave_lp_errors, as.vector))),

                 
                 
                mean(unlist(lapply(LP_errorsele11$ave_lp_errors, as.vector))),
                mean(unlist(lapply(LP_errorsele12$ave_lp_errors, as.vector))),
                # mean(unlist(lapply(LP_errorsele13$ave_lp_errors, as.vector))),
                mean(unlist(lapply(LP_errorsele21$ave_lp_errors, as.vector))),
                mean(unlist(lapply(LP_errorsele22$ave_lp_errors, as.vector))),
                mean(unlist(lapply(LP_errorsele23$ave_lp_errors, as.vector))),
                 
                
                
                mean(unlist(lapply(LP_errorstwe11$ave_lp_errors, as.vector))),
                mean(unlist(lapply(LP_errorstwe12$ave_lp_errors, as.vector))),
                # mean(unlist(lapply(LP_errorstweh13$ave_lp_errors, as.vector))),
                mean(unlist(lapply(LP_errorstwe21$ave_lp_errors, as.vector))),
                mean(unlist(lapply(LP_errorstwe22$ave_lp_errors, as.vector))),
                mean(unlist(lapply(LP_errorstwe23$ave_lp_errors, as.vector)))
                
                 )

f_mc_error_lp =  c(MC_error_lp_first11,
                     MC_error_lp_first12,
                     # MC_error_lp_first13,
                     MC_error_lp_first21,
                     MC_error_lp_first22,
                     MC_error_lp_first23,
      
                     MC_error_lp_second11,
                     MC_error_lp_second12,
                     # MC_error_lp_second13,
                     MC_error_lp_second21,
                     MC_error_lp_second22,
                     MC_error_lp_second23,
      
                     MC_error_lp_third11,
                     MC_error_lp_third12,
                     # MC_error_lp_third13,
                     MC_error_lp_third21,
                     MC_error_lp_third22,
                     MC_error_lp_third23,
      
                     MC_error_lp_fourth11,
                     MC_error_lp_fourth12,
                     # MC_error_lp_fourth13,
                     MC_error_lp_fourth21,
                     MC_error_lp_fourth22,
                     MC_error_lp_fourth23,
      
                     MC_error_lp_fifth11,
                     MC_error_lp_fifth12,
                     # MC_error_lp_fifth13,
                     MC_error_lp_fifth21,
                     MC_error_lp_fifth22,
                     MC_error_lp_fifth23,
                     
                     MC_error_lp_six11,
                     MC_error_lp_six12,
                     # MC_error_lp_six13,
                     MC_error_lp_six21,
                     MC_error_lp_six22,
                     MC_error_lp_six23,
                     
                     MC_error_lp_sev11,
                     MC_error_lp_sev12,
                     # MC_error_lp_sev13,
                     MC_error_lp_sev21,
                     MC_error_lp_sev22,
                     MC_error_lp_sev23,
                     
                     MC_error_lp_eig11,
                     MC_error_lp_eig12,
                     # MC_error_lp_eig13,
                     MC_error_lp_eig21,
                     MC_error_lp_eig22,
                     MC_error_lp_eig23,
                     
                     MC_error_lp_nine11,
                     MC_error_lp_nine12,
                     # MC_error_lp_nine13,
                     MC_error_lp_nine21,
                     MC_error_lp_nine22,
                     MC_error_lp_nine23,
                     
                     
                     MC_error_lp_ten11,
                     MC_error_lp_ten12,
                     # MC_error_lp_ten13,
                     MC_error_lp_ten21,
                     MC_error_lp_ten22,
                     MC_error_lp_ten23,
                     
                     MC_error_lp_ele11,
                     MC_error_lp_ele12,
                     # MC_error_lp_ele13,
                     MC_error_lp_ele21,
                     MC_error_lp_ele22,
                     MC_error_lp_ele23,
                    
                     MC_error_lp_twe11,
                     MC_error_lp_twe12,
                     # MC_error_lp_twe13,
                     MC_error_lp_twe21,
                     MC_error_lp_twe22,
                     MC_error_lp_twe23)

f_ci_lower_lperr = f_mean_lp_error  - 1.96*f_mc_error_lp
f_ci_upper_lperr = f_mean_lp_error  + 1.96*f_mc_error_lp
```





## Plot with Means and MC errors

```{r}
xvecs = c(rep(1,2),rep(2,3))
xvecf = c(1,2,1,2,3)
results_final_mse <- data.frame(
  sample_size = c(rep(c(500, 2500, 5000), times = 4, each = 5)),
  covariates = c(rep(5,15),rep(10,15),rep(5,15),rep(10,15)),
  outcomes = c(rep(4,30),rep(8,30)),
  ranks_simulate = c(rep(xvecs,12)), #1
  ranks_fit = c(rep(xvecf,12)), #2
  mean_mse = f_mean_mse,
  ci_lower = f_ci_lower_mse,
  ci_upper = f_ci_upper_mse
  
)


plotmse = ggplot(results_final_mse, aes(x = sample_size, y = mean_mse, color = factor(ranks_fit), shape = factor(ranks_simulate))) +
  geom_point(size = 1.5, alpha = 0.5,position = position_dodge(width = 1000))+
  geom_errorbar(aes(ymin = ci_lower, ymax = ci_upper), width = 20, position = position_dodge(width = 1000)) +
   geom_vline(xintercept = c(500, 2500, 5000), linetype = "dashed", color = "grey", linewidth = 0.55) +  # Add vertical lines
  facet_grid(covariates ~ outcomes, labeller = label_both) +
  scale_x_continuous(breaks = c(500, 2500, 5000), labels=c('500', '1000', '5000')) +
  scale_y_continuous(breaks = pretty_breaks(n = 8), limits = c(0, max(results_final_mse$mean_mse))) +
  scale_color_manual(values = c("purple","#F8766D", "#00BA38"), name = "Ranks Fit") +
  scale_shape_manual(values = c(16, 17), name = "Ranks Simulate") +
  labs(
    x = "Sample size (n)",
    y = "Average MSE",
    title = "Simulation study 1 results",
    subtitle = "Average MSE for different combinations of parameters \n with 95% Monte Carlo confidence intervals"
  ) +
  theme_minimal() +
  theme(
    axis.title.x = element_text(size = 14), 
    axis.title.y = element_text(size = 14),  
    plot.title = element_text(hjust = 0.5, size = 20),
    plot.subtitle = element_text(hjust = 0.5, size = 14),
    strip.text = element_text(size = 14),
    legend.key.size = unit(0.1, "lines"),  #legend keys
    legend.text = element_text(size = 14),  #legend text
    legend.title = element_text(size = 14),  # legend title
    panel.grid.major.x = element_blank(),  # Remove major vertical grid lines
    panel.grid.minor.x = element_blank(),  # Remove minor vertical grid lines
    panel.grid.major.y = element_line(color = "grey", linewidth = 0.5),  # Keep major horizontal grid lines
    panel.grid.minor.y = element_blank()  # Remove minor horizontal grid lines
  )


results_final_bias <- data.frame(
  sample_size = c(rep(c(500, 2500, 5000), times = 4, each = 5)),
  covariates = c(rep(5,15),rep(10,15),rep(5,15),rep(10,15)),
  outcomes = c(rep(4,30),rep(8,30)),
  ranks_simulate = c(rep(xvecs,12)), #1
  ranks_fit = c(rep(xvecf,12)), #2
  mean_bias = f_mean_bias,
  ci_lower = f_ci_lower_bias,
  ci_upper = f_ci_upper_bias
)


plotbias = ggplot(results_final_bias, aes(x = sample_size, y = mean_bias, color = factor(ranks_fit), shape = factor(ranks_simulate))) +
  geom_point(size = 1.5, alpha = 0.5,position = position_dodge(width = 1000))+
  geom_errorbar(aes(ymin = ci_lower, ymax = ci_upper), width = 20, position = position_dodge(width = 1000)) +
  geom_vline(xintercept = c(500, 2500, 5000), linetype = "dashed", color = "grey",linewidth= 0.55) +  # Add vertical lines
  facet_grid(covariates ~ outcomes, labeller = label_both) +
  scale_x_continuous(breaks = c(500, 2500, 5000), labels=c('500', '1000', '5000')) +
  scale_y_continuous(breaks = pretty_breaks(n = 5), limits = c(0, max(max(results_final_bias$mean_bias),max(results_final_lp$mean_lp)))) +
  scale_color_manual(values = c("purple", "#F8766D","#00BA38"), name = "Ranks Fit") +
  scale_shape_manual(values = c(16, 17), name = "Ranks Simulate") +
  labs(
    x = "Sample size (n)",
    y = "Average absolute bias",
    title = "",
    subtitle = "Average absolute bias for different combinations of parameters \n with 95% Monte Carlo confidence intervals"
  ) +
  theme_minimal() +
  theme(
    axis.title.x = element_text(size = 14), 
    axis.title.y = element_text(size = 14), 
    plot.title = element_text(hjust = 0.5, size = 20),
    plot.subtitle = element_text(hjust = 0.5, size = 14),
    strip.text = element_text(size = 14),
    legend.key.size = unit(0.1, "lines"),
    legend.text = element_text(size = 14),
    legend.title = element_text(size = 14),
    panel.grid.major.x = element_blank(),  # Remove major vertical grid lines
    panel.grid.minor.x = element_blank(),  # Remove minor vertical grid lines
    panel.grid.major.y = element_line(color = "grey", linewidth = 0.5),  # Keep major horizontal grid lines
    panel.grid.minor.y = element_blank()  # Remove minor horizontal grid lines
  )



results_final_lp <- data.frame(
  sample_size = c(rep(c(500, 2500, 5000), times = 4, each = 5)),
  covariates = c(rep(5,15),rep(10,15),rep(5,15),rep(10,15)),
  outcomes = c(rep(4,30),rep(8,30)),
  ranks_simulate = c(rep(xvecs,12)), #1
  ranks_fit = c(rep(xvecf,12)), #2
  mean_lp = f_mean_lp_error,
  ci_lower = f_ci_lower_lperr,
  ci_upper = f_ci_upper_lperr
)


plotlp = ggplot(results_final_lp, aes(x = sample_size, y = mean_lp, color = factor(ranks_fit), shape = factor(ranks_simulate))) +
  geom_point(size = 1.5, alpha = 0.5,position = position_dodge(width = 1000))+
  geom_errorbar(aes(ymin = ci_lower, ymax = ci_upper), width = 20, position = position_dodge(width = 1000)) +
  geom_vline(xintercept = c(500, 2500, 5000), linetype = "dashed", color = "grey",linewidth= 0.55) +  # Add vertical lines
  facet_grid(covariates ~ outcomes, labeller = label_both) +
  scale_x_continuous(breaks = c(500, 2500, 5000), labels=c('500', '1000', '5000')) +
  scale_y_continuous(breaks = pretty_breaks(n = 5), limits = c(0, max(max(results_final_bias$mean_bias),max(results_final_lp$mean_lp))))+
  scale_color_manual(values = c("purple", "#F8766D","#00BA38"), name = "Ranks Fit") +
  scale_shape_manual(values = c(16, 17), name = "Ranks Simulate") +
  labs(
    x = "Sample size (n)",
    y = "Average linear predictor error",
    title = "",
    subtitle = "Average linear predictor error for different combinations of \n parameters with 95% Monte Carlo confidence intervals"
  ) +
  theme_minimal() +
  theme(
    axis.title.x = element_text(size = 14), 
    axis.title.y = element_text(size = 14), 
    plot.title = element_text(hjust = 0.5, size = 20),
    plot.subtitle = element_text(hjust = 0.5, size = 14),
    strip.text = element_text(size = 14),
    legend.key.size = unit(0.1, "lines"),
    legend.text = element_text(size = 14),
    legend.title = element_text(size = 14),
    panel.grid.major.x = element_blank(),  # Remove major vertical grid lines
    panel.grid.minor.x = element_blank(),  # Remove minor vertical grid lines
    panel.grid.major.y = element_line(color = "grey", linewidth = 0.5),  # Keep major horizontal grid lines
    panel.grid.minor.y = element_blank()  # Remove minor horizontal grid lines
  )




xvecs = c(rep(1,2),rep(2,3))
xvecf = c(1,2,1,2,3)
results_final_mc_mse <- data.frame(
  sample_size = c(rep(c(500, 2500, 5000), times = 4, each = 5)),
  covariates = c(rep(5,15),rep(10,15),rep(5,15),rep(10,15)),
  outcomes = c(rep(4,30),rep(8,30)),
  ranks_simulate = c(rep(xvecs,12)), #1
  ranks_fit = c(rep(xvecf,12)), #2
  mc_error_mse = f_mc_error_mse
)

plotmsemc = ggplot(results_final_mc_mse, aes(x = sample_size, y = mc_error_mse, color = factor(ranks_fit), shape = factor(ranks_simulate))) +
  # geom_jitter(size = 2, alpha = 0.7, width = 100, height = 0)+
  geom_point(size = 1.5, alpha = 0.8,position = position_dodge(width = 1000))+
  # geom_point(size = 2,alpha = 0.6) +
   geom_vline(xintercept = c(500, 2500, 5000), linetype = "dashed", color = "grey", linewidth = 0.55) +  # Add vertical lines
  facet_grid(covariates ~ outcomes, labeller = label_both) +
  scale_x_continuous(breaks = c(500, 2500, 5000), labels=c('500', '1000', '5000')) +
  scale_y_continuous(breaks = pretty_breaks(n = 8), limits = c(0, max(results_final_mc_mse$mc_error_mse))) +
  scale_color_manual(values = c("purple","#F8766D", "#00BA38"), name = "Ranks Fit") +
  scale_shape_manual(values = c(16, 17), name = "Ranks Simulate") +
  labs(
    x = "Sample Size (n)",
    y = "MC ERROR MSE",
    title = "Simulation Study 1 Results",
    subtitle = "MC ERROR MSE for different combinations of parameters"
  ) +
  theme_minimal() +
  theme(
    axis.title.x = element_text(size = 14), 
    axis.title.y = element_text(size = 14), 
    plot.title = element_text(hjust = 0.5, size = 20),
    plot.subtitle = element_text(hjust = 0.5, size = 14),
    strip.text = element_text(size = 14),
    legend.key.size = unit(0.1, "lines"),  #legend keys
    legend.text = element_text(size = 14),  #legend text
    legend.title = element_text(size = 14),  # legend title
    panel.grid.major.x = element_blank(),  # Remove major vertical grid lines
    panel.grid.minor.x = element_blank(),  # Remove minor vertical grid lines
    panel.grid.major.y = element_line(color = "grey", linewidth = 0.5),  # Keep major horizontal grid lines
    panel.grid.minor.y = element_blank()  # Remove minor horizontal grid lines
  )


results_final_mc_bias <- data.frame(
  sample_size = c(rep(c(500, 2500, 5000), times = 4, each = 5)),
  covariates = c(rep(5,15),rep(10,15),rep(5,15),rep(10,15)),
  outcomes = c(rep(4,30),rep(8,30)),
  ranks_simulate = c(rep(xvecs,12)), #1
  ranks_fit = c(rep(xvecf,12)), #2
  mc_error_bias = f_mc_error_bias
)

plotbiasmc = ggplot(results_final_mc_bias, aes(x = sample_size, y = mc_error_bias, color = factor(ranks_fit), shape = factor(ranks_simulate))) +
  # geom_jitter(size = 2, alpha = 0.7, width = 100, height = 0)+
  geom_point(size = 1.5, alpha = 0.8,position = position_dodge(width = 1000))+
  # geom_point(size = 2, alpha = 0.6) +F
  geom_vline(xintercept = c(500, 2500, 5000), linetype = "dashed", color = "grey",linewidth= 0.55) +  # Add vertical lines
  facet_grid(covariates ~ outcomes, labeller = label_both) +
  scale_x_continuous(breaks = c(500, 2500, 5000), labels=c('500', '1000', '5000')) +
  scale_y_continuous(breaks = pretty_breaks(n = 5), limits = c(0, max(results_final_mc_bias$mc_error_bias))) +
  scale_color_manual(values = c("purple", "#F8766D","#00BA38"), name = "Ranks Fit") +
  scale_shape_manual(values = c(16, 17), name = "Ranks Simulate") +
  labs(
    x = "Sample Size (n)",
    y = "MC ERROR Absolute Bias",
    title = "Simulation Study 1 Results",
    subtitle = "MC ERROR Absolute Bias for different combinations of parameters"
  ) +
  theme_minimal() +
  theme(
    axis.title.x = element_text(size = 14), 
    axis.title.y = element_text(size = 14), 
    plot.title = element_text(hjust = 0.5, size = 20),
    plot.subtitle = element_text(hjust = 0.5, size = 14),
    strip.text = element_text(size = 14),
    legend.key.size = unit(0.1, "lines"),
    legend.text = element_text(size = 14),
    legend.title = element_text(size = 14),
    panel.grid.major.x = element_blank(),  # Remove major vertical grid lines
    panel.grid.minor.x = element_blank(),  # Remove minor vertical grid lines
    panel.grid.major.y = element_line(color = "grey", linewidth = 0.5),  # Keep major horizontal grid lines
    panel.grid.minor.y = element_blank()  # Remove minor horizontal grid lines
  )


results_final_mc_lp <- data.frame(
  sample_size = c(rep(c(500, 2500, 5000), times = 4, each = 5)),
  covariates = c(rep(5,15),rep(10,15),rep(5,15),rep(10,15)),
  outcomes = c(rep(4,30),rep(8,30)),
  ranks_simulate = c(rep(xvecs,12)), #1
  ranks_fit = c(rep(xvecf,12)), #2
  mc_error_lp = f_mc_error_lp
)

plotlpmc = ggplot(results_final_mc_lp, aes(x = sample_size, y = mc_error_lp, color = factor(ranks_fit), shape = factor(ranks_simulate))) +
  # geom_jitter(size = 2, alpha = 0.7, width = 100, height = 0)+
  geom_point(size = 1.5, alpha = 0.8,position = position_dodge(width = 1000))+
  # geom_point(size = 2, alpha = 0.6) +F
  geom_vline(xintercept = c(500, 2500, 5000), linetype = "dashed", color = "grey",linewidth= 0.55) +  # Add vertical lines
  facet_grid(covariates ~ outcomes, labeller = label_both) +
  scale_x_continuous(breaks = c(500, 2500, 5000), labels=c('500', '1000', '5000')) +
  scale_y_continuous(breaks = pretty_breaks(n = 5), limits = c(0, max(results_final_mc_lp$mc_error_lp))) +
  scale_color_manual(values = c("purple", "#F8766D","#00BA38"), name = "Ranks Fit") +
  scale_shape_manual(values = c(16, 17), name = "Ranks Simulate") +
  labs(
    x = "Sample Size (n)",
    y = "MC ERROR linear predictor",
    title = "Simulation Study 1 Results",
    subtitle = "MC ERROR linear predictor for different combinations of parameters"
  ) +
  theme_minimal() +
  theme(
    axis.title.x = element_text(size = 14), 
    axis.title.y = element_text(size = 14), 
    plot.title = element_text(hjust = 0.5, size = 20),
    plot.subtitle = element_text(hjust = 0.5, size = 14),
    strip.text = element_text(size = 14),
    legend.key.size = unit(0.1, "lines"),
    legend.text = element_text(size = 14),
    legend.title = element_text(size = 14),
    panel.grid.major.x = element_blank(),  # Remove major vertical grid lines
    panel.grid.minor.x = element_blank(),  # Remove minor vertical grid lines
    panel.grid.major.y = element_line(color = "grey", linewidth = 0.5),  # Keep major horizontal grid lines
    panel.grid.minor.y = element_blank()  # Remove minor horizontal grid lines
  )



plotmse
plotbias
plotlp
plotmsemc
plotbiasmc
plotlpmc

ggsave("plotmset.png", plot = plotmse, width = 8, height = 4.5, dpi = 300)
ggsave("plotbias.png", plot = plotbias, width = 8, height = 4.5, dpi = 300)
ggsave("plotlp.png", plot = plotlp, width = 8, height = 4.5, dpi = 300)
ggsave("plotmsemc.png", plot = plotmsemc, width = 8, height = 4.5, dpi = 300)
ggsave("plotbiasmc.png", plot = plotbiasmc, width = 8, height = 4.5, dpi = 300)
```
## Tables with Means and MC errors
```{r}
combine_simulation_results <- function(mse_df, bias_df, lp_df, mc_mse_df, mc_bias_df, mc_lp_df) {
  # Remove CI columns from the main dataframes
  mse_df <- subset(mse_df, select = -c(ci_lower, ci_upper))
  bias_df <- subset(bias_df, select = -c(ci_lower, ci_upper))
  lp_df <- subset(lp_df, select = -c(ci_lower, ci_upper))
  
  # Rename the mean columns to 'value' and add metric indicator
  names(mse_df)[names(mse_df) == "mean_mse"] <- "value"
  mse_df$metric <- "MSE"
  
  names(bias_df)[names(bias_df) == "mean_bias"] <- "value"
  bias_df$metric <- "ABSOLUTE BIAS"
  
  names(lp_df)[names(lp_df) == "mean_lp"] <- "value"
  lp_df$metric <- "LP ERROR"
  
  # Rename Monte Carlo error columns and add them to respective dataframes
  mse_df$mc_error <- mc_mse_df$mc_error_mse
  bias_df$mc_error <- mc_bias_df$mc_error_bias
  lp_df$mc_error <- mc_lp_df$mc_error_lp
  
  # Combine all dataframes
  combined_df <- rbind(mse_df, bias_df, lp_df)
  
  # Reorder columns
  combined_df <- combined_df[, c("sample_size", "covariates", "outcomes", 
                                "ranks_simulate", "ranks_fit", "metric",
                                "value", "mc_error")]
  
  # Sort by metric type and other variables
  combined_df <- combined_df[order(combined_df$metric, 
                                 combined_df$sample_size,
                                 combined_df$covariates,
                                 combined_df$outcomes,
                                 combined_df$ranks_simulate,
                                 combined_df$ranks_fit), ]
  
  return(combined_df)
}

# Example usage:
# Assuming your dataframes are named results_final_mse, results_final_bias,
# results_final_lp, results_final_mc_mse, results_final_mc_bias, and results_final_mc_lp

combined_results <- combine_simulation_results(
  results_final_mse,
  results_final_bias,
  results_final_lp,
  results_final_mc_mse,
  results_final_mc_bias,
  results_final_mc_lp
)

combined_results$sample_size[combined_results$sample_size == 2500] <- 1000

combined_results$value <- formatC(combined_results$value, format = "e", digits = 4)
combined_results$mc_error <- formatC(combined_results$mc_error, format = "e", digits = 4)

latex_table <- xtable(combined_results,digits = c(0, 0, 0, 0, 0, 0, 0, 4, 4))


print(latex_table, comment = FALSE, include.rownames = FALSE)
```

### B cor and LP cor

```{r}
B_mean_cor =  c(mean(performance_results11$cor_coeff_vec),
                 mean(performance_results12$cor_coeff_vec),
                 # mean(performance_results13$cor_coeff_vec),
                 mean(performance_results21$cor_coeff_vec),
                 mean(performance_results22$cor_coeff_vec),
                 mean(performance_results23$cor_coeff_vec),
  
                 mean(performance_results11a$cor_coeff_vec),
                 mean(performance_results12a$cor_coeff_vec),
                 # mean(performance_results13a$cor_coeff_vec),
                 mean(performance_results21a$cor_coeff_vec),
                 mean(performance_results22a$cor_coeff_vec),
                 mean(performance_results23a$cor_coeff_vec),
  
                 mean(performance_results11b$cor_coeff_vec),
                 mean(performance_results12b$cor_coeff_vec),
                 # mean(performance_results13b$cor_coeff_vec),
                 mean(performance_results21b$cor_coeff_vec),
                 mean(performance_results22b$cor_coeff_vec),
                 mean(performance_results23b$cor_coeff_vec),
  
                 mean(performance_results11c$cor_coeff_vec),
                 mean(performance_results12c$cor_coeff_vec),
                 # mean(performance_results13c$cor_coeff_vec),
                 mean(performance_results21c$cor_coeff_vec),
                 mean(performance_results22c$cor_coeff_vec),
                 mean(performance_results23c$cor_coeff_vec),
  
                 mean(performance_results11d$cor_coeff_vec),
                 mean(performance_results12d$cor_coeff_vec),
                 # mean(performance_results13d$cor_coeff_vec),
                 mean(performance_results21d$cor_coeff_vec),
                 mean(performance_results22d$cor_coeff_vec),
                 mean(performance_results23d$cor_coeff_vec),
  
                 mean(performance_results11e$cor_coeff_vec),
                 mean(performance_results12e$cor_coeff_vec),
                 # mean(performance_results13e$cor_coeff_vec),
                 mean(performance_results21e$cor_coeff_vec),
                 mean(performance_results22e$cor_coeff_vec),
                 mean(performance_results23e$cor_coeff_vec),
                 
                 mean(performance_results11f$cor_coeff_vec),
                 mean(performance_results12f$cor_coeff_vec),
                 # mean(performance_results13f$cor_coeff_vec),
                 mean(performance_results21f$cor_coeff_vec),
                 mean(performance_results22f$cor_coeff_vec),
                 mean(performance_results23f$cor_coeff_vec),
                 
                 mean(performance_results11g$cor_coeff_vec),
                 mean(performance_results12g$cor_coeff_vec),
                 # mean(performance_results13g$cor_coeff_vec),
                 mean(performance_results21g$cor_coeff_vec),
                 mean(performance_results22g$cor_coeff_vec),
                 mean(performance_results23g$cor_coeff_vec),
                 
                 
                 mean(performance_results11h$cor_coeff_vec),
                 mean(performance_results12h$cor_coeff_vec),
                 # mean(performance_results13h$cor_coeff_vec),
                 mean(performance_results21h$cor_coeff_vec),
                 mean(performance_results22h$cor_coeff_vec),
                 mean(performance_results23h$cor_coeff_vec),
                 
                 
                 mean(performance_results11i$cor_coeff_vec),
                 mean(performance_results12i$cor_coeff_vec),
                 # mean(performance_results13i$cor_coeff_vec),
                 mean(performance_results21i$cor_coeff_vec),
                 mean(performance_results22i$cor_coeff_vec),
                 mean(performance_results23i$cor_coeff_vec),
                 
                 mean(performance_results11j$cor_coeff_vec),
                 mean(performance_results12j$cor_coeff_vec),
                 # mean(performance_results13j$cor_coeff_vec),
                 mean(performance_results21j$cor_coeff_vec),
                 mean(performance_results22j$cor_coeff_vec),
                 mean(performance_results23j$cor_coeff_vec),
                
                 mean(performance_results11k$cor_coeff_vec),
                 mean(performance_results12k$cor_coeff_vec),
                 # mean(performance_results13k$cor_coeff_vec),
                 mean(performance_results21k$cor_coeff_vec),
                 mean(performance_results22k$cor_coeff_vec),
                 mean(performance_results23k$cor_coeff_vec)
                 )



lp_mean_cor =  c( mean(LP_errorsfirst11$cor_lp),
                      mean(LP_errorsfirst12$cor_lp),
                      mean(LP_errorsfirst21$cor_lp),
                      mean(LP_errorsfirst22$cor_lp),
                      mean(LP_errorsfirst23$cor_lp),
                      
                      mean(LP_errorssecond11$cor_lp),
                      mean(LP_errorssecond12$cor_lp),
                      mean(LP_errorssecond21$cor_lp),
                      mean(LP_errorssecond22$cor_lp),
                      mean(LP_errorssecond23$cor_lp),
                      
                      mean(LP_errorsthird11$cor_lp),
                      mean(LP_errorsthird12$cor_lp),
                      mean(LP_errorsthird21$cor_lp),
                      mean(LP_errorsthird22$cor_lp),
                      mean(LP_errorsthird23$cor_lp),
                      
                      mean(LP_errorsfourth11$cor_lp),
                      mean(LP_errorsfourth12$cor_lp),
                      mean(LP_errorsfourth21$cor_lp),
                      mean(LP_errorsfourth22$cor_lp),
                      mean(LP_errorsfourth23$cor_lp),
                      
                      mean(LP_errorsfifth11$cor_lp),
                      mean(LP_errorsfifth12$cor_lp),
                      mean(LP_errorsfifth21$cor_lp),
                      mean(LP_errorsfifth22$cor_lp),
                      mean(LP_errorsfifth23$cor_lp),
                      
                      mean(LP_errorssix11$cor_lp),
                      mean(LP_errorssix12$cor_lp),
                      mean(LP_errorssix21$cor_lp),
                      mean(LP_errorssix22$cor_lp),
                      mean(LP_errorssix23$cor_lp),
                      
                      mean(LP_errorssev11$cor_lp),
                      mean(LP_errorssev12$cor_lp),
                      mean(LP_errorssev21$cor_lp),
                      mean(LP_errorssev22$cor_lp),
                      mean(LP_errorssev23$cor_lp),
                      
                      mean(LP_errorseig11$cor_lp),
                      mean(LP_errorseig12$cor_lp),
                      mean(LP_errorseig21$cor_lp),
                      mean(LP_errorseig22$cor_lp),
                      mean(LP_errorseig23$cor_lp),
                      
                      mean(LP_errorsnine11$cor_lp),
                      mean(LP_errorsnine12$cor_lp),
                      mean(LP_errorsnine21$cor_lp),
                      mean(LP_errorsnine22$cor_lp),
                      mean(LP_errorsnine23$cor_lp),
                      
                      mean(LP_errorsten11$cor_lp),
                      mean(LP_errorsten12$cor_lp),
                      mean(LP_errorsten21$cor_lp),
                      mean(LP_errorsten22$cor_lp),
                      mean(LP_errorsten23$cor_lp),
                      
                      mean(LP_errorsele11$cor_lp),
                      mean(LP_errorsele12$cor_lp),
                      mean(LP_errorsele21$cor_lp),
                      mean(LP_errorsele22$cor_lp),
                      mean(LP_errorsele23$cor_lp),
                      
                      mean(LP_errorstwe11$cor_lp),
                      mean(LP_errorstwe12$cor_lp),
                      mean(LP_errorstwe21$cor_lp),
                      mean(LP_errorstwe22$cor_lp),
                      mean(LP_errorstwe23$cor_lp)
                      )




B_sd_cor =  c(sd(performance_results11$cor_coeff_vec),
                 sd(performance_results12$cor_coeff_vec),
                 # sd(performance_results13$cor_coeff_vec),
                 sd(performance_results21$cor_coeff_vec),
                 sd(performance_results22$cor_coeff_vec),
                 sd(performance_results23$cor_coeff_vec),
  
                 sd(performance_results11a$cor_coeff_vec),
                 sd(performance_results12a$cor_coeff_vec),
                 # sd(performance_results13a$cor_coeff_vec),
                 sd(performance_results21a$cor_coeff_vec),
                 sd(performance_results22a$cor_coeff_vec),
                 sd(performance_results23a$cor_coeff_vec),
  
                 sd(performance_results11b$cor_coeff_vec),
                 sd(performance_results12b$cor_coeff_vec),
                 # sd(performance_results13b$cor_coeff_vec),
                 sd(performance_results21b$cor_coeff_vec),
                 sd(performance_results22b$cor_coeff_vec),
                 sd(performance_results23b$cor_coeff_vec),
  
                 sd(performance_results11c$cor_coeff_vec),
                 sd(performance_results12c$cor_coeff_vec),
                 # sd(performance_results13c$cor_coeff_vec),
                 sd(performance_results21c$cor_coeff_vec),
                 sd(performance_results22c$cor_coeff_vec),
                 sd(performance_results23c$cor_coeff_vec),
  
                 sd(performance_results11d$cor_coeff_vec),
                 sd(performance_results12d$cor_coeff_vec),
                 # sd(performance_results13d$cor_coeff_vec),
                 sd(performance_results21d$cor_coeff_vec),
                 sd(performance_results22d$cor_coeff_vec),
                 sd(performance_results23d$cor_coeff_vec),
  
                 sd(performance_results11e$cor_coeff_vec),
                 sd(performance_results12e$cor_coeff_vec),
                 # sd(performance_results13e$cor_coeff_vec),
                 sd(performance_results21e$cor_coeff_vec),
                 sd(performance_results22e$cor_coeff_vec),
                 sd(performance_results23e$cor_coeff_vec),
                 
                 sd(performance_results11f$cor_coeff_vec),
                 sd(performance_results12f$cor_coeff_vec),
                 # sd(performance_results13f$cor_coeff_vec),
                 sd(performance_results21f$cor_coeff_vec),
                 sd(performance_results22f$cor_coeff_vec),
                 sd(performance_results23f$cor_coeff_vec),
                 
                 sd(performance_results11g$cor_coeff_vec),
                 sd(performance_results12g$cor_coeff_vec),
                 # sd(performance_results13g$cor_coeff_vec),
                 sd(performance_results21g$cor_coeff_vec),
                 sd(performance_results22g$cor_coeff_vec),
                 sd(performance_results23g$cor_coeff_vec),
                 
                 
                 sd(performance_results11h$cor_coeff_vec),
                 sd(performance_results12h$cor_coeff_vec),
                 # sd(performance_results13h$cor_coeff_vec),
                 sd(performance_results21h$cor_coeff_vec),
                 sd(performance_results22h$cor_coeff_vec),
                 sd(performance_results23h$cor_coeff_vec),
                 
                 
                 sd(performance_results11i$cor_coeff_vec),
                 sd(performance_results12i$cor_coeff_vec),
                 # sd(performance_results13i$cor_coeff_vec),
                 sd(performance_results21i$cor_coeff_vec),
                 sd(performance_results22i$cor_coeff_vec),
                 sd(performance_results23i$cor_coeff_vec),
                 
                 sd(performance_results11j$cor_coeff_vec),
                 sd(performance_results12j$cor_coeff_vec),
                 # sd(performance_results13j$cor_coeff_vec),
                 sd(performance_results21j$cor_coeff_vec),
                 sd(performance_results22j$cor_coeff_vec),
                 sd(performance_results23j$cor_coeff_vec),
                
                 sd(performance_results11k$cor_coeff_vec),
                 sd(performance_results12k$cor_coeff_vec),
                 # sd(performance_results13k$cor_coeff_vec),
                 sd(performance_results21k$cor_coeff_vec),
                 sd(performance_results22k$cor_coeff_vec),
                 sd(performance_results23k$cor_coeff_vec)
                 )



lp_sd_cor =  c( sd(LP_errorsfirst11$cor_lp),
                      sd(LP_errorsfirst12$cor_lp),
                      sd(LP_errorsfirst21$cor_lp),
                      sd(LP_errorsfirst22$cor_lp),
                      sd(LP_errorsfirst23$cor_lp),
                      
                      sd(LP_errorssecond11$cor_lp),
                      sd(LP_errorssecond12$cor_lp),
                      sd(LP_errorssecond21$cor_lp),
                      sd(LP_errorssecond22$cor_lp),
                      sd(LP_errorssecond23$cor_lp),
                      
                      sd(LP_errorsthird11$cor_lp),
                      sd(LP_errorsthird12$cor_lp),
                      sd(LP_errorsthird21$cor_lp),
                      sd(LP_errorsthird22$cor_lp),
                      sd(LP_errorsthird23$cor_lp),
                      
                      sd(LP_errorsfourth11$cor_lp),
                      sd(LP_errorsfourth12$cor_lp),
                      sd(LP_errorsfourth21$cor_lp),
                      sd(LP_errorsfourth22$cor_lp),
                      sd(LP_errorsfourth23$cor_lp),
                      
                      sd(LP_errorsfifth11$cor_lp),
                      sd(LP_errorsfifth12$cor_lp),
                      sd(LP_errorsfifth21$cor_lp),
                      sd(LP_errorsfifth22$cor_lp),
                      sd(LP_errorsfifth23$cor_lp),
                      
                      sd(LP_errorssix11$cor_lp),
                      sd(LP_errorssix12$cor_lp),
                      sd(LP_errorssix21$cor_lp),
                      sd(LP_errorssix22$cor_lp),
                      sd(LP_errorssix23$cor_lp),
                      
                      sd(LP_errorssev11$cor_lp),
                      sd(LP_errorssev12$cor_lp),
                      sd(LP_errorssev21$cor_lp),
                      sd(LP_errorssev22$cor_lp),
                      sd(LP_errorssev23$cor_lp),
                      
                      sd(LP_errorseig11$cor_lp),
                      sd(LP_errorseig12$cor_lp),
                      sd(LP_errorseig21$cor_lp),
                      sd(LP_errorseig22$cor_lp),
                      sd(LP_errorseig23$cor_lp),
                      
                      sd(LP_errorsnine11$cor_lp),
                      sd(LP_errorsnine12$cor_lp),
                      sd(LP_errorsnine21$cor_lp),
                      sd(LP_errorsnine22$cor_lp),
                      sd(LP_errorsnine23$cor_lp),
                      
                      sd(LP_errorsten11$cor_lp),
                      sd(LP_errorsten12$cor_lp),
                      sd(LP_errorsten21$cor_lp),
                      sd(LP_errorsten22$cor_lp),
                      sd(LP_errorsten23$cor_lp),
                      
                      sd(LP_errorsele11$cor_lp),
                      sd(LP_errorsele12$cor_lp),
                      sd(LP_errorsele21$cor_lp),
                      sd(LP_errorsele22$cor_lp),
                      sd(LP_errorsele23$cor_lp),
                      
                      sd(LP_errorstwe11$cor_lp),
                      sd(LP_errorstwe12$cor_lp),
                      sd(LP_errorstwe21$cor_lp),
                      sd(LP_errorstwe22$cor_lp),
                      sd(LP_errorstwe23$cor_lp)
                      )



xvecs = c(rep(1,2),rep(2,3))
xvecf = c(1,2,1,2,3)
table_final_b_lp_cor <- data.frame(
  sample_size = c(rep(c(500, 1000, 5000), times = 4, each = 5)),
  covariates = c(rep(5,15),rep(10,15),rep(5,15),rep(10,15)),
  outcomes = c(rep(4,30),rep(8,30)),
  ranks_simulate = c(rep(xvecs,12)), #1
  ranks_fit = c(rep(xvecf,12)), #2
  B_mean_cor = B_mean_cor,
  B_sd_cor = B_sd_cor,
  lp_mean_cor = lp_mean_cor,
  lp_sd_cor=lp_sd_cor
)

table_final_b_lp_cor$B_mean_cor <- formatC(table_final_b_lp_cor$B_mean_cor, format = "e", digits = 4)
table_final_b_lp_cor$B_sd_cor <- formatC(table_final_b_lp_cor$B_sd_cor , format = "e", digits = 4)
table_final_b_lp_cor$lp_mean_cor <- formatC(table_final_b_lp_cor$lp_mean_cor, format = "e", digits = 4)
table_final_b_lp_cor$lp_sd_cor <- formatC(table_final_b_lp_cor$lp_sd_cor, format = "e", digits = 4)

latex_table2 <- xtable(table_final_b_lp_cor,digits = c(0, 0, 0, 0, 0, 0, 4, 4, 4, 4))
print(latex_table2, comment = FALSE, include.rownames = FALSE)
```





# Check why seed 42 did not work

## Function to check if Beta Matrix is close to rank 1

```{r}
rank_check = function(Beta_m){
  
svd_result <- svd(Beta_m)

# Singular values
singular_values <- svd_result$d
sigma_1 <- singular_values[1]
sigma_2 <- singular_values[2]

# Check closeness to rank 1
ratio <- sigma_2 / sigma_1
difference <- sigma_1 - sigma_2


cat("Singular values:", singular_values, "\n")
cat("Ratio of singular values:", ratio, "\n")
cat("Difference between singular values:", difference, "\n")

# Determine if the matrix is close to rank 1
if (ratio < 0.01) {
  cat("The matrix is close to rank 1.\n")
} else {
  cat("The matrix is not close to rank 1.\n")
}
}
```


## Generate data with seed 42 and show that results are not as expected

```{r}
 set.seed(42)

 #chose variables
 n = 5000 # Number of observations
 p <- 5  # Number of covariates
 k <- 4   # Number of outcomes
 n_simulations = 1

 r <- 2  # Rank of simulated data

 datasau = simulations_data(n,p,r,k,n_simulations)
```



```{r}
 nranks <- 1

 dlong <- datasau$dlong_list[[1]]

 #construct formula for the model
 predictor_names <- grep("^x", names(dlong), value = TRUE)
 formula_str <- paste("Surv(Tstop, status) ~", paste(predictor_names, collapse = " + "))
 formula <- as.formula(formula_str)

 model_r1 <- redrank(formula, data = dlong, R = nranks, print.level = 1)

 Beta_hat_r1 <- model_r1$Alpha %*% model_r1$Gamma
 Beta_true <- datasau$Beta_matrix
```




```{r}
 nranks <- 2

 dlong <- datasau$dlong_list[[1]]

 #construct formula for the model
 predictor_names <- grep("^x", names(dlong), value = TRUE)
 formula_str <- paste("Surv(Tstop, status) ~", paste(predictor_names, collapse = " + "))
 formula <- as.formula(formula_str)

 model_r2 <- redrank(formula, data = dlong, R = nranks, print.level = 1)

 Beta_hat_r2 <- model_r2$Alpha %*% model_r2$Gamma
 Beta_true <- datasau$Beta_matrix
```

### Results not as expected since mse and bias for rank2 should be smaller than rank1 when data are generated under rank 2

```{r}
 performance_redrank_model(Beta_true, model_r2,  Print_flug=F)$mse_out
 performance_redrank_model(Beta_true, model_r1, Print_flug=F)$mse_out

 performance_redrank_model(Beta_true, model_r2,  Print_flug=F)$bias_out
 performance_redrank_model(Beta_true, model_r1,  Print_flug=F)$bias_out
```
## Explanation why results not as expected

```{r}
rankMatrix(datasau$Beta_matrix) #indeed rank 2
rank_check(datasau$Beta_matrix) #but close to rank 1--> that is the reason for unexpected results and we should be carefull
```

```{r}
rank_check(datas1$Beta_matrix)
rank_check(datas1a$Beta_matrix)
rank_check(datas1b$Beta_matrix)
rank_check(datas1c$Beta_matrix)
rank_check(datas1d$Beta_matrix)
rank_check(datas1e$Beta_matrix)
rank_check(datas1f$Beta_matrix)
rank_check(datas1g$Beta_matrix)
rank_check(datas1h$Beta_matrix)
rank_check(datas1i$Beta_matrix)
rank_check(datas1j$Beta_matrix)
rank_check(datas1k$Beta_matrix)


```
```{r}
rank_check(datas2$Beta_matrix)
rank_check(datas2a$Beta_matrix)
rank_check(datas2b$Beta_matrix)
rank_check(datas2c$Beta_matrix)
rank_check(datas2d$Beta_matrix)
rank_check(datas2e$Beta_matrix)
rank_check(datas2f$Beta_matrix)
rank_check(datas2g$Beta_matrix)
rank_check(datas2h$Beta_matrix)
rank_check(datas2i$Beta_matrix)
rank_check(datas2j$Beta_matrix)
rank_check(datas2k$Beta_matrix)
```

